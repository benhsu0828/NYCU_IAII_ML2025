import torch
import argparse
import random
import sys
import wandb
import json
import re
import pandas as pd
import os
from datasets import Dataset, IterableDataset
import sys
import gc
import time
# CPT è¨“ç·´é…ç½®

from transformers import TrainingArguments
sys.stdout.flush()  # å¼·åˆ¶åˆ·æ–°è¼¸å‡ºç·©è¡

def preprocess_taigi_text(text):
    """è™•ç†å°æ–‡è³‡æ–™çš„é è™•ç†"""

    # 1. ç§»é™¤ https/http é–‹é ­çš„ç¶²å€ï¼Œç›´åˆ°é‡åˆ°æ¨™é»ç¬¦è™Ÿæˆ–ç©ºæ ¼
    # åŒ¹é…åˆ° )ã€ã€‚ã€ï¼Œã€ç©ºç™½ ç­‰ç¬¦è™Ÿç‚ºæ­¢
    text = re.sub(r'https?://[^\s)ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]+', '', text)

    # 2. ç§»é™¤ www é–‹é ­çš„ç¶²å€ç‰‡æ®µ
    text = re.sub(r'www\.[^\s)ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]+', '', text)

    # 3. ç§»é™¤æ®˜ç•™çš„åŸŸåç‰‡æ®µï¼ˆæ›´å¯¬é¬†çš„åŒ¹é…ï¼‰
    text = re.sub(r'\b\w+\.(com|org|net|edu|gov|tw|io|co|info|biz)(/[^\s)ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]*)?', '', text)

    # 4. ç§»é™¤è¡Œé¦–çš„æ®µè½ç·¨è™Ÿï¼ˆå¦‚ï¼š1. 2. 3.ï¼‰
    text = re.sub(r'^\d+\.\s*', '', text, flags=re.MULTILINE)

    # 5. çµ±ä¸€æ¨™é»ç¬¦è™Ÿ
    text = text.replace('ã€‚', 'ã€‚')
    text = text.replace('ï¼Œ', 'ï¼Œ')

    # 6. çµ±ä¸€ç ´æŠ˜è™Ÿ
    text = text.replace('â€”', '-')

    # 7. ç§»é™¤éå¤šçš„ç©ºç™½å’Œæ›è¡Œ
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def load_cpt_taigi_data(preprocessed_file="../data/cpt_dataset.parquet"):
    """
    è¼‰å…¥é è™•ç†å¥½çš„ CPT è³‡æ–™
    
    Args:
        preprocessed_file: é è™•ç†å¥½çš„è³‡æ–™æª”æ¡ˆè·¯å¾‘ (.parquet æˆ– .jsonl)
    
    Returns:
        Dataset: è¼‰å…¥çš„è³‡æ–™é›†
    """
    if not os.path.exists(preprocessed_file):
        print(f"âš ï¸  æ‰¾ä¸åˆ°é è™•ç†æª”æ¡ˆ: {preprocessed_file}")
        print("è«‹å…ˆåŸ·è¡Œ: python preprocess_cpt_data.py")
        raise FileNotFoundError(f"è«‹å…ˆåŸ·è¡Œè³‡æ–™é è™•ç†è…³æœ¬ç”¢ç”Ÿ {preprocessed_file}")
    
    print(f"ğŸ“‚ è¼‰å…¥é è™•ç†è³‡æ–™: {preprocessed_file}")
    
    # æ ¹æ“šæª”æ¡ˆæ ¼å¼è¼‰å…¥
    if preprocessed_file.endswith('.parquet'):
        cpt_dataset = Dataset.from_parquet(preprocessed_file)
    elif preprocessed_file.endswith('.jsonl') or preprocessed_file.endswith('.json'):
        cpt_dataset = Dataset.from_json(preprocessed_file)
    else:
        raise ValueError("ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼Œè«‹ä½¿ç”¨ .parquet æˆ– .jsonl")
    
    print(f"âœ… è¼‰å…¥å®Œæˆï¼ç¸½ç­†æ•¸: {len(cpt_dataset)}")
    print(f"ğŸ“Š ç¬¬ä¸€ç­†è³‡æ–™é è¦½: {cpt_dataset[0]['text'][:100]}...")
    
    return cpt_dataset

def inference(model_path, test_data, output_dir, max_seq_length, batch_size=2):
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = model_path,
        max_seq_length = max_seq_length,
        dtype = None,
        load_in_4bit = True,
    )

    # å°‡æ¨¡å‹è¨­å®šç‚ºæ¨ç†æ¨¡å¼
    FastLanguageModel.for_inference(model)

    # è¨­å®š tokenizer padding
    tokenizer.padding_side = 'left'
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # å¦‚æœè¼¸å‡ºæª”æ¡ˆå·²å­˜åœ¨ï¼Œå…ˆåˆªé™¤ä»¥é¿å…é‡è¤‡å¯«å…¥
    if os.path.exists(output_dir):
        os.remove(output_dir)

    test_df = pd.read_csv(test_data)
    # åˆå§‹åŒ– write_header è®Šæ•¸
    write_header = True

    print(f"é–‹å§‹é æ¸¬ï¼Œç¸½ç­†æ•¸: {len(test_df)}ï¼ŒBatch Size: {batch_size}")

    # ä½¿ç”¨ range æ¯æ¬¡è·³ batch_size çš„æ­¥é•·
    for i in range(0, len(test_df), batch_size):
        # å–å‡ºç›®å‰çš„ batch è³‡æ–™
        batch_df = test_df.iloc[i : i + batch_size]

        prompts = []
        ids = []

        # æº–å‚™é€™å€‹ batch çš„æ‰€æœ‰ Prompt
        for index, raw in batch_df.iterrows():
            question_background = raw['å‰æ–‡']
            question = raw['é¡Œå¹¹']
            answer1 = raw['é¸é …1']
            answer2 = raw['é¸é …2']
            answer3 = raw['é¸é …3']
            answer4 = raw['é¸é …4']

            # ä½¿ç”¨èˆ‡è¨“ç·´æ™‚ç›¸åŒçš„æ ¼å¼
            prompt = f"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å•ç­”åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šå‰æ–‡çš„èƒŒæ™¯ï¼Œå›ç­”é¡Œç›®å•é¡Œï¼Œåªè¦é¸å‡ºæ­£ç¢ºçš„é¸é …ç·¨è™Ÿ(1-4)ã€‚\nå‰æ–‡ï¼š{question_background}\nå•é¡Œï¼š{question}\nå¾ä»¥ä¸‹å››å€‹é¸é …é¸å‡ºæ­£ç¢ºçš„é¸é …ç·¨è™Ÿ\né¸é …1ï¼š{answer1}\né¸é …2ï¼š{answer2}\né¸é …3ï¼š{answer3}\né¸é …4ï¼š{answer4}\n"

            prompts.append(prompt)
            ids.append(raw['ID'])

        # æ‰¹æ¬¡ tokenize
        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=2048).to("cuda")

        # æ‰¹æ¬¡ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=10,  # å› ç‚ºåªéœ€è¦ç”Ÿæˆæ•¸å­—ï¼Œæ‰€ä»¥è¨­å°ä¸€é»
                do_sample=False,    # ä½¿ç”¨ greedy decoding ç¢ºä¿çµæœä¸€è‡´
                pad_token_id=tokenizer.eos_token_id,
                use_cache=True
            )

        # è§£ç¢¼ç”Ÿæˆçš„æ–‡å­—ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        predicted_texts = []
        for j, output in enumerate(outputs):
            input_length = inputs['input_ids'][j].shape[0]
            generated_tokens = output[input_length:]
            predicted_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

            # ç§»é™¤æ‰€æœ‰ç‰¹æ®Šæ¨™è¨˜å’Œå¤šé¤˜æ–‡å­—
            # å°‹æ‰¾ç¬¬ä¸€å€‹æ•¸å­— 1-4
            match = re.search(r'^[1-4]', predicted_text)
            if match:
                clean_answer = match.group()
            else:
                # å¦‚æœæ²’æ‰¾åˆ°ï¼Œå˜—è©¦å¾æ•´æ®µæ–‡å­—ä¸­æ‰¾
                match = re.search(r'[1-4]', predicted_text)
                clean_answer = match.group() if match else "1"  # é è¨­ç‚º1

            predicted_texts.append(clean_answer)

        # å»ºç«‹ Batch çš„ DataFrame
        output_batch = pd.DataFrame({
            'ID': ids,
            'Answer': predicted_texts
        })

        # å¯«å…¥ CSV (append æ¨¡å¼)
        output_batch.to_csv(output_dir, mode='a', header=write_header, index=False, encoding='utf-8-sig')

        # ç¬¬ä¸€æ¬¡å¯«å…¥å¾Œï¼Œä¹‹å¾Œéƒ½ä¸éœ€è¦ header
        write_header = False

        print(f"å·²è™•ç†: {min(i + batch_size, len(test_df))} / {len(test_df)}")

    print("é æ¸¬å®Œæˆï¼")

if __name__ == "__main__":
    #å®šç¾©æ¨¡å‹åç¨±èˆ‡æœ€å¤§åºåˆ—é•·åº¦
    parser = argparse.ArgumentParser(description="Taigi LLM Training Pipeline")
    parser.add_argument("--run_cpt", action="store_true", help="æ˜¯å¦åŸ·è¡Œ CPT (Continued Pre-Training)")
    parser.add_argument("--run_sft", action="store_true", help="æ˜¯å¦åŸ·è¡Œ SFT (Supervised Fine-Tuning)")
    parser.add_argument("--run_inf", action="store_true", help="æ˜¯å¦åŸ·è¡Œ Inference (é æ¸¬)")
    args = parser.parse_args()
    
    try:
        import torch
        if torch.cuda.is_available():
            print("CUDA å¯ç”¨ï¼š", torch.cuda.get_device_name(0))
        else:
            print("CUDA ä¸å¯ç”¨ï¼Œæœƒæ”¹ç”¨ CPU æˆ– 4-bit on CPU æ¨¡å¼")
    except Exception as e:
        print("æª¢æŸ¥ CUDA æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š", e)

    from unsloth import FastLanguageModel
    from trl import SFTTrainer, SFTConfig
    '''
    model available: 
    Bohanlu/Taigi-Llama-2-7B, 
    unsloth/Qwen2.5-7B-Instruct, 
    unsloth/Yi-1.5-6B-Chat
    '''
    model_name = "unsloth/Qwen2.5-1.5B-Instruct"
    wandb_project_name = "Ass4-LLM"
    wandb_name1 = "Taigi-CPT-Qwen2.5-v1"
    wandb_name2 = "Taigi-SFT-Qwen2.5-v1"
    max_seq_length = 1024
    random_seed = 9527
    cpt_output_dir = "../model/Qwen2.5_cpt_model"
    final_output_dir = "../model/Qwen2.5_final_model"
    inference_output_dir = "../output_Qwen2.5.csv"
    # ç™»å…¥ wandb
    wandb.login(key="6505e7e06b7f53ea56b61b94658f226c523ebacc")
    # os.environ["WANDB_MODE"] = "offline"  # é¿å…ç¶²è·¯å•é¡Œ
    # Start a new wandb run to track this script.
    if args.run_cpt:
        cpt_run = wandb.init(
            entity="paohuah-national-yang-ming-chiao-tung-university",
            project=wandb_project_name,
            name=wandb_name1,
            config={
                "stage": "CPT",
                "model_name": model_name,
                "learning_rate": 2e-4,
                "max_steps": 1000,
                "lora_r": 64,
                "lora_alpha": 128,
                "max_seq_length": max_seq_length,
                "batch_size": 1,  # 8GB é…ç½®
                "gradient_accumulation": 16,
                "vram": "8GB",
            },
            tags=["CPT", "domain_adaptation"]
        )
        # è¼‰å…¥ä¸¦è™•ç† CPT å°æ–‡è³‡æ–™ï¼ˆä½¿ç”¨é è™•ç†å¥½çš„æª”æ¡ˆï¼‰
        cpt_dataset = load_cpt_taigi_data("../data/cpt_dataset.parquet")

        # ========== éšæ®µ 1: CPT - æŒçºŒé è¨“ç·´ ==========

        # è¼‰å…¥åŸºç¤æ¨¡å‹
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name = model_name,
            max_seq_length = max_seq_length,
            dtype = None,
            load_in_4bit = True,
        )

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id


        # CPT éšæ®µçš„ LoRA é…ç½®ï¼ˆè¼ƒå¤§çš„ rankï¼‰
        model = FastLanguageModel.get_peft_model(
            model,
            r = 32,  # CPT ç”¨è¼ƒå¤§çš„ rank å­¸ç¿’æ›´å¤šçŸ¥è­˜
            target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                            "gate_proj", "up_proj", "down_proj"],
            lora_alpha = 64,  # å°æ‡‰èª¿æ•´ alpha
            lora_dropout = 0.05,  # CPT ç”¨è¼ƒå°çš„ dropout
            bias = "none",
            use_gradient_checkpointing = "unsloth",
            random_state = random_seed,
        )

        cpt_config = SFTConfig(
            dataset_text_field="text",  # æ ¹æ“šä½ çš„ dataset æ¬„ä½åç¨±æ”¹
            per_device_train_batch_size=1,
            gradient_accumulation_steps=16,
            warmup_steps=100,
            # ä½ ä¹‹å‰ç”¨ max_stepsï¼Œä½†æ–°ç‰ˆå»ºè­°ç”¨ num_train_epochs
            #num_train_epochs = 50,            # è¦–ä½ è³‡æ–™é‡æ±ºå®š
            max_steps=1000,
            learning_rate=2e-4,
            weight_decay=0.01,
            optim="adamw_8bit",
            lr_scheduler_type="cosine",
            fp16=False,
            bf16=True,
            max_grad_norm=1.0,
            logging_steps=10,
            save_strategy="steps",
            save_steps=200,
            save_total_limit=1,
            output_dir="outputs_cpt",
            # windows ä¸‹ data loader è¨­å®šå¦‚ä¸‹ï¼Œæ¸›å°‘å¯èƒ½éŒ¯èª¤
            dataloader_pin_memory = False,
            dataloader_num_workers = 0,
            dataset_num_proc=1,
            seed=random_seed,
        )

        cpt_trainer = SFTTrainer(
            model=model,
            processing_class=tokenizer,
            train_dataset=cpt_dataset,
            args=cpt_config,
        )
        # cpt_trainer = SFTTrainer(
        #     model = model,
        #     processing_class = tokenizer,
        #     train_dataset = cpt_dataset,
        #     packing = False,
        #     args = SFTConfig(
        #         per_device_train_batch_size = 1,
        #         gradient_accumulation_steps = 16,
        #         warmup_steps = 100,
        #         max_steps = 1000,  # CPT éœ€è¦æ›´å¤šæ­¥é©Ÿ
        #         learning_rate = 2e-4,  # CPT ç”¨è¼ƒé«˜å­¸ç¿’ç‡
        #         fp16 = False,
        #         bf16 = True,
        #         logging_steps = 10,
        #         optim = "adamw_8bit",
        #         weight_decay = 0.01,
        #         lr_scheduler_type = "cosine",
        #         seed = random_seed,
        #         output_dir = "outputs_cpt",
        #         report_to = "wandb",
        #         run_name = "Taigi-CPT",
        #         max_length = max_seq_length,
        #         # é¡å¤–å„ªåŒ–
        #         max_grad_norm = 1.0,
        #         dataloader_num_workers = 0,   # Windows ä¸Šè¨­ç‚º 0
        #         dataloader_pin_memory = False,  # æ¸›å°‘ RAM ä½¿ç”¨
        #         save_total_limit = 1,  # åªä¿ç•™æœ€å¾Œä¸€å€‹ checkpoint
        #     ),
        # )

        # åŸ·è¡Œ CPT è¨“ç·´
        print("é–‹å§‹ CPT éšæ®µè¨“ç·´...")
        cpt_trainer.train()

        # å„²å­˜ CPT æ¨¡å‹
        model.save_pretrained(cpt_output_dir)
        tokenizer.save_pretrained(cpt_output_dir)
        print("CPT éšæ®µå®Œæˆï¼")

        cpt_run.finish()
        # æ¸…ç†è¨˜æ†¶é«”
        del cpt_trainer
        del model
        del tokenizer
        del cpt_dataset
        gc.collect()
        torch.cuda.empty_cache()
        # ç­‰å¾… 3 ç§’è®“ç³»çµ±é‡‹æ”¾è¨˜æ†¶é«”
        time.sleep(3)
    else:
        print("è·³é CPT éšæ®µ")

    # ========== éšæ®µ 2: SFT - ç›£ç£å¼å¾®èª¿ ==========
    if args.run_sft:
        sft_run = wandb.init(
            entity="paohuah-national-yang-ming-chiao-tung-university",
            project=wandb_project_name,
            name= wandb_name2,
            config={
                "stage": "SFT",
                "base_model": "./model/cpt_model",
                "learning_rate": 1e-5,
                "max_steps": 100,
                "lora_r": 16,
                "lora_alpha": 32,
                "max_seq_length": max_seq_length,
                "batch_size": 1,
                "gradient_accumulation": 16,
                "vram": "8GB",
            },
            tags=["SFT", "qa_task", "8GB-VRAM"]
        )
        if args.run_cpt or os.path.exists(cpt_output_dir):
            load_model_path = cpt_output_dir
            print(f"è¼‰å…¥ CPT æ¨¡å‹: {load_model_path}")
        else:
            load_model_path = model_name
            print(f"è¼‰å…¥åŸå§‹æ¨¡å‹: {load_model_path}")

        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name = load_model_path,
            max_seq_length = max_seq_length,
            dtype = None,
            load_in_4bit = True,
        )

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id


        model = FastLanguageModel.get_peft_model(
            model,
            r = 4,  # é™ä½åˆ° 4 æ¸›å°‘è¨˜æ†¶é«”
            target_modules = ["q_proj", "v_proj"],  # åªè¨“ç·´ 2 å€‹æ¨¡çµ„
            lora_alpha = 8,  # å°æ‡‰èª¿æ•´
            lora_dropout = 0.1,
            bias = "none",
            use_gradient_checkpointing = "unsloth",
            random_state = random_seed,
        )

        model.train()

        # æº–å‚™ SFT è³‡æ–™ï¼ˆæœ‰æ¨™è¨»çš„å•ç­”å°ï¼‰
        df = pd.read_csv("../data/AI_conv.csv")

        sft_dataset_data = []
        for _, row in df.iterrows():
            # çµæ§‹åŒ–çš„å•ç­”æ ¼å¼
            prompt = f"æ ¹æ“šå‰æ–‡å…§å®¹å›ç­”å•é¡Œ\nå‰æ–‡ï¼š{row['æ–‡ç« ']}\nå•é¡Œï¼š{row['å•é¡Œ']}\næ ¹æ“šå•é¡Œï¼Œå¾ä»¥ä¸‹å››å€‹é¸é …é¸å‡ºæ­£ç¢ºçš„é¸é …ç·¨è™Ÿ(1-4)\né¸é …1ï¼š{row['é¸é …1']}\né¸é …2ï¼š{row['é¸é …2']}\né¸é …3ï¼š{row['é¸é …3']}\né¸é …4ï¼š{row['é¸é …4']}\nç­”æ¡ˆï¼š{str(row['æ­£ç¢ºç­”æ¡ˆ'])}"

            sft_dataset_data.append({
                "text": prompt
            })

        sft_dataset = Dataset.from_list(sft_dataset_data)

        # SFT è¨“ç·´é…ç½®
        sft_config = SFTConfig(
            dataset_text_field="text",  # æ ¹æ“šä½ çš„ dataset æ¬„ä½åç¨±æ”¹
            per_device_train_batch_size=1,
            gradient_accumulation_steps=8,
            warmup_steps=100,
            # ä½ ä¹‹å‰ç”¨ max_stepsï¼Œä½†æ–°ç‰ˆå»ºè­°ç”¨ num_train_epochs
            #num_train_epochs = 5,            # è¦–ä½ è³‡æ–™é‡æ±ºå®š
            max_steps=1000,
            learning_rate=1e-5,
            weight_decay=0.01,
            optim="adamw_8bit",
            lr_scheduler_type="cosine",
            fp16=False,
            bf16=True,
            max_grad_norm=1.0,
            logging_steps=10,
            save_strategy="steps",
            save_steps=200,
            save_total_limit=1,
            output_dir="outputs_cpt",
            # windows ä¸‹ data loader è¨­å®šå¦‚ä¸‹ï¼Œæ¸›å°‘å¯èƒ½éŒ¯èª¤
            dataloader_pin_memory = False,
            dataloader_num_workers = 0,
            dataset_num_proc=1,
            seed=random_seed,
        )

        sft_trainer = SFTTrainer(
            model=model,
            processing_class=tokenizer,
            train_dataset=sft_dataset,
            args=sft_config,  
        )
        # # SFT è¨“ç·´é…ç½®
        # sft_trainer = SFTTrainer(
        #     model = model,
        #     processing_class = tokenizer, 
        #     train_dataset = sft_dataset,
        #     packing = False,
        #     args = SFTConfig(
        #         per_device_train_batch_size = 1,
        #         gradient_accumulation_steps = 16,
        #         warmup_steps = 10,
        #         max_steps = 100,  # SFT æ­¥é©Ÿè¼ƒå°‘
        #         learning_rate = 1e-5,  # SFT ç”¨è¼ƒå°å­¸ç¿’ç‡
        #         fp16 = False,
        #         bf16 = True,
        #         logging_steps = 1,
        #         optim = "adamw_8bit",
        #         weight_decay = 0.01,
        #         lr_scheduler_type = "cosine",
        #         seed = random_seed,
        #         output_dir = "outputs_sft",
        #         report_to = "wandb",
        #         run_name = "Taigi-SFT",
        #         max_length = max_seq_length,
        #         dataloader_num_workers = 0,
        #         dataloader_pin_memory = False,
        #     ),
        # )

        # åŸ·è¡Œ SFT è¨“ç·´
        print("é–‹å§‹ SFT éšæ®µè¨“ç·´...")
        sft_trainer.train()

        # å„²å­˜æœ€çµ‚æ¨¡å‹
        model.save_pretrained(final_output_dir)
        tokenizer.save_pretrained(final_output_dir)
        print("SFT éšæ®µå®Œæˆï¼")

        sft_run.finish()
    else:
        print("è·³é SFT éšæ®µ")

    # ========== æ¨ç†æ¸¬è©¦ ==========
    if args.run_inf:
         # æ±ºå®šæ¨ç†ç”¨çš„æ¨¡å‹è·¯å¾‘
        if args.run_sft or os.path.exists(final_output_dir):
            inference_model_path = final_output_dir
            print(f"Inference ä½¿ç”¨ SFT å¾Œçš„æ¨¡å‹: {inference_model_path}")
        elif args.run_cpt or os.path.exists(cpt_output_dir):
            inference_model_path = cpt_output_dir
            print(f"Inference ä½¿ç”¨ CPT å¾Œçš„æ¨¡å‹: {inference_model_path}")
        else:
            inference_model_path = model_name
            print(f"Inference ä½¿ç”¨åŸå§‹æ¨¡å‹: {inference_model_path}")

        inference(
            model_path = inference_model_path,
            test_data = "../data/1001-question-v3.csv",
            output_dir = inference_output_dir,
            max_seq_length = max_seq_length,
            batch_size = 2
        )
    else:
        print("è·³éæ¨ç†æ¸¬è©¦éšæ®µ")