[34m[1mwandb[0m: Detected [huggingface_hub.inference] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
Traceback (most recent call last):
  File "C:\Users\bente\OneDrive\Ê°åÈù¢\NYCU_IAII_ML2025\Ass4-LLM\src\main_CPT_SFT.py", line 390, in <module>
    model, tokenizer = FastLanguageModel.from_pretrained(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bente\miniconda3\envs\Ass4\Lib\site-packages\unsloth\models\loader.py", line 328, in from_pretrained
    model_types = get_transformers_model_type(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\bente\miniconda3\envs\Ass4\Lib\site-packages\unsloth_zoo\hf_utils.py", line 112, in get_transformers_model_type
    raise RuntimeError(
RuntimeError: Unsloth: No config file found - are you sure the `model_name` is correct?
If you're using a model on your local device, confirm if the folder location exists.
If you're using a HuggingFace online model, check if it exists.
