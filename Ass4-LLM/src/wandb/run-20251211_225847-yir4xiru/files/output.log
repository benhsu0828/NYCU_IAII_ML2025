[34m[1mwandb[0m: Detected [huggingface_hub.inference] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
ğŸ“‚ è¼‰å…¥é è™•ç†è³‡æ–™: ../data/cpt_dataset.parquet
âœ… è¼‰å…¥å®Œæˆï¼ç¸½ç­†æ•¸: 1150
ğŸ“Š ç¬¬ä¸€ç­†è³‡æ–™é è¦½: å„ä½é„‰è¦ªé€å®¶å¥½ï¼Œç›¸ä¿¡é€å®¶åŠ åŠ æ¸›æ¸›ç†Ÿä¼¼å’±æºªæ±æ‘çš„æ‘é•·ä¼¯ä»”ï¼Œæ¯‹å…æˆ‘åŠ ä»‹ç´¹ã€‚æ¯‹éï¼Œä¹Ÿå°±æ˜¯å› ç‚ºé€å®¶å°æ‘é•·ä¼¯å‚·éäº†è§£ï¼Œæ”æ‘é•·ä¼¯ä»”ã€æ‘é•·ä¼¯ä»”æŒ‰å‘¢å…±å«ï¼Œæ‰æœ‰æˆ‘å¯«é€™ä»½å‚³å–®çš„å¿…è¦ã€‚æ˜¯tihï¼Œæ‘é•·ä¼¯ä»”æ¬²å‡ºä¾†é¸æœ¬å±†çš„é„‰é•·å›‰...
==((====))==  Unsloth 2025.12.4: Fast Llama patching. Transformers: 4.57.3.
   \\   /|    NVIDIA Graphics Device. Num GPUs = 1. Max memory: 7.536 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.9.1+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1
\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.
Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.
Unsloth 2025.12.4 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.
[trl.trainer.sft_trainer|WARNING]You are using a per_device_train_batch_size of 1 with padding-free training. Using a batch size of 1 anihilate the benefits of padding-free training. Please consider increasing the batch size to at least 2.
ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.
é–‹å§‹ CPT éšæ®µè¨“ç·´...
The model is already on multiple devices. Skipping the move to device specified in `args`.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,150 | Num Epochs = 50 | Total steps = 3,600
O^O/ \_/ \    Batch size per device = 1 | Gradient accumulation steps = 16
\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16
 "-____-"     Trainable parameters = 72,613,888 of 6,133,649,408 (1.18% trained)
  0%|â–                                                                                                                | 9/3600 [04:50<31:38:28, 31.72s/it]
Unsloth: Will smartly offload gradients to save VRAM!
