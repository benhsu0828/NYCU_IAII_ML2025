[34m[1mwandb[0m: Detected [huggingface_hub.inference] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
ğŸ“‚ è¼‰å…¥é è™•ç†è³‡æ–™: ../data/cpt_dataset.parquet
âœ… è¼‰å…¥å®Œæˆï¼ç¸½ç­†æ•¸: 1150
ğŸ“Š ç¬¬ä¸€ç­†è³‡æ–™é è¦½: å„ä½é„‰è¦ªé€å®¶å¥½ï¼Œç›¸ä¿¡é€å®¶åŠ åŠ æ¸›æ¸›ç†Ÿä¼¼å’±æºªæ±æ‘çš„æ‘é•·ä¼¯ä»”ï¼Œæ¯‹å…æˆ‘åŠ ä»‹ç´¹ã€‚æ¯‹éï¼Œä¹Ÿå°±æ˜¯å› ç‚ºé€å®¶å°æ‘é•·ä¼¯å‚·éäº†è§£ï¼Œæ”æ‘é•·ä¼¯ä»”ã€æ‘é•·ä¼¯ä»”æŒ‰å‘¢å…±å«ï¼Œæ‰æœ‰æˆ‘å¯«é€™ä»½å‚³å–®çš„å¿…è¦ã€‚æ˜¯tihï¼Œæ‘é•·ä¼¯ä»”æ¬²å‡ºä¾†é¸æœ¬å±†çš„é„‰é•·å›‰...
==((====))==  Unsloth 2025.12.4: Fast Llama patching. Transformers: 4.57.3.
   \\   /|    NVIDIA Graphics Device. Num GPUs = 1. Max memory: 7.536 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.9.1+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1
\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.
Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.
Unsloth 2025.12.4 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.
[trl.trainer.sft_trainer|WARNING]You are using a per_device_train_batch_size of 1 with padding-free training. Using a batch size of 1 anihilate the benefits of padding-free training. Please consider increasing the batch size to at least 2.
ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.
é–‹å§‹ CPT éšæ®µè¨“ç·´...
The model is already on multiple devices. Skipping the move to device specified in `args`.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,150 | Num Epochs = 14 | Total steps = 1,000
O^O/ \_/ \    Batch size per device = 1 | Gradient accumulation steps = 16
\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16
 "-____-"     Trainable parameters = 72,613,888 of 6,133,649,408 (1.18% trained)
  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                             | 47/1000 [26:45<9:15:39, 34.98s/it]Traceback (most recent call last):
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 3.4569, 'grad_norm': 0.6974219679832458, 'learning_rate': 1.8e-05, 'epoch': 0.14}
{'loss': 3.3935, 'grad_norm': 0.6365823149681091, 'learning_rate': 3.8e-05, 'epoch': 0.28}
{'loss': 3.2258, 'grad_norm': 0.6492664217948914, 'learning_rate': 5.8e-05, 'epoch': 0.42}
{'loss': 3.1727, 'grad_norm': 0.7863137722015381, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.56}
  File "/home/ben/æ¡Œé¢/NYCU_IAII_ML2025/Ass4-LLM/src/./main_CPT_SFT.py", line 299, in <module>
    # å„²å­˜ CPT æ¨¡å‹
  File "/home/ben/æ¡Œé¢/NYCU_IAII_ML2025/Ass4-LLM/src/unsloth_compiled_cache/UnslothSFTTrainer.py", line 55, in wrapper
    output = f(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ben/miniconda3/envs/Ass4/lib/python3.11/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 330, in _fast_inner_training_loop
  File "/home/ben/æ¡Œé¢/NYCU_IAII_ML2025/Ass4-LLM/src/unsloth_compiled_cache/UnslothSFTTrainer.py", line 1082, in training_step
    return super().training_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 91, in _unsloth_training_step
  File "/home/ben/miniconda3/envs/Ass4/lib/python3.11/site-packages/accelerate/accelerator.py", line 2852, in backward
    loss.backward(**kwargs)
  File "/home/ben/miniconda3/envs/Ass4/lib/python3.11/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ben/miniconda3/envs/Ass4/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ben/miniconda3/envs/Ass4/lib/python3.11/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
