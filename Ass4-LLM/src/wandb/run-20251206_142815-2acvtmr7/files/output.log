[34m[1mwandb[0m: Detected [huggingface_hub.inference] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
è®€å–: taigi-literature-abt/novels.json
è®€å–: taigi-literature-achiak/outputs.json
è®€å–: taigi-literature-asts/outputs.json
è®€å–: taigi-literature-khg/outputs.json
è®€å–: taigi-literature-kkh/novels.json
è®€å–: taigi-literature-ljk/preprocessed.json
è®€å–: taigi-literature-manlajo/output.json
è®€å–: taigi-literature-ngkh/outputs.json
è®€å–: taigi-literature-olbt/novels.json
è®€å–: taigi-literature-ots/novels.json
è®€å–: taigi-literature-pikh/outputs.json
è®€å–: taigi-literature-sslts/novels.json
è®€å–: taigi-literature-tks/novels.json
è®€å–: taigi-literature-tsk/new_poetry.json
è®€å–: taigi-literature-ttshs/outputs.json
ç¸½å…±è®€å–äº† 3433 ç­† CPT è³‡æ–™
å»é‡å¾Œ: 3431 ç­†

ç¯„ä¾‹æ–‡æœ¬ï¼ˆå‰3ç­†ï¼‰:

ç¬¬ 1 ç­† (é•·åº¦: 1017):
å„ä½é„‰è¦ªé€å®¶å¥½ï¼Œç›¸ä¿¡é€å®¶åŠ åŠ æ¸›æ¸›ç†Ÿä¼¼å’±æºªæ±æ‘çš„æ‘é•·ä¼¯ä»”ï¼Œæ¯‹å…æˆ‘åŠ ä»‹ç´¹ã€‚æ¯‹éï¼Œä¹Ÿå°±æ˜¯å› ç‚ºé€å®¶å°æ‘é•·ä¼¯å‚·éäº†è§£ï¼Œæ”æ‘é•·ä¼¯ä»”ã€æ‘é•·ä¼¯ä»”æŒ‰å‘¢å…±å«ï¼Œæ‰æœ‰æˆ‘å¯«é€™ä»½å‚³å–®çš„å¿…è¦ã€‚æ˜¯tihï¼Œæ‘é•·ä¼¯ä»”æ¬²å‡ºä¾†é¸æœ¬å±†çš„é„‰é•·å›‰ï¼Œå°‡ä¾†å’±å°±æ„›å«ä¼Šé„‰é•·ä¼¯lioohã€‚æ‘é•·ä¼¯ä»”ç”¨äººå”¯æ‰ï¼Œç›®è‰²çœŸå¥½ï¼Œé€™ä»¶ä»£èªŒçœ¾äººçŸ¥ã€‚æ‰€ä»¥æ‘é•·ä¼¯ä»”æ¯‹æ‰æœƒç•¶ä½‡å’±é€™ä¸ªåœ°æ–¹å¾›èµ·1é®æ¿Ÿå¹´ã€‚å°å¼Ÿä¸æ‰ï¼Œäºˆæ‘é•·ä¼¯ä»”ç›¸è‘—ï¼Œæ„›æˆ‘æ”‘ç­†å…±ä¼Šä¸€ç”Ÿçš„å‚³å¥‡å¯«è½ä¾†ã€‚è½‰ä¾†ç«¹æ‘é€™å¹¾å†¬ï¼Œæˆ‘è¹›...

ç¬¬ 2 ç­† (é•·åº¦: 987):
ã€Œé—œéµæ™‚åˆ»ã€ï¼ˆkuan-kiÄn-sÃ®-khikï¼‰ã€Œæµè¨€è¿½è¿½è¿½ã€é€™æ¬¾ç¯€ç›®ï¼Œå®šå®šåšé€™ä¸ªé˜¿èˆä¸€ç”Ÿèµ·èµ·è½è½çš„æ•…äº‹ï¼Œæ‹„å¥½äºˆæ‘é•·ä¼¯ä»”è½‰ï¼ˆ tsuÄnï¼‰å°æ™‚çœ‹è‘—ã€‚æ‘é•·ä¼¯ä»”ç™®çœ‹é€™æ¬¾ç¯€ç›®ä¾†äºˆè…¹å…§æœ‰åº•ã€‚çœ‹å®Œäº†å¾Œï¼Œä¼Šç…æ‡·å¿µå½¼ä¸ªä¼Šæ¯‹æŒçœ‹éçš„æ—¥æœ¬æ™‚ä»£çš„ä¾¿æ‰€ã€‚å®‰ä½‡æœ‰é­šæ± çš„åŸ•æ–—çš„å¯®ä»”å…§ï¼Œæ˜¯åŒçˆ¾æš¢ çš„ä¸€ä»¶ä»£èªŒï¼Œä¼ŠæŒ‰å‘¢ä¾†è©±ä»™ã€‚å¤æ—©æ™‚çš„æŸä¸€æ—¥ï¼Œ ç‚¸å½ˆå°å¤©é ‚è¼¾ï¼ˆ lÃ¬nï¼‰ è½ï¼Œ ä½‡æ°´é¾œåœ³çš„ç”°æºçˆ†ç‚¸ã€‚ç‚¸å½ˆçš®ä¸€å¡Šå½ˆè‘—åœ‹æ ¡çš„ç¦®å ‚ï¼Œä¸€å¡Šå½ˆè‘—...

ç¬¬ 3 ç­† (é•·åº¦: 1024):
è¦‹æ“ºé¸èˆ‰ï¼Œå¥¶æ¯ä¸€å®šæ”¯æŒæ‘é•·ä¼¯ï¼Œç„¡è«–æ˜¯é¸ä»£è¡¨ã€è¾²æœƒç†äº‹ï¼Œä¼Šæ”èªæ˜ å½¼ä¸ªè‡ªç´°çœ‹ç”²å¤§æ¼¢çš„é¢æ¨¡ä»”ï¼Œç„¡ç¬¬äºŒå¥è©±å°ä»”å°±å…±é “è½å»ã€‚é ‚å›å¥¶æ¯å»æˆå¤§ç—…é™¢é–‹åˆ€æ›é—œç¯€ï¼Œä¼Šå…±æ‘é•·ä¼¯ä»”æŠ•ï¼Œç—…é™¢ç«Ÿç„¶è¬›æ„›ç­‰å€‹å¤–æœˆæ‰æœƒå¾—é–‹åˆ€ï¼Œç­‰ç”²å½¼é™£å°±è¢‚è¡Œè·¯çŸ£å•¦ã€‚æ‘é•·ä¼¯ä»”å…±è­°å“¡è¬›ï¼Œè­°å“¡å…±åŠ©ç†è¬›ï¼ŒåŠ©ç†æ•²é›»è©±äºˆç—…é™¢ï¼Œå°±æŒ‰å‘¢ææ—©æ›ä¸€å‰¯å…¨æ–°çš„é—œç¯€è½‰ä¾†ã€‚è·” ï¼ˆ kuï¼‰è½å»ï¼Œé¦¬ä¸Šå°±æœƒç”¨å¾—ğ¬¦°èµ·ä¾†ã€‚æ‘é•·ä¼¯è¾¦ä»£èªŒï¼Œ è‹¥è¦ªåƒé€™å‰¯æ–°é»é»çš„é—œç¯€éçˆ¾ä»”æœ‰åŠ›ã€‚å¥¶æ¯é‚£è¬›é‚£æŒ‡...

æœ€çµ‚è¨“ç·´è³‡æ–™ç­†æ•¸: 3431
C:\Users\bente\miniconda3\envs\Ass4\Lib\site-packages\unsloth_zoo\gradient_checkpointing.py:348: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\c10/cuda/CUDAAllocatorConfig.h:35.)
  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f"{DEVICE_TYPE_TORCH}:{i}") for i in range(n_gpus)])
==((====))==  Unsloth 2025.11.6: Fast Llama patching. Transformers: 4.57.2.
   \\   /|    NVIDIA GeForce RTX 5050. Num GPUs = 1. Max memory: 7.96 GB. Platform: Windows.
O^O/ \_/ \    Torch: 2.9.1+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
model.safetensors.index.json: 23.9kB [00:00, 23.9MB/s]
model-00001-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4.98G/4.98G [00:44<00:00, 113MB/s]
model-00002-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4.92G/4.92G [00:43<00:00, 113MB/s]
model-00003-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.97G/3.97G [00:35<00:00, 113MB/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.22s/it]
generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187/187 [00:00<?, ?B/s]
tokenizer_config.json: 1.16kB [00:00, 592kB/s]
tokenizer.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 813k/813k [00:00<00:00, 1.64MB/s]
special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:00<?, ?B/s]
Unsloth: Will load Bohanlu/Taigi-Llama-2-7B as a legacy tokenizer.
Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.
Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.
Unsloth 2025.11.6 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.
Unsloth: Tokenizing ["text"] (num_proc=20):   0%|                                                                                               | 0/3431 [00:00<?, ? examples/s]
