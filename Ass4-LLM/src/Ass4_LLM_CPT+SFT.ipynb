{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAinTKkIOy_M"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KVdvMa-PGnz"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/NYCU/Ass4-LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !unzip ./data/IMA-Taiwan.zip -d ./data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 讀取模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import wandb\n",
        "\n",
        "wandb.login(key=\"6505e7e06b7f53ea56b61b94658f226c523ebacc\")\n",
        "# Start a new wandb run to track this script.\n",
        "cpt_run = wandb.init(\n",
        "    entity=\"paohuah-national-yang-ming-chiao-tung-university\",\n",
        "    project=\"Ass4-LLM\",\n",
        "    name=\"Taigi-CPT-V1\",\n",
        "    config={\n",
        "        \"stage\": \"CPT\",\n",
        "        \"model_name\": \"Bohanlu/Taigi-Llama-2-13B\",\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"max_steps\": 1000,\n",
        "        \"lora_r\": 64,\n",
        "        \"lora_alpha\": 128,\n",
        "    },\n",
        "    tags=[\"CPT\", \"domain_adaptation\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CPT訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 準備 CPT 資料（純文本）\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from datasets import Dataset\n",
        "\n",
        "def preprocess_taigi_text(text):\n",
        "    \"\"\"處理台文資料的預處理\"\"\"\n",
        "    \n",
        "    # 1. 移除 https/http 開頭的網址，直到遇到標點符號或空格\n",
        "    # 匹配到 )、。、，、空白 等符號為止\n",
        "    text = re.sub(r'https?://[^\\s)。，！？；：]+', '', text)\n",
        "    \n",
        "    # 2. 移除 www 開頭的網址片段\n",
        "    text = re.sub(r'www\\.[^\\s)。，！？；：]+', '', text)\n",
        "    \n",
        "    # 3. 移除殘留的域名片段（更寬鬆的匹配）\n",
        "    text = re.sub(r'\\b\\w+\\.(com|org|net|edu|gov|tw|io|co|info|biz)(/[^\\s)。，！？；：]*)?', '', text)\n",
        "    \n",
        "    # 4. 移除行首的段落編號（如：1. 2. 3.）\n",
        "    text = re.sub(r'^\\d+\\.\\s*', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 5. 統一標點符號\n",
        "    text = text.replace('。', '。')\n",
        "    text = text.replace('，', '，')\n",
        "    \n",
        "    # 6. 統一破折號\n",
        "    text = text.replace('—', '-')\n",
        "    \n",
        "    # 7. 移除過多的空白和換行\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 收集所有 JSON 檔案的資料\n",
        "all_cpt_texts = []\n",
        "\n",
        "# 設定最大文本長度（考慮 tokenizer 的限制）\n",
        "MAX_TEXT_LENGTH = 2048  # 字符數，不是 token 數\n",
        "\n",
        "for file_dir in os.listdir(\"./data/IMA-Taiwan\"):\n",
        "    dir_path = f\"./data/IMA-Taiwan/{file_dir}\"\n",
        "    \n",
        "    for file in os.listdir(dir_path):\n",
        "        if file.endswith(\".json\"):\n",
        "            file_path = os.path.join(dir_path, file)\n",
        "            print(f\"讀取: {file_dir}/{file}\")\n",
        "            \n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    json_data = json.load(f)\n",
        "                    \n",
        "                    # 如果是 list，需要先合併同一篇文章\n",
        "                    if isinstance(json_data, list):\n",
        "                        if json_data and 'title' in json_data[0]:\n",
        "                            from collections import defaultdict\n",
        "                            # 建立字典，key = title, value = 該文章的所有段落\n",
        "                            articles = defaultdict(list)\n",
        "                            \n",
        "                            for item in json_data:\n",
        "                                if 'text' in item and 'title' in item:\n",
        "                                    title = item['title']\n",
        "                                    articles[title].append(item['text'])\n",
        "                            \n",
        "                            # 處理每一篇文章（每個 title）\n",
        "                            for title, paragraphs in articles.items():\n",
        "                                # 合併同一篇文章的所有段落\n",
        "                                full_text = ''.join(paragraphs)\n",
        "                                \n",
        "                                # 一次性處理完整文章\n",
        "                                cleaned_text = preprocess_taigi_text(full_text)\n",
        "                                \n",
        "                                # ===== 處理過長的文本 =====\n",
        "                                if len(cleaned_text) >= 100:\n",
        "                                    # 如果文章太長，分段處理\n",
        "                                    if len(cleaned_text) > MAX_TEXT_LENGTH:\n",
        "                                        # 按句號切分\n",
        "                                        sentences = re.split(r'[。！？\\n]+', cleaned_text)\n",
        "                                        \n",
        "                                        current_chunk = \"\"\n",
        "                                        for sentence in sentences:\n",
        "                                            sentence = sentence.strip()\n",
        "                                            if not sentence:\n",
        "                                                continue\n",
        "                                            \n",
        "                                            # 如果加入這句話會超過限制，先保存當前 chunk\n",
        "                                            if len(current_chunk) + len(sentence) > MAX_TEXT_LENGTH:\n",
        "                                                if len(current_chunk) >= 100:\n",
        "                                                    all_cpt_texts.append({\"text\": current_chunk})\n",
        "                                                current_chunk = sentence + \"。\"\n",
        "                                            else:\n",
        "                                                current_chunk += sentence + \"。\"\n",
        "                                        \n",
        "                                        # 保存最後一個 chunk\n",
        "                                        if len(current_chunk) >= 100:\n",
        "                                            all_cpt_texts.append({\"text\": current_chunk})\n",
        "                                    else:\n",
        "                                        # 文章長度適中，直接加入\n",
        "                                        all_cpt_texts.append({\"text\": cleaned_text})\n",
        "                        \n",
        "                        # 如果沒有 title，每個元素獨立處理\n",
        "                        else:\n",
        "                            for item in json_data:\n",
        "                                if 'text' in item:\n",
        "                                    cleaned_text = preprocess_taigi_text(item['text'])\n",
        "                                    if 50 <= len(cleaned_text) <= MAX_TEXT_LENGTH:\n",
        "                                        all_cpt_texts.append({\"text\": cleaned_text})\n",
        "                    \n",
        "                    # 如果是 dict\n",
        "                    elif isinstance(json_data, dict):\n",
        "                        if 'text' in json_data:\n",
        "                            cleaned_text = preprocess_taigi_text(json_data['text'])\n",
        "                            if 50 <= len(cleaned_text) <= MAX_TEXT_LENGTH:\n",
        "                                all_cpt_texts.append({\"text\": cleaned_text})\n",
        "                            \n",
        "            except Exception as e:\n",
        "                print(f\"讀取 {file_path} 時發生錯誤: {e}\")\n",
        "\n",
        "print(f\"總共讀取了 {len(all_cpt_texts)} 筆 CPT 資料\")\n",
        "\n",
        "cpt_dataset = Dataset.from_list(all_cpt_texts)\n",
        "\n",
        "# 去重\n",
        "unique_texts = []\n",
        "seen = set()\n",
        "for item in all_cpt_texts:\n",
        "    text = item['text']\n",
        "    if text not in seen:\n",
        "        seen.add(text)\n",
        "        unique_texts.append(item)\n",
        "\n",
        "print(f\"去重後: {len(unique_texts)} 筆\")\n",
        "\n",
        "# 查看範例\n",
        "if unique_texts:\n",
        "    print(\"\\n範例文本（前3筆）:\")\n",
        "    for i, item in enumerate(unique_texts[:3]):\n",
        "        print(f\"\\n第 {i+1} 筆 (長度: {len(item['text'])}):\")\n",
        "        print(item['text'][:200] + \"...\" if len(item['text']) > 200 else item['text'])\n",
        "\n",
        "cpt_dataset = Dataset.from_list(unique_texts)\n",
        "print(f\"\\n最終訓練資料筆數: {len(cpt_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 階段 1: CPT - 持續預訓練 ==========\n",
        "import json\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 載入基礎模型\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Bohanlu/Taigi-Llama-2-13B\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# CPT 階段的 LoRA 配置（較大的 rank）\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64,  # CPT 用較大的 rank 學習更多知識\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 128,  # 對應調整 alpha\n",
        "    lora_dropout = 0.05,  # CPT 用較小的 dropout\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 9527,\n",
        ")\n",
        "\n",
        "# CPT 訓練配置\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "cpt_trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = cpt_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = True,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 100,\n",
        "        max_steps = 1000,  # CPT 需要更多步驟\n",
        "        learning_rate = 2e-4,  # CPT 用較高學習率\n",
        "        fp16 = False,\n",
        "        bf16 = True,\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 9527,\n",
        "        output_dir = \"outputs_cpt\",\n",
        "        report_to = \"wandb\",\n",
        "        run_name = \"Taigi-CPT\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 執行 CPT 訓練\n",
        "print(\"開始 CPT 階段訓練...\")\n",
        "cpt_trainer.train()\n",
        "\n",
        "# 儲存 CPT 模型\n",
        "model.save_pretrained(\"./model/cpt_model\")\n",
        "tokenizer.save_pretrained(\"./model/cpt_model\")\n",
        "print(\"CPT 階段完成！\")\n",
        "\n",
        "cpt_run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sft_run = wandb.init(\n",
        "    entity=\"paohuah-national-yang-ming-chiao-tung-university\",\n",
        "    project=\"Ass4-LLM\",\n",
        "    name=\"Taigi-SFT-V1\",\n",
        "    config={\n",
        "        \"stage\": \"SFT\",\n",
        "        \"base_model\": \"cpt_model\",\n",
        "        \"learning_rate\": 1e-5,\n",
        "        \"max_steps\": 100,\n",
        "        \"lora_r\": 16,\n",
        "        \"lora_alpha\": 32,\n",
        "    },\n",
        "    tags=[\"SFT\", \"qa_task\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ...existing code...\n",
        "# ========== 階段 2: SFT - 監督式微調 ==========\n",
        "\n",
        "# 載入 CPT 後的模型\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"./model/cpt_model\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# 先 merge LoRA weights 到 base model\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# 現在可以重新添加新的 LoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.1,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "# 準備 SFT 資料（有標註的問答對）\n",
        "df = pd.read_csv(\"./data/AI_conv.csv\")\n",
        "\n",
        "sft_dataset_data = []\n",
        "for _, row in df.iterrows():\n",
        "    # 結構化的問答格式\n",
        "    prompt = f\"根據前文內容回答問題\\n前文：{row['文章']}\\n問題：{row['問題']}\\n根據問題，從以下四個選項選出正確的選項編號(1-4)\\n選項1：{row['選項1']}\\n選項2：{row['選項2']}\\n選項3：{row['選項3']}\\n選項4：{row['選項4']}\\n答案：{str(row['正確答案'])}\"\n",
        "    \n",
        "    sft_dataset_data.append({\n",
        "        \"text\": prompt\n",
        "    })\n",
        "\n",
        "sft_dataset = Dataset.from_list(sft_dataset_data)\n",
        "\n",
        "# SFT 訓練配置\n",
        "sft_trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = sft_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = True,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        max_steps = 100,  # SFT 步驟較少\n",
        "        learning_rate = 1e-5,  # SFT 用較小學習率\n",
        "        fp16 = False,\n",
        "        bf16 = True,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 9527,\n",
        "        output_dir = \"outputs_sft\",\n",
        "        report_to = \"wandb\",\n",
        "        run_name = \"Taigi-SFT\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 執行 SFT 訓練\n",
        "print(\"開始 SFT 階段訓練...\")\n",
        "sft_trainer.train()\n",
        "\n",
        "# 儲存最終模型\n",
        "model.save_pretrained(\"./model/final_model\")\n",
        "tokenizer.save_pretrained(\"./model/final_model\")\n",
        "print(\"SFT 階段完成！\")\n",
        "\n",
        "sft_run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "# 清理 GPU 記憶體\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"./model/final_model\", # 你的微調模型路徑\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# 將模型設定為推理模式\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# 設定 tokenizer padding\n",
        "tokenizer.padding_side = 'left'\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 讀取測試資料並開始預測\n",
        "test_data = \"./data/1001-question-v3.csv\"\n",
        "output_dir = \"./data/output.csv\"\n",
        "\n",
        "# 如果輸出檔案已存在，先刪除以避免重複寫入\n",
        "if os.path.exists(output_dir):\n",
        "    os.remove(output_dir)\n",
        "\n",
        "test_df = pd.read_csv(test_data)\n",
        "# 初始化 write_header 變數\n",
        "write_header = True\n",
        "# 設定批次大小 (視顯卡記憶體大小調整，通常設 4, 8, 16, 32)\n",
        "batch_size = 4\n",
        "\n",
        "print(f\"開始預測，總筆數: {len(test_df)}，Batch Size: {batch_size}\")\n",
        "\n",
        "# 使用 range 每次跳 batch_size 的步長\n",
        "for i in range(0, len(test_df), batch_size):\n",
        "    # 取出目前的 batch 資料\n",
        "    batch_df = test_df.iloc[i : i + batch_size]\n",
        "    \n",
        "    prompts = []\n",
        "    ids = []\n",
        "    \n",
        "    # 準備這個 batch 的所有 Prompt\n",
        "    for index, raw in batch_df.iterrows():\n",
        "        question_background = raw['前文']\n",
        "        question = raw['題幹']\n",
        "        answer1 = raw['選項1']\n",
        "        answer2 = raw['選項2']\n",
        "        answer3 = raw['選項3']\n",
        "        answer4 = raw['選項4']\n",
        "        \n",
        "        # 使用與訓練時相同的格式\n",
        "        prompt = f\"你是一個專業的問答助手，請根據前文的背景，回答題目問題，只要選出正確的選項編號(1-4)。\\n前文：{question_background}\\n問題：{question}\\n從以下四個選項選出正確的選項編號\\n選項1：{answer1}\\n選項2：{answer2}\\n選項3：{answer3}\\n選項4：{answer4}\\n\"\n",
        "        \n",
        "        prompts.append(prompt)\n",
        "        ids.append(raw['ID'])\n",
        "    \n",
        "    # 批次 tokenize\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).to(\"cuda\")\n",
        "    \n",
        "    # 批次生成\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=10,  # 因為只需要生成數字，所以設小一點\n",
        "            do_sample=False,    # 使用 greedy decoding 確保結果一致\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True\n",
        "        )\n",
        "    \n",
        "    # 解碼生成的文字（只取新生成的部分）\n",
        "    predicted_texts = []\n",
        "    for j, output in enumerate(outputs):\n",
        "        input_length = inputs['input_ids'][j].shape[0]\n",
        "        generated_tokens = output[input_length:]\n",
        "        predicted_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "        # 移除所有特殊標記和多餘文字\n",
        "        import re\n",
        "        # 尋找第一個數字 1-4\n",
        "        match = re.search(r'^[1-4]', predicted_text)\n",
        "        if match:\n",
        "            clean_answer = match.group()\n",
        "        else:\n",
        "            # 如果沒找到，嘗試從整段文字中找\n",
        "            match = re.search(r'[1-4]', predicted_text)\n",
        "            clean_answer = match.group() if match else \"1\"  # 預設為1\n",
        "        \n",
        "        predicted_texts.append(clean_answer)\n",
        "\n",
        "    # 建立 Batch 的 DataFrame\n",
        "    output_batch = pd.DataFrame({\n",
        "        'ID': ids,\n",
        "        'Answer': predicted_texts\n",
        "    })\n",
        "    \n",
        "    # 寫入 CSV (append 模式)\n",
        "    output_batch.to_csv(output_dir, mode='a', header=write_header, index=False, encoding='utf-8-sig')\n",
        "    \n",
        "    # 第一次寫入後，之後都不需要 header\n",
        "    write_header = False\n",
        "    \n",
        "    print(f\"已處理: {min(i + batch_size, len(test_df))} / {len(test_df)}\")\n",
        "\n",
        "print(\"預測完成！\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
