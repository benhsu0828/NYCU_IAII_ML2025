"""
å°èª CPT è³‡æ–™é è™•ç†è…³æœ¬
åŠŸèƒ½ï¼šè®€å–åŸå§‹ JSON è³‡æ–™ â†’ æ¸…ç† â†’ å»é‡ â†’ å„²å­˜ç‚º parquet æ ¼å¼
ä½¿ç”¨æ–¹å¼ï¼špython preprocess_cpt_data.py
"""

import json
import os
import re
import gc
from datasets import Dataset
from tqdm import tqdm

def preprocess_taigi_text(text):
    """è™•ç†å°æ–‡è³‡æ–™çš„é è™•ç†"""
    # 1. ç§»é™¤ https/http é–‹é ­çš„ç¶²å€
    text = re.sub(r'https?://[^\s)ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]+', '', text)
    # 2. ç§»é™¤ www é–‹é ­çš„ç¶²å€ç‰‡æ®µ
    text = re.sub(r'www\.[^\s)ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]+', '', text)
    # 3. ç§»é™¤æ®˜ç•™çš„åŸŸåç‰‡æ®µ
    text = re.sub(r'\b\w+\.(com|org|net|edu|gov|tw|io|co|info|biz)(/[^\s)ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]*)?', '', text)
    # 4. ç§»é™¤è¡Œé¦–çš„æ®µè½ç·¨è™Ÿ
    text = re.sub(r'^\d+\.\s*', '', text, flags=re.MULTILINE)
    # 5. çµ±ä¸€æ¨™é»ç¬¦è™Ÿ
    text = text.replace('ã€‚', 'ã€‚').replace('ï¼Œ', 'ï¼Œ')
    # 6. çµ±ä¸€ç ´æŠ˜è™Ÿ
    text = text.replace('â€”', '-')
    # 7. ç§»é™¤éå¤šçš„ç©ºç™½å’Œæ›è¡Œ
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def process_cpt_data(input_dir, output_file, max_seq_length=1024):
    """
    è™•ç† CPT è³‡æ–™ä¸¦å„²å­˜
    
    Args:
        input_dir: è¼¸å…¥è³‡æ–™ç›®éŒ„ (IMA-Taiwan)
        output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ (.parquet æˆ– .jsonl)
        max_seq_length: æœ€å¤§åºåˆ—é•·åº¦
    """
    
    all_texts = []
    seen_hashes = set()
    
    # æª¢æŸ¥ç›®éŒ„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_dir):
        print(f"éŒ¯èª¤: ç›®éŒ„ {input_dir} ä¸å­˜åœ¨")
        return
    
    # è¨ˆç®—ç¸½æª”æ¡ˆæ•¸
    total_files = sum([len([f for f in os.listdir(os.path.join(input_dir, d)) if f.endswith('.json')]) 
                       for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))])
    
    print(f"é–‹å§‹è™•ç† {total_files} å€‹ JSON æª”æ¡ˆ...")
    
    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦
    with tqdm(total=total_files, desc="è™•ç†æª”æ¡ˆ") as pbar:
        for file_dir in os.listdir(input_dir):
            dir_path = os.path.join(input_dir, file_dir)
            if not os.path.isdir(dir_path):
                continue

            for file in os.listdir(dir_path):
                if not file.endswith(".json"):
                    continue
                    
                file_path = os.path.join(dir_path, file)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)
                        
                        # è™•ç†ä¸åŒçš„ JSON çµæ§‹
                        texts = process_json_structure(json_data, max_seq_length)
                        
                        # å»é‡ä¸¦åŠ å…¥
                        for text in texts:
                            text_hash = hash(text)
                            if text_hash not in seen_hashes and len(text) >= 100:
                                seen_hashes.add(text_hash)
                                all_texts.append({"text": text})
                        
                except Exception as e:
                    print(f"\nè®€å– {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                
                pbar.update(1)
    
    print(f"\nè™•ç†å®Œæˆï¼å…± {len(all_texts)} ç­†æœ‰æ•ˆè³‡æ–™")
    
    # è½‰æ›ç‚º Dataset ä¸¦å„²å­˜
    print("å»ºç«‹ Dataset...")
    dataset = Dataset.from_list(all_texts)
    
    # å„²å­˜ç‚º parquet æ ¼å¼ï¼ˆæ•ˆç‡æœ€é«˜ï¼‰
    if output_file.endswith('.parquet'):
        print(f"å„²å­˜ç‚º Parquet æ ¼å¼: {output_file}")
        dataset.to_parquet(output_file)
    # æˆ–å„²å­˜ç‚º JSONL æ ¼å¼ï¼ˆç›¸å®¹æ€§æœ€å¥½ï¼‰
    elif output_file.endswith('.jsonl'):
        print(f"å„²å­˜ç‚º JSONL æ ¼å¼: {output_file}")
        dataset.to_json(output_file)
    else:
        print("éŒ¯èª¤: è¼¸å‡ºæ ¼å¼å¿…é ˆæ˜¯ .parquet æˆ– .jsonl")
        return
    
    print(f"âœ… è³‡æ–™å·²å„²å­˜è‡³: {output_file}")
    print(f"ğŸ“Š è³‡æ–™çµ±è¨ˆ:")
    print(f"  - ç¸½ç­†æ•¸: {len(dataset)}")
    print(f"  - æª”æ¡ˆå¤§å°: {os.path.getsize(output_file) / 1024 / 1024:.2f} MB")
    
    # é¡¯ç¤ºå‰ 3 ç­†ç¯„ä¾‹
    print("\nå‰ 3 ç­†è³‡æ–™ç¯„ä¾‹:")
    for i, example in enumerate(dataset.select(range(min(3, len(dataset))))):
        print(f"\n[{i+1}] é•·åº¦: {len(example['text'])} å­—")
        print(f"å…§å®¹é è¦½: {example['text'][:150]}...")
    
    del all_texts, seen_hashes
    gc.collect()


def process_json_structure(json_data, max_seq_length):
    """è™•ç†ä¸åŒçµæ§‹çš„ JSON è³‡æ–™"""
    texts = []
    
    if isinstance(json_data, list):
        # è™•ç†æœ‰ title çš„æ–‡ç« åˆ—è¡¨
        if json_data and 'title' in json_data[0]:
            from collections import defaultdict
            articles = defaultdict(list)
            
            for item in json_data:
                if 'text' in item and 'title' in item:
                    articles[item['title']].append(item['text'])
            
            for title, paragraphs in articles.items():
                full_text = ''.join(paragraphs)
                cleaned_text = preprocess_taigi_text(full_text)
                
                if len(cleaned_text) >= 100:
                    # åˆ‡åˆ†éé•·æ–‡æœ¬
                    if len(cleaned_text) > max_seq_length:
                        chunks = split_long_text(cleaned_text, max_seq_length)
                        texts.extend(chunks)
                    else:
                        texts.append(cleaned_text)
        
        # è™•ç†ä¸€èˆ¬åˆ—è¡¨
        else:
            for item in json_data:
                if 'text' in item:
                    cleaned_text = preprocess_taigi_text(item['text'])
                    if 50 <= len(cleaned_text) <= max_seq_length:
                        texts.append(cleaned_text)
    
    elif isinstance(json_data, dict):
        if 'text' in json_data:
            cleaned_text = preprocess_taigi_text(json_data['text'])
            if 50 <= len(cleaned_text) <= max_seq_length:
                texts.append(cleaned_text)
    
    return texts


def split_long_text(text, max_length):
    """åˆ‡åˆ†éé•·çš„æ–‡æœ¬"""
    chunks = []
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        if len(current_chunk) + len(sentence) > max_length:
            if len(current_chunk) >= 100:
                chunks.append(current_chunk)
            current_chunk = sentence + "ã€‚"
        else:
            current_chunk += sentence + "ã€‚"
    
    if len(current_chunk) >= 100:
        chunks.append(current_chunk)
    
    return chunks


if __name__ == "__main__":
    # è¨­å®šè·¯å¾‘
    INPUT_DIR = "../data/IMA-Taiwan"
    OUTPUT_FILE = "../data/cpt_dataset.parquet"  # æˆ– .jsonl
    MAX_SEQ_LENGTH = 1024
    
    print("=" * 60)
    print("å°èª CPT è³‡æ–™é è™•ç†")
    print("=" * 60)
    
    # åŸ·è¡Œè™•ç†
    process_cpt_data(INPUT_DIR, OUTPUT_FILE, MAX_SEQ_LENGTH)
    
    print("\nâœ… è™•ç†å®Œæˆï¼")
    print(f"ä¸‹æ¬¡è¨“ç·´æ™‚ï¼Œç›´æ¥è¼‰å…¥: {OUTPUT_FILE}")
