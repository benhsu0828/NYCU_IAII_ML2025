{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAinTKkIOy_M"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KVdvMa-PGnz"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/NYCU/Ass4-LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 讀取模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# 載入模型和 tokenizer\n",
        "max_seq_length = 2048 # 根據你的需要調整\n",
        "dtype = None # None 為自動偵測，Float16 用於 Tesla T4, V100, Bfloat16 用於 Ampere+\n",
        "load_in_4bit = True # 使用 4bit 量化以減少 VRAM 使用\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Bohanlu/Taigi-Llama-2-13B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 添加 LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # 選擇任何大於 0 的數字！建議使用 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.1, # 支援任何值，但 0 是最佳化的\n",
        "    bias = \"none\",    # 支援任何值，但 \"none\" 是最佳化的\n",
        "    use_gradient_checkpointing = \"unsloth\", # True 或 \"unsloth\" 用於非常長的上下文\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # 我們支援 rank stabilized LoRA\n",
        "    loftq_config = None, # 以及 LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 讀取你已經準備好的訓練資料\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 讀取訓練資料\n",
        "df = pd.read_csv(\"./data/AI_conv.csv\")\n",
        "\n",
        "# 準備 Unsloth 格式的資料\n",
        "dataset_data = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    prompt = f\"前文：{row['文章']}\\n問題：{row['問題']}\\n從以下四個選項選出正確的選項編號(1-4)\\n選項1：{row['選項1']}\\n選項2：{row['選項2']}\\n選項3：{row['選項3']}\\n選項4：{row['選項4']}\\n答案：{str(row['正確答案'])}\"\n",
        "    \n",
        "    dataset_data.append({\n",
        "        \"text\": prompt\n",
        "    })\n",
        "\n",
        "# 轉換為 Dataset 格式\n",
        "dataset = Dataset.from_list(dataset_data)\n",
        "print(f\"訓練資料筆數: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.login(key=\"6505e7e06b7f53ea56b61b94658f226c523ebacc\")\n",
        "# Start a new wandb run to track this script.\n",
        "run = wandb.init(\n",
        "    entity=\"paohuah-national-yang-ming-chiao-tung-university\",\n",
        "    project=\"Ass4-LLM\",\n",
        "    name=\"LLM-fine-tune-V2\",\n",
        "    config={\n",
        "        \"model_name\": \"Bohanlu/Taigi-Llama-2-13B\",\n",
        "        \"max_seq_length\": max_seq_length,\n",
        "        \"batch_size\": 2,\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"max_steps\": 60,\n",
        "        \"lora_r\": 16,\n",
        "        \"lora_alpha\": 16,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = True, # 可以讓短序列的訓練速度提升 5 倍！\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 2,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60, # 根據你的資料量調整\n",
        "        learning_rate = 5e-5,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 9527,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to=\"wandb\",  # 添加這行來啟用 wandb 記錄\n",
        "        run_name=\"LLM-fine-tune-V1\",  # 添加運行名稱\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 顯示訓練前的記憶體統計\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "# 開始訓練\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 儲存 LoRA 模型\n",
        "model.save_pretrained(\"./model/lora_model\")\n",
        "tokenizer.save_pretrained(\"./model/lora_model\")\n",
        "\n",
        "# # 如果你想要儲存完整模型（16bit）\n",
        "# model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "\n",
        "# 如果你想要儲存為 GGUF 格式以便後續使用\n",
        "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"./model/lora_model\", # 你的微調模型路徑\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 將模型設定為推理模式\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# 設定 tokenizer padding\n",
        "tokenizer.padding_side = 'left'\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 讀取測試資料並開始預測\n",
        "test_data = \"./data/1001-question-v3.csv\"\n",
        "output_dir = \"./data/output.csv\"\n",
        "\n",
        "# 如果輸出檔案已存在，先刪除以避免重複寫入\n",
        "if os.path.exists(output_dir):\n",
        "    os.remove(output_dir)\n",
        "\n",
        "test_df = pd.read_csv(test_data)\n",
        "# 初始化 write_header 變數\n",
        "write_header = True\n",
        "# 設定批次大小 (視顯卡記憶體大小調整，通常設 4, 8, 16, 32)\n",
        "batch_size = 8\n",
        "\n",
        "print(f\"開始預測，總筆數: {len(test_df)}，Batch Size: {batch_size}\")\n",
        "\n",
        "# 使用 range 每次跳 batch_size 的步長\n",
        "for i in range(0, len(test_df), batch_size):\n",
        "    # 取出目前的 batch 資料\n",
        "    batch_df = test_df.iloc[i : i + batch_size]\n",
        "    \n",
        "    prompts = []\n",
        "    ids = []\n",
        "    \n",
        "    # 準備這個 batch 的所有 Prompt\n",
        "    for index, raw in batch_df.iterrows():\n",
        "        question_background = raw['前文']\n",
        "        question = raw['題幹']\n",
        "        answer1 = raw['選項1']\n",
        "        answer2 = raw['選項2']\n",
        "        answer3 = raw['選項3']\n",
        "        answer4 = raw['選項4']\n",
        "        \n",
        "        # 使用與訓練時相同的格式\n",
        "        prompt = f\"你是一個專業的問答助手，請根據前文的背景，回答題目問題，只要選出正確的選項編號(1-4)。\\n前文：{question_background}\\n問題：{question}\\n從以下四個選項選出正確的選項編號\\n選項1：{answer1}\\n選項2：{answer2}\\n選項3：{answer3}\\n選項4：{answer4}\\n\"\n",
        "        \n",
        "        prompts.append(prompt)\n",
        "        ids.append(raw['ID'])\n",
        "    \n",
        "    # 批次 tokenize\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length).to(\"cuda\")\n",
        "    \n",
        "    # 批次生成\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=10,  # 因為只需要生成數字，所以設小一點\n",
        "            do_sample=False,    # 使用 greedy decoding 確保結果一致\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True\n",
        "        )\n",
        "    \n",
        "    # 解碼生成的文字（只取新生成的部分）\n",
        "    predicted_texts = []\n",
        "    for j, output in enumerate(outputs):\n",
        "        input_length = inputs['input_ids'][j].shape[0]\n",
        "        generated_tokens = output[input_length:]\n",
        "        predicted_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "        # 移除所有特殊標記和多餘文字\n",
        "        import re\n",
        "        # 尋找第一個數字 1-4\n",
        "        match = re.search(r'^[1-4]', predicted_text)\n",
        "        if match:\n",
        "            clean_answer = match.group()\n",
        "        else:\n",
        "            # 如果沒找到，嘗試從整段文字中找\n",
        "            match = re.search(r'[1-4]', predicted_text)\n",
        "            clean_answer = match.group() if match else \"1\"  # 預設為1\n",
        "        \n",
        "        predicted_texts.append(clean_answer)\n",
        "\n",
        "    # 建立 Batch 的 DataFrame\n",
        "    output_batch = pd.DataFrame({\n",
        "        'ID': ids,\n",
        "        'Answer': predicted_texts\n",
        "    })\n",
        "    \n",
        "    # 寫入 CSV (append 模式)\n",
        "    output_batch.to_csv(output_dir, mode='a', header=write_header, index=False, encoding='utf-8-sig')\n",
        "    \n",
        "    # 第一次寫入後，之後都不需要 header\n",
        "    write_header = False\n",
        "    \n",
        "    print(f\"已處理: {min(i + batch_size, len(test_df))} / {len(test_df)}\")\n",
        "\n",
        "print(\"預測完成！\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
