{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.5904059040590406,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005904059040590406,
      "grad_norm": 0.42034780979156494,
      "learning_rate": 9.000000000000001e-07,
      "loss": 3.2724,
      "step": 10
    },
    {
      "epoch": 0.011808118081180811,
      "grad_norm": 0.5239431858062744,
      "learning_rate": 1.9000000000000002e-06,
      "loss": 3.3294,
      "step": 20
    },
    {
      "epoch": 0.017712177121771217,
      "grad_norm": 0.4345778226852417,
      "learning_rate": 2.9e-06,
      "loss": 3.2316,
      "step": 30
    },
    {
      "epoch": 0.023616236162361623,
      "grad_norm": 0.4283798336982727,
      "learning_rate": 3.900000000000001e-06,
      "loss": 3.25,
      "step": 40
    },
    {
      "epoch": 0.02952029520295203,
      "grad_norm": 0.47423413395881653,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 3.2741,
      "step": 50
    },
    {
      "epoch": 0.035424354243542434,
      "grad_norm": 0.5516859292984009,
      "learning_rate": 5.9e-06,
      "loss": 3.3008,
      "step": 60
    },
    {
      "epoch": 0.041328413284132844,
      "grad_norm": 0.5349335670471191,
      "learning_rate": 6.9e-06,
      "loss": 3.2529,
      "step": 70
    },
    {
      "epoch": 0.047232472324723246,
      "grad_norm": 0.6545725464820862,
      "learning_rate": 7.9e-06,
      "loss": 3.2971,
      "step": 80
    },
    {
      "epoch": 0.053136531365313655,
      "grad_norm": 0.601364254951477,
      "learning_rate": 8.900000000000001e-06,
      "loss": 3.2552,
      "step": 90
    },
    {
      "epoch": 0.05904059040590406,
      "grad_norm": 0.5768705606460571,
      "learning_rate": 9.9e-06,
      "loss": 3.3,
      "step": 100
    },
    {
      "epoch": 0.06494464944649446,
      "grad_norm": 0.5927848219871521,
      "learning_rate": 9.997532801828659e-06,
      "loss": 3.1731,
      "step": 110
    },
    {
      "epoch": 0.07084870848708487,
      "grad_norm": 0.7083469033241272,
      "learning_rate": 9.989007341460251e-06,
      "loss": 3.1734,
      "step": 120
    },
    {
      "epoch": 0.07675276752767528,
      "grad_norm": 0.7205077409744263,
      "learning_rate": 9.974403544143942e-06,
      "loss": 3.1797,
      "step": 130
    },
    {
      "epoch": 0.08265682656826569,
      "grad_norm": 0.6518892645835876,
      "learning_rate": 9.953739202357219e-06,
      "loss": 3.1453,
      "step": 140
    },
    {
      "epoch": 0.08856088560885608,
      "grad_norm": 0.7850819826126099,
      "learning_rate": 9.927039492417452e-06,
      "loss": 3.1849,
      "step": 150
    },
    {
      "epoch": 0.09446494464944649,
      "grad_norm": 0.8964515924453735,
      "learning_rate": 9.894336943808426e-06,
      "loss": 3.0804,
      "step": 160
    },
    {
      "epoch": 0.1003690036900369,
      "grad_norm": 0.7181990146636963,
      "learning_rate": 9.85567139954818e-06,
      "loss": 3.1049,
      "step": 170
    },
    {
      "epoch": 0.10627306273062731,
      "grad_norm": 0.8330034017562866,
      "learning_rate": 9.811089967646427e-06,
      "loss": 3.1395,
      "step": 180
    },
    {
      "epoch": 0.1121771217712177,
      "grad_norm": 0.8431456685066223,
      "learning_rate": 9.760646963710694e-06,
      "loss": 3.0228,
      "step": 190
    },
    {
      "epoch": 0.11808118081180811,
      "grad_norm": 0.9530393481254578,
      "learning_rate": 9.704403844771128e-06,
      "loss": 3.0036,
      "step": 200
    },
    {
      "epoch": 0.12398523985239852,
      "grad_norm": 0.8726267218589783,
      "learning_rate": 9.642429134404568e-06,
      "loss": 3.0343,
      "step": 210
    },
    {
      "epoch": 0.12988929889298892,
      "grad_norm": 0.7964041829109192,
      "learning_rate": 9.574798339249124e-06,
      "loss": 2.9492,
      "step": 220
    },
    {
      "epoch": 0.13579335793357933,
      "grad_norm": 0.7904793620109558,
      "learning_rate": 9.501593857010968e-06,
      "loss": 2.9326,
      "step": 230
    },
    {
      "epoch": 0.14169741697416974,
      "grad_norm": 0.8444026708602905,
      "learning_rate": 9.42290487607542e-06,
      "loss": 3.0037,
      "step": 240
    },
    {
      "epoch": 0.14760147601476015,
      "grad_norm": 0.9012743234634399,
      "learning_rate": 9.338827266844643e-06,
      "loss": 2.9705,
      "step": 250
    },
    {
      "epoch": 0.15350553505535056,
      "grad_norm": 0.8311192989349365,
      "learning_rate": 9.24946346493432e-06,
      "loss": 2.9855,
      "step": 260
    },
    {
      "epoch": 0.15940959409594097,
      "grad_norm": 0.7561183571815491,
      "learning_rate": 9.154922346371641e-06,
      "loss": 2.8956,
      "step": 270
    },
    {
      "epoch": 0.16531365313653137,
      "grad_norm": 0.8239404559135437,
      "learning_rate": 9.055319094946633e-06,
      "loss": 2.9564,
      "step": 280
    },
    {
      "epoch": 0.17121771217712178,
      "grad_norm": 0.9069269299507141,
      "learning_rate": 8.950775061878453e-06,
      "loss": 2.94,
      "step": 290
    },
    {
      "epoch": 0.17712177121771217,
      "grad_norm": 0.6898872256278992,
      "learning_rate": 8.841417617967618e-06,
      "loss": 2.8557,
      "step": 300
    },
    {
      "epoch": 0.18302583025830257,
      "grad_norm": 0.7615789771080017,
      "learning_rate": 8.727379998414311e-06,
      "loss": 2.8283,
      "step": 310
    },
    {
      "epoch": 0.18892988929889298,
      "grad_norm": 0.8132147789001465,
      "learning_rate": 8.608801140491811e-06,
      "loss": 2.872,
      "step": 320
    },
    {
      "epoch": 0.1948339483394834,
      "grad_norm": 0.8648617267608643,
      "learning_rate": 8.485825514272824e-06,
      "loss": 2.867,
      "step": 330
    },
    {
      "epoch": 0.2007380073800738,
      "grad_norm": 0.8318236470222473,
      "learning_rate": 8.358602946614952e-06,
      "loss": 2.9185,
      "step": 340
    },
    {
      "epoch": 0.2066420664206642,
      "grad_norm": 0.9805688858032227,
      "learning_rate": 8.227288438619754e-06,
      "loss": 2.8605,
      "step": 350
    },
    {
      "epoch": 0.21254612546125462,
      "grad_norm": 0.8330485820770264,
      "learning_rate": 8.092041976787772e-06,
      "loss": 2.8615,
      "step": 360
    },
    {
      "epoch": 0.21845018450184503,
      "grad_norm": 0.8854000568389893,
      "learning_rate": 7.953028338099628e-06,
      "loss": 2.8936,
      "step": 370
    },
    {
      "epoch": 0.2243542435424354,
      "grad_norm": 0.9416084289550781,
      "learning_rate": 7.810416889260653e-06,
      "loss": 2.87,
      "step": 380
    },
    {
      "epoch": 0.23025830258302582,
      "grad_norm": 0.8693863153457642,
      "learning_rate": 7.66438138035365e-06,
      "loss": 2.8705,
      "step": 390
    },
    {
      "epoch": 0.23616236162361623,
      "grad_norm": 1.072405219078064,
      "learning_rate": 7.515099733151177e-06,
      "loss": 2.8181,
      "step": 400
    },
    {
      "epoch": 0.24206642066420664,
      "grad_norm": 0.9055215716362,
      "learning_rate": 7.362753824345271e-06,
      "loss": 2.882,
      "step": 410
    },
    {
      "epoch": 0.24797047970479705,
      "grad_norm": 0.8339914083480835,
      "learning_rate": 7.207529263958727e-06,
      "loss": 2.7559,
      "step": 420
    },
    {
      "epoch": 0.25387453874538746,
      "grad_norm": 0.945785641670227,
      "learning_rate": 7.049615169207864e-06,
      "loss": 2.8021,
      "step": 430
    },
    {
      "epoch": 0.25977859778597784,
      "grad_norm": 0.9488581418991089,
      "learning_rate": 6.889203934092337e-06,
      "loss": 2.8497,
      "step": 440
    },
    {
      "epoch": 0.2656826568265683,
      "grad_norm": 0.9324724674224854,
      "learning_rate": 6.7264909949926735e-06,
      "loss": 2.8202,
      "step": 450
    },
    {
      "epoch": 0.27158671586715866,
      "grad_norm": 0.8777108192443848,
      "learning_rate": 6.561674592561164e-06,
      "loss": 2.748,
      "step": 460
    },
    {
      "epoch": 0.2774907749077491,
      "grad_norm": 1.2900238037109375,
      "learning_rate": 6.3949555301961474e-06,
      "loss": 2.8208,
      "step": 470
    },
    {
      "epoch": 0.2833948339483395,
      "grad_norm": 1.090032696723938,
      "learning_rate": 6.2265369293940135e-06,
      "loss": 2.8891,
      "step": 480
    },
    {
      "epoch": 0.2892988929889299,
      "grad_norm": 1.2377632856369019,
      "learning_rate": 6.056623982276945e-06,
      "loss": 2.8476,
      "step": 490
    },
    {
      "epoch": 0.2952029520295203,
      "grad_norm": 1.0649234056472778,
      "learning_rate": 5.885423701597918e-06,
      "loss": 2.839,
      "step": 500
    },
    {
      "epoch": 0.3011070110701107,
      "grad_norm": 1.02595055103302,
      "learning_rate": 5.7131446685275595e-06,
      "loss": 2.8191,
      "step": 510
    },
    {
      "epoch": 0.3070110701107011,
      "grad_norm": 1.068042278289795,
      "learning_rate": 5.539996778530114e-06,
      "loss": 2.7475,
      "step": 520
    },
    {
      "epoch": 0.3129151291512915,
      "grad_norm": 1.4668890237808228,
      "learning_rate": 5.366190985638159e-06,
      "loss": 2.7804,
      "step": 530
    },
    {
      "epoch": 0.31881918819188193,
      "grad_norm": 0.963041365146637,
      "learning_rate": 5.1919390454376e-06,
      "loss": 2.8289,
      "step": 540
    },
    {
      "epoch": 0.3247232472324723,
      "grad_norm": 0.884363055229187,
      "learning_rate": 5.0174532570761194e-06,
      "loss": 2.7029,
      "step": 550
    },
    {
      "epoch": 0.33062730627306275,
      "grad_norm": 1.0488829612731934,
      "learning_rate": 4.842946204609359e-06,
      "loss": 2.8567,
      "step": 560
    },
    {
      "epoch": 0.33653136531365313,
      "grad_norm": 1.2312426567077637,
      "learning_rate": 4.668630498000001e-06,
      "loss": 2.7569,
      "step": 570
    },
    {
      "epoch": 0.34243542435424357,
      "grad_norm": 1.0559073686599731,
      "learning_rate": 4.494718514085269e-06,
      "loss": 2.7773,
      "step": 580
    },
    {
      "epoch": 0.34833948339483395,
      "grad_norm": 0.9875098466873169,
      "learning_rate": 4.321422137828479e-06,
      "loss": 2.8188,
      "step": 590
    },
    {
      "epoch": 0.35424354243542433,
      "grad_norm": 1.0936897993087769,
      "learning_rate": 4.148952504169839e-06,
      "loss": 2.8056,
      "step": 600
    },
    {
      "epoch": 0.36014760147601477,
      "grad_norm": 1.0099458694458008,
      "learning_rate": 3.977519740791049e-06,
      "loss": 2.8056,
      "step": 610
    },
    {
      "epoch": 0.36605166051660515,
      "grad_norm": 1.1982539892196655,
      "learning_rate": 3.8073327121070968e-06,
      "loss": 2.8353,
      "step": 620
    },
    {
      "epoch": 0.3719557195571956,
      "grad_norm": 1.1362451314926147,
      "learning_rate": 3.6385987647971287e-06,
      "loss": 2.7459,
      "step": 630
    },
    {
      "epoch": 0.37785977859778597,
      "grad_norm": 1.2027746438980103,
      "learning_rate": 3.471523475184472e-06,
      "loss": 2.7644,
      "step": 640
    },
    {
      "epoch": 0.3837638376383764,
      "grad_norm": 0.9706839919090271,
      "learning_rate": 3.3063103987735433e-06,
      "loss": 2.764,
      "step": 650
    },
    {
      "epoch": 0.3896678966789668,
      "grad_norm": 1.0017446279525757,
      "learning_rate": 3.1431608222488276e-06,
      "loss": 2.7955,
      "step": 660
    },
    {
      "epoch": 0.3955719557195572,
      "grad_norm": 1.0149141550064087,
      "learning_rate": 2.98227351823805e-06,
      "loss": 2.7334,
      "step": 670
    },
    {
      "epoch": 0.4014760147601476,
      "grad_norm": 1.0830539464950562,
      "learning_rate": 2.8238445031383634e-06,
      "loss": 2.7392,
      "step": 680
    },
    {
      "epoch": 0.407380073800738,
      "grad_norm": 0.9743989706039429,
      "learning_rate": 2.6680667983005446e-06,
      "loss": 2.8258,
      "step": 690
    },
    {
      "epoch": 0.4132841328413284,
      "grad_norm": 1.077612280845642,
      "learning_rate": 2.5151301948622235e-06,
      "loss": 2.8193,
      "step": 700
    },
    {
      "epoch": 0.4191881918819188,
      "grad_norm": 1.080512523651123,
      "learning_rate": 2.3652210225166122e-06,
      "loss": 2.7971,
      "step": 710
    },
    {
      "epoch": 0.42509225092250924,
      "grad_norm": 1.0084196329116821,
      "learning_rate": 2.218521922498476e-06,
      "loss": 2.7727,
      "step": 720
    },
    {
      "epoch": 0.4309963099630996,
      "grad_norm": 1.2735198736190796,
      "learning_rate": 2.075211625063923e-06,
      "loss": 2.7779,
      "step": 730
    },
    {
      "epoch": 0.43690036900369006,
      "grad_norm": 0.9445642232894897,
      "learning_rate": 1.9354647317351187e-06,
      "loss": 2.7636,
      "step": 740
    },
    {
      "epoch": 0.44280442804428044,
      "grad_norm": 1.065163016319275,
      "learning_rate": 1.799451502575222e-06,
      "loss": 2.7255,
      "step": 750
    },
    {
      "epoch": 0.4487084870848708,
      "grad_norm": 0.9861858487129211,
      "learning_rate": 1.6673376487527382e-06,
      "loss": 2.7804,
      "step": 760
    },
    {
      "epoch": 0.45461254612546126,
      "grad_norm": 1.0838457345962524,
      "learning_rate": 1.5392841306479667e-06,
      "loss": 2.7163,
      "step": 770
    },
    {
      "epoch": 0.46051660516605164,
      "grad_norm": 1.1417193412780762,
      "learning_rate": 1.4154469617475864e-06,
      "loss": 2.831,
      "step": 780
    },
    {
      "epoch": 0.4664206642066421,
      "grad_norm": 1.2676067352294922,
      "learning_rate": 1.2959770185662502e-06,
      "loss": 2.817,
      "step": 790
    },
    {
      "epoch": 0.47232472324723246,
      "grad_norm": 1.0962598323822021,
      "learning_rate": 1.1810198568267906e-06,
      "loss": 2.8288,
      "step": 800
    },
    {
      "epoch": 0.4782287822878229,
      "grad_norm": 1.308537483215332,
      "learning_rate": 1.0707155341229902e-06,
      "loss": 2.7818,
      "step": 810
    },
    {
      "epoch": 0.4841328413284133,
      "grad_norm": 1.1848556995391846,
      "learning_rate": 9.651984392809916e-07,
      "loss": 2.7575,
      "step": 820
    },
    {
      "epoch": 0.4900369003690037,
      "grad_norm": 0.9938908219337463,
      "learning_rate": 8.645971286271903e-07,
      "loss": 2.7433,
      "step": 830
    },
    {
      "epoch": 0.4959409594095941,
      "grad_norm": 1.03016197681427,
      "learning_rate": 7.690341693621805e-07,
      "loss": 2.7393,
      "step": 840
    },
    {
      "epoch": 0.5018450184501845,
      "grad_norm": 0.978121280670166,
      "learning_rate": 6.786259902314768e-07,
      "loss": 2.727,
      "step": 850
    },
    {
      "epoch": 0.5077490774907749,
      "grad_norm": 1.2946912050247192,
      "learning_rate": 5.934827396750392e-07,
      "loss": 2.7169,
      "step": 860
    },
    {
      "epoch": 0.5136531365313654,
      "grad_norm": 1.49881911277771,
      "learning_rate": 5.137081516283582e-07,
      "loss": 2.8062,
      "step": 870
    },
    {
      "epoch": 0.5195571955719557,
      "grad_norm": 1.179948329925537,
      "learning_rate": 4.3939941913863525e-07,
      "loss": 2.7906,
      "step": 880
    },
    {
      "epoch": 0.5254612546125461,
      "grad_norm": 1.1380518674850464,
      "learning_rate": 3.7064707595002636e-07,
      "loss": 2.8075,
      "step": 890
    },
    {
      "epoch": 0.5313653136531366,
      "grad_norm": 1.2441903352737427,
      "learning_rate": 3.0753488620222037e-07,
      "loss": 2.7273,
      "step": 900
    },
    {
      "epoch": 0.537269372693727,
      "grad_norm": 1.2204400300979614,
      "learning_rate": 2.5013974237673824e-07,
      "loss": 2.7929,
      "step": 910
    },
    {
      "epoch": 0.5431734317343173,
      "grad_norm": 1.1648929119110107,
      "learning_rate": 1.9853157161528468e-07,
      "loss": 2.772,
      "step": 920
    },
    {
      "epoch": 0.5490774907749078,
      "grad_norm": 1.1336748600006104,
      "learning_rate": 1.5277325052430569e-07,
      "loss": 2.7699,
      "step": 930
    },
    {
      "epoch": 0.5549815498154982,
      "grad_norm": 0.9927330017089844,
      "learning_rate": 1.1292052856952063e-07,
      "loss": 2.788,
      "step": 940
    },
    {
      "epoch": 0.5608856088560885,
      "grad_norm": 1.277382493019104,
      "learning_rate": 7.90219601537906e-08,
      "loss": 2.7801,
      "step": 950
    },
    {
      "epoch": 0.566789667896679,
      "grad_norm": 1.085891842842102,
      "learning_rate": 5.111884546105506e-08,
      "loss": 2.773,
      "step": 960
    },
    {
      "epoch": 0.5726937269372694,
      "grad_norm": 1.1369402408599854,
      "learning_rate": 2.9245180138423033e-08,
      "loss": 2.7262,
      "step": 970
    },
    {
      "epoch": 0.5785977859778598,
      "grad_norm": 1.072426676750183,
      "learning_rate": 1.3427613877709523e-08,
      "loss": 2.735,
      "step": 980
    },
    {
      "epoch": 0.5845018450184502,
      "grad_norm": 0.9530848264694214,
      "learning_rate": 3.685417946894254e-09,
      "loss": 2.7631,
      "step": 990
    },
    {
      "epoch": 0.5904059040590406,
      "grad_norm": 1.2807438373565674,
      "learning_rate": 3.0461711048035415e-11,
      "loss": 2.7449,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.342005179158528e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
