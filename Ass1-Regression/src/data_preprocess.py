import pandas as pd
import os
from pathlib import Path
from sklearn.preprocessing import LabelEncoder
import re
import numpy as np

def load_data():
    """
    è®€å– train, valid, test ä¸‰å€‹ Excel æª”æ¡ˆ
    è¿”å›: train_df, valid_df, test_df
    """
    # å–å¾—ä¸Šç´šç›®éŒ„çš„ Dataset/raw/ è·¯å¾‘
    current_dir = Path(os.getcwd())
    data_dir = current_dir.parent / "Dataset" / "raw"
    
    print(f"è³‡æ–™ç›®éŒ„: {data_dir}")
    # è®€å–ä¸‰å€‹æª”æ¡ˆ
    train_path = data_dir / "train-v2.xlsx"
    valid_path = data_dir / "valid-v2.xlsx" 
    test_path = data_dir / "test-reindex-test-v2.1.xlsx"
    
    print(f"è®€å–è¨“ç·´è³‡æ–™: {train_path}")
    train_df = pd.read_excel(train_path)
    
    print(f"è®€å–é©—è­‰è³‡æ–™: {valid_path}")
    valid_df = pd.read_excel(valid_path)
    
    print(f"è®€å–æ¸¬è©¦è³‡æ–™: {test_path}")
    test_df = pd.read_excel(test_path)
    
    print(f"Train shape: {train_df.shape}")
    print(f"Valid shape: {valid_df.shape}")
    print(f"Test shape: {test_df.shape}")
    
    return train_df, valid_df, test_df

def drop_columns(df, columns_to_drop):
    """
    åˆªé™¤æŒ‡å®šæ¬„ä½
    
    Args:
        df: DataFrame
        columns_to_drop: list of column names to drop
    
    Returns:
        DataFrame with specified columns dropped
    """
    if not columns_to_drop:
        return df
        
    existing_cols = [col for col in columns_to_drop if col in df.columns]
    missing_cols = [col for col in columns_to_drop if col not in df.columns]
    
    if missing_cols:
        print(f"è­¦å‘Š: ä»¥ä¸‹æ¬„ä½ä¸å­˜åœ¨æ–¼è³‡æ–™ä¸­: {missing_cols}")
    
    if existing_cols:
        print(f"åˆªé™¤æ¬„ä½: {existing_cols}")
        df = df.drop(columns=existing_cols)
    
    return df

def save_processed_data(train_df, valid_df, test_df, suffix="processed"):
    """
    å„²å­˜å‰è™•ç†å¾Œçš„è³‡æ–™åˆ° Dataset/processed/ ç›®éŒ„
    
    Args:
        train_df, valid_df, test_df: å‰è™•ç†å¾Œçš„ DataFrames
        suffix: æª”æ¡ˆåç¨±å¾Œç¶´
    """
    # å»ºç«‹ processed ç›®éŒ„è·¯å¾‘
    current_dir = Path(os.getcwd())
    processed_dir = current_dir.parent / "Dataset" / "processed"
    print(f"å„²å­˜å‰è™•ç†å¾Œçš„è³‡æ–™åˆ°: {processed_dir}")
    
    # å»ºç«‹ç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    processed_dir.mkdir(parents=True, exist_ok=True)
    
    # å®šç¾©å„²å­˜è·¯å¾‘
    train_save_path = processed_dir / f"train_{suffix}.csv"
    valid_save_path = processed_dir / f"valid_{suffix}.csv"
    test_save_path = processed_dir / f"test_{suffix}.csv"
    
    # å„²å­˜ç‚º CSV æª”æ¡ˆ
    print(f"å„²å­˜è¨“ç·´è³‡æ–™åˆ°: {train_save_path}")
    train_df.to_csv(train_save_path, index=False, encoding='utf-8-sig')
    
    print(f"å„²å­˜é©—è­‰è³‡æ–™åˆ°: {valid_save_path}")
    valid_df.to_csv(valid_save_path, index=False, encoding='utf-8-sig')
    
    print(f"å„²å­˜æ¸¬è©¦è³‡æ–™åˆ°: {test_save_path}")
    test_df.to_csv(test_save_path, index=False, encoding='utf-8-sig')
    
    print("æ‰€æœ‰è³‡æ–™å·²æˆåŠŸå„²å­˜!")

def preprocess_data(columns_to_drop=None, save_data=True):
    """
    å®Œæ•´è³‡æ–™å‰è™•ç†æµç¨‹
    
    Args:
        columns_to_drop: list of column names to drop
        save_data: bool, æ˜¯å¦å„²å­˜å‰è™•ç†å¾Œçš„è³‡æ–™
    
    Returns:
        train_df, valid_df, test_df (preprocessed)
    """
    if columns_to_drop is None:
        # é è¨­è¦åˆªé™¤çš„æ¬„ä½ï¼ˆå¯æ ¹æ“šå¯¦éš›éœ€æ±‚ä¿®æ”¹ï¼‰
        columns_to_drop = []
        # ç¯„ä¾‹: columns_to_drop = ['id', 'unnecessary_col']
    
    # è¼‰å…¥è³‡æ–™
    train_df, valid_df, test_df = load_data()
    
    # åˆªé™¤æŒ‡å®šæ¬„ä½
    train_df = drop_columns(train_df, columns_to_drop)
    valid_df = drop_columns(valid_df, columns_to_drop)
    test_df = drop_columns(test_df, columns_to_drop)
    

    print("\nè³‡æ–™æ¬„ä½ç§»é™¤å®Œç•¢:")
    print(f"Train shape after preprocessing: {train_df.shape}")
    print(f"Valid shape after preprocessing: {valid_df.shape}")
    print(f"Test shape after preprocessing: {test_df.shape}")

    print("\né–‹å§‹è³‡æ–™ç·¨ç¢¼...")
    
    '''
    è‡ªå®šç¾©è¦å‰‡è™•ç†äº¤æ˜“ç­†æ£ æ•¸ï¼Œé€éæ­£å‰‡è¡¨é”å¼æå–æ•¸å­—éƒ¨åˆ†ä¸¦è½‰ç‚ºæ•´æ•¸å¾ŒåŠ ç¸½
    ä¾‹å¦‚: 'åœŸåœ°2å»ºç‰©1è»Šä½1' -> 4ï¼Œ'åœŸåœ°7å»ºç‰©1è»Šä½0' -> 8
    '''
    def _sum_numbers(text):
        nums = re.findall(r'\d+', str(text))
        return sum(int(n) for n in nums) if nums else 0
    if 'äº¤æ˜“ç­†æ£Ÿæ•¸' in train_df.columns:
        train_df['äº¤æ˜“ç­†æ£Ÿæ•¸'] = train_df['äº¤æ˜“ç­†æ£Ÿæ•¸'].apply(_sum_numbers)
    if 'äº¤æ˜“ç­†æ£Ÿæ•¸' in valid_df.columns:
        valid_df['äº¤æ˜“ç­†æ£Ÿæ•¸'] = valid_df['äº¤æ˜“ç­†æ£Ÿæ•¸'].apply(_sum_numbers)
    if 'äº¤æ˜“ç­†æ£Ÿæ•¸' in test_df.columns:
        test_df['äº¤æ˜“ç­†æ£Ÿæ•¸'] = test_df['äº¤æ˜“ç­†æ£Ÿæ•¸'].apply(_sum_numbers)

    # è™•ç†æ—¥æœŸæ¬„ä½
    if 'äº¤æ˜“å¹´æœˆæ—¥' in train_df.columns:
        print("è™•ç†äº¤æ˜“å¹´æœˆæ—¥...")
        train_df, valid_df, test_df = encode_date_features(
            train_df, valid_df, test_df, 
            date_columns=['äº¤æ˜“å¹´æœˆæ—¥'], 
            method='multiple_features'  # æˆ–æ”¹ç‚º 'days_since', 'cyclical', 'timestamp'
        )

    onehot_cols = ['é„‰é®å¸‚å€', 'å»ºç‰©å‹æ…‹']
    label_cols = ['äº¤æ˜“æ¨™çš„', 'éƒ½å¸‚åœŸåœ°ä½¿ç”¨åˆ†å€']
    train_df, valid_df, test_df = dataEncode(train_df, valid_df, test_df, onehot_cols=onehot_cols, label_cols=label_cols)

    # å°é½Šè³‡æ–™é›†æ¬„ä½ï¼Œç¢ºä¿ç¶­åº¦ä¸€è‡´
    train_df, valid_df, test_df = align_dataframe_columns(train_df, valid_df, test_df)

    train_Val_dropCol = ['ç·¨è™Ÿ']
    train_df = drop_columns(train_df, train_Val_dropCol)
    valid_df = drop_columns(valid_df, train_Val_dropCol)

    # è™•ç†ç¼ºå¤±å€¼ï¼šå°‡ NaN å€¼å¡«è£œç‚º 0
    print("\nè™•ç†ç¼ºå¤±å€¼...")
    train_df, valid_df, test_df = handle_missing_values(
        train_df, valid_df, test_df, 
        strategy='zero',  # å¯ä»¥æ”¹ç‚º 'mean', 'median', 'mode', 'drop'
        target_col='ç¸½åƒ¹å…ƒ'
    )

    # å„²å­˜å‰è™•ç†å¾Œçš„è³‡æ–™
    if save_data:
        save_processed_data(train_df, valid_df, test_df)
    
    return train_df, valid_df, test_df


def dataEncode(train_df, valid_df, test_df, onehot_cols=None, label_cols=None):
    """
    æŒ‡å®šæ¬„ä½ one-hot encoding èˆ‡ label encoding
    Args:
        train_df, valid_df, test_df: DataFrame
        onehot_cols: list, è¦åš one-hot çš„æ¬„ä½
        label_cols: list, è¦åš label encoding çš„æ¬„ä½
    """
    if onehot_cols is None:
        onehot_cols = []
    if label_cols is None:
        label_cols = []

    # One-hot encoding
    if onehot_cols:
        print(f"One-hot encoding æ¬„ä½: {onehot_cols}")
        for col in onehot_cols:
            print("è™•ç†æ¬„ä½:", col)
            # å°å–®ä¸€æ¬„ä½åš one-hotï¼Œç„¶å¾Œåˆä½µå›åŸ DataFrame
            train_dummy = pd.get_dummies(train_df[col], prefix=col, drop_first=False)
            valid_dummy = pd.get_dummies(valid_df[col], prefix=col, drop_first=False)
            test_dummy = pd.get_dummies(test_df[col], prefix=col, drop_first=False)
            
            # åˆªé™¤åŸæ¬„ä½ï¼ŒåŠ å…¥ç·¨ç¢¼å¾Œçš„æ¬„ä½
            train_df = train_df.drop(columns=[col]).join(train_dummy)
            valid_df = valid_df.drop(columns=[col]).join(valid_dummy)
            test_df = test_df.drop(columns=[col]).join(test_dummy)

        print("One-hot encoding å®Œæˆ")
        # save_processed_data(train_df, valid_df, test_df, suffix="onehot")

    # Label encoding
    if label_cols:
        print(f"Label encoding æ¬„ä½: {label_cols}")
        le_dict = {}
        
        for col in label_cols:
            print(f"è™•ç† Label encoding: {col}")
            
            # Fit only on training data
            le = LabelEncoder()
            train_df[col] = le.fit_transform(train_df[col].astype(str))
            le_dict[col] = le
            
            # Transform valid data with unseen label handling  
            def safe_transform_with_new_labels(series, encoder):
                """å®‰å…¨è½‰æ›ï¼Œæœªè¦‹éçš„æ¨™ç±¤çµ¦æ–°ç·¨è™Ÿ"""
                result = []
                classes_set = set(encoder.classes_)
                next_label = len(encoder.classes_)  # ä¸‹ä¸€å€‹å¯ç”¨çš„ç·¨è™Ÿ
                new_label_map = {}  # è¨˜éŒ„æ–°æ¨™ç±¤çš„æ˜ å°„
                
                for value in series.astype(str):
                    if value in classes_set:
                        result.append(encoder.transform([value])[0])
                    elif value in new_label_map:
                        result.append(new_label_map[value])
                    else:
                        # çµ¦æœªè¦‹éçš„æ¨™ç±¤æ–°ç·¨è™Ÿ
                        new_label_map[value] = next_label
                        result.append(next_label)
                        next_label += 1
                        
                return result
            
            valid_df[col] = safe_transform_with_new_labels(valid_df[col], le)
            test_df[col] = safe_transform_with_new_labels(test_df[col], le)
            
            # å°å‡ºç·¨ç¢¼è³‡è¨Š
            print(f"  {col}: {len(le.classes_)} å€‹é¡åˆ¥")
            
            # æª¢æŸ¥æœªè¦‹éçš„æ¨™ç±¤
            valid_unseen = sum(1 for x in valid_df[col] if x == -1)
            test_unseen = sum(1 for x in test_df[col] if x == -1)
            if valid_unseen > 0:
                print(f"  Valid ä¸­æœªè¦‹éçš„æ¨™ç±¤: {valid_unseen} å€‹")
            if test_unseen > 0:
                print(f"  Test ä¸­æœªè¦‹éçš„æ¨™ç±¤: {test_unseen} å€‹")

    print("\nè³‡æ–™ç·¨ç¢¼å®Œæˆ:")
    print(f"Train shape after encoding: {train_df.shape}")
    print(f"Valid shape after encoding: {valid_df.shape}")
    print(f"Test shape after encoding: {test_df.shape}")
    
    return train_df, valid_df, test_df

def process_transaction_date(df, date_column='äº¤æ˜“å¹´æœˆæ—¥', method='multiple_features'):
    """
    è™•ç†äº¤æ˜“å¹´æœˆæ—¥æ¬„ä½
    
    Parameters:
    method: ç·¨ç¢¼æ–¹å¼
        - 'timestamp': è½‰æ›ç‚ºæ™‚é–“æˆ³è¨˜
        - 'days_since': è¨ˆç®—è·é›¢æŸå€‹åŸºæº–æ—¥çš„å¤©æ•¸
        - 'multiple_features': æ‹†åˆ†ç‚ºå¹´ã€æœˆã€æ—¥ã€æ˜ŸæœŸç­‰å¤šå€‹ç‰¹å¾µ
        - 'cyclical': å¾ªç’°ç·¨ç¢¼ï¼ˆé©åˆæœˆä»½ã€æ˜ŸæœŸï¼‰
    """
    if date_column not in df.columns:
        print(f"æ¬„ä½ {date_column} ä¸å­˜åœ¨")
        return df
    
    df_result = df.copy()
    
    def parse_taiwan_date(date_str):
        """å°‡æ°‘åœ‹å¹´æœˆæ—¥è½‰æ›ç‚ºè¥¿å…ƒå¹´æœˆæ—¥"""
        try:
            date_str = str(int(date_str))  # ç¢ºä¿æ˜¯æ•´æ•¸å­—ä¸²
            if len(date_str) == 7:  # 1080721 æ ¼å¼
                year = int(date_str[:3]) + 1911  # æ°‘åœ‹è½‰è¥¿å…ƒ
                month = int(date_str[3:5])
                day = int(date_str[5:7])
                return pd.Timestamp(year, month, day)
            else:
                return pd.NaT
        except:
            return pd.NaT
    
    # å…ˆè½‰æ›ç‚º pandas Timestamp
    df_result['parsed_date'] = df_result[date_column].apply(parse_taiwan_date)
    
    if method == 'multiple_features':
        # æ–¹æ³•3: æ‹†åˆ†ç‚ºå¤šå€‹ç‰¹å¾µï¼ˆæ¨è–¦ï¼‰
        df_result[f'{date_column}_å¹´'] = df_result['parsed_date'].dt.year
        df_result[f'{date_column}_æœˆ'] = df_result['parsed_date'].dt.month
        # df_result[f'{date_column}_æ—¥'] = df_result['parsed_date'].dt.day
        # df_result[f'{date_column}_æ˜ŸæœŸ'] = df_result['parsed_date'].dt.dayofweek  # 0=é€±ä¸€
        df_result[f'{date_column}_å­£åº¦'] = df_result['parsed_date'].dt.quarter
        # df_result[f'{date_column}_æ˜¯å¦é€±æœ«'] = (df_result['parsed_date'].dt.dayofweek >= 5).astype(int)
        
        # ç§»é™¤åŸå§‹æ¬„ä½
        df_result = df_result.drop(columns=[date_column])
        print("æ‹†åˆ†ç‚ºå¤šå€‹æ™‚é–“ç‰¹å¾µ")
        
    elif method == 'cyclical':
        # æ–¹æ³•4: å¾ªç’°ç·¨ç¢¼ï¼ˆä¿æŒé€±æœŸæ€§ï¼‰
        import math
        
        # å¹´ä»½ï¼ˆç·šæ€§ï¼‰
        df_result[f'{date_column}_å¹´'] = df_result['parsed_date'].dt.year
        
        # æœˆä»½ï¼ˆå¾ªç’°ç·¨ç¢¼ï¼‰
        month = df_result['parsed_date'].dt.month
        df_result[f'{date_column}_æœˆ_sin'] = month.apply(lambda x: math.sin(2 * math.pi * x / 12))
        df_result[f'{date_column}_æœˆ_cos'] = month.apply(lambda x: math.cos(2 * math.pi * x / 12))
        
        # æ—¥æœŸï¼ˆå¾ªç’°ç·¨ç¢¼ï¼Œå‡è¨­30å¤©ä¸€å¾ªç’°ï¼‰
        day = df_result['parsed_date'].dt.day
        df_result[f'{date_column}_æ—¥_sin'] = day.apply(lambda x: math.sin(2 * math.pi * x / 30))
        df_result[f'{date_column}_æ—¥_cos'] = day.apply(lambda x: math.cos(2 * math.pi * x / 30))
        
        # æ˜ŸæœŸï¼ˆå¾ªç’°ç·¨ç¢¼ï¼‰
        weekday = df_result['parsed_date'].dt.dayofweek
        df_result[f'{date_column}_æ˜ŸæœŸ_sin'] = weekday.apply(lambda x: math.sin(2 * math.pi * x / 7))
        df_result[f'{date_column}_æ˜ŸæœŸ_cos'] = weekday.apply(lambda x: math.cos(2 * math.pi * x / 7))
        
        # ç§»é™¤åŸå§‹æ¬„ä½
        df_result = df_result.drop(columns=[date_column])
        print("ä½¿ç”¨å¾ªç’°ç·¨ç¢¼")
    
    # æ¸…ç†è‡¨æ™‚æ¬„ä½
    if 'parsed_date' in df_result.columns:
        df_result = df_result.drop(columns=['parsed_date'])
    
    return df_result

def encode_date_features(train_df, valid_df, test_df, date_columns=None, method='multiple_features'):
    """
    å°å¤šå€‹è³‡æ–™é›†çµ±ä¸€è™•ç†æ—¥æœŸæ¬„ä½
    """
    if date_columns is None:
        date_columns = ['äº¤æ˜“å¹´æœˆæ—¥']
    
    for col in date_columns:
        if col in train_df.columns:
            print(f"è™•ç†æ—¥æœŸæ¬„ä½: {col}")
            train_df = process_transaction_date(train_df, col, method)
            valid_df = process_transaction_date(valid_df, col, method)
            test_df = process_transaction_date(test_df, col, method)
    
    return train_df, valid_df, test_df

def align_dataframe_columns(train_df, valid_df, test_df, target_columns=None):
    """
    å°é½Šä¸‰å€‹è³‡æ–™é›†çš„æ¬„ä½ï¼Œç¢ºä¿ç¶­åº¦ä¸€è‡´
    
    Parameters:
    target_columns: ç›®æ¨™è®Šæ•¸æ¬„ä½åç¨±åˆ—è¡¨ï¼Œé€™äº›æ¬„ä½åªåœ¨ train/valid ä¿ç•™
    """
    if target_columns is None:
        target_columns = ['ç¸½åƒ¹å…ƒ']
    
    print("å°é½Šè³‡æ–™é›†æ¬„ä½...")
    print(f"å°é½Šå‰ - Train: {train_df.shape}, Valid: {valid_df.shape}, Test: {test_df.shape}")
    
    # å–å¾—æ‰€æœ‰ç‰¹å¾µæ¬„ä½ï¼ˆæ’é™¤ç›®æ¨™è®Šæ•¸ï¼‰
    all_train_cols = set(train_df.columns)
    all_valid_cols = set(valid_df.columns)
    all_test_cols = set(test_df.columns)
    
    # æ‰¾å‡ºæ‰€æœ‰ç‰¹å¾µæ¬„ä½ï¼ˆéç›®æ¨™è®Šæ•¸ï¼‰
    feature_cols_train = all_train_cols - set(target_columns)
    feature_cols_valid = all_valid_cols - set(target_columns)
    feature_cols_test = all_test_cols - set(target_columns)
    
    # å–è¯é›†ä½œç‚ºçµ±ä¸€çš„ç‰¹å¾µæ¬„ä½
    unified_feature_cols = list(feature_cols_train | feature_cols_valid | feature_cols_test)
    unified_feature_cols.sort()  # ç¢ºä¿é †åºä¸€è‡´
    
    print(f"çµ±ä¸€ç‰¹å¾µæ¬„ä½æ•¸é‡: {len(unified_feature_cols)}")
    
    # å°é½Šç‰¹å¾µæ¬„ä½
    train_df_aligned = train_df.reindex(columns=unified_feature_cols, fill_value=0)
    valid_df_aligned = valid_df.reindex(columns=unified_feature_cols, fill_value=0)
    test_df_aligned = test_df.reindex(columns=unified_feature_cols, fill_value=0)
    
    # åŠ å›ç›®æ¨™è®Šæ•¸ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    for target_col in target_columns:
        if target_col in train_df.columns:
            train_df_aligned[target_col] = train_df[target_col]
        if target_col in valid_df.columns:
            valid_df_aligned[target_col] = valid_df[target_col]
        # test é€šå¸¸æ²’æœ‰ç›®æ¨™è®Šæ•¸ï¼Œä¸åŠ å›
    
    print(f"å°é½Šå¾Œ - Train: {train_df_aligned.shape}, Valid: {valid_df_aligned.shape}, Test: {test_df_aligned.shape}")
    
    # æª¢æŸ¥æ˜¯å¦é‚„æœ‰å·®ç•°
    train_features = [col for col in train_df_aligned.columns if col not in target_columns]
    valid_features = [col for col in valid_df_aligned.columns if col not in target_columns]
    test_features = list(test_df_aligned.columns)
    
    if len(train_features) != len(valid_features) or len(train_features) != len(test_features):
        print("è­¦å‘Šï¼šç‰¹å¾µæ¬„ä½æ•¸é‡ä»ä¸ä¸€è‡´ï¼")
        print(f"Train ç‰¹å¾µæ•¸: {len(train_features)}")
        print(f"Valid ç‰¹å¾µæ•¸: {len(valid_features)}")
        print(f"Test ç‰¹å¾µæ•¸: {len(test_features)}")
    else:
        print("âœ“ ç‰¹å¾µæ¬„ä½å·²æˆåŠŸå°é½Š")
    
    return train_df_aligned, valid_df_aligned, test_df_aligned

def validate_model_ready_data(train_df, valid_df, test_df, target_col):
    """
    é©—è­‰è³‡æ–™æ˜¯å¦æº–å‚™å¥½è¨“ç·´æ¨¡å‹
    """
    # åˆ†é›¢ç‰¹å¾µ
    X_train = train_df.drop([target_col], axis=1, errors='ignore')
    X_valid = valid_df.drop([target_col], axis=1, errors='ignore')
    X_test = test_df.drop(["ç·¨è™Ÿ"], axis=1, errors='ignore')

    print("=== æ¨¡å‹è¨“ç·´è³‡æ–™é©—è­‰ ===")
    print(f"X_train shape: {X_train.shape}")
    print(f"X_valid shape: {X_valid.shape}")
    print(f"X_test shape: {X_test.shape}")
    
    # æª¢æŸ¥ç‰¹å¾µç¶­åº¦
    if X_train.shape[1] != X_valid.shape[1]:
        print("âŒ Train å’Œ Valid ç‰¹å¾µæ•¸é‡ä¸ä¸€è‡´ï¼")
        return False
    
    if X_train.shape[1] != X_test.shape[1]:
        print("âŒ Train å’Œ Test ç‰¹å¾µæ•¸é‡ä¸ä¸€è‡´ï¼")
        return False
    
    # æª¢æŸ¥æ¬„ä½åç¨±
    if list(X_train.columns) != list(X_valid.columns):
        print("âŒ Train å’Œ Valid æ¬„ä½åç¨±ä¸ä¸€è‡´ï¼")
        return False
        
    if list(X_train.columns) != list(X_test.columns):
        print("âŒ Train å’Œ Test æ¬„ä½åç¨±ä¸ä¸€è‡´ï¼")
        return False
    
    print("âœ… è³‡æ–™å·²æº–å‚™å¥½è¨“ç·´æ¨¡å‹ï¼")
    return True

def handle_missing_values(train_df, valid_df, test_df, strategy='zero', target_col='ç¸½åƒ¹å…ƒ'):
    """
    è™•ç†ç¼ºå¤±å€¼çš„å‡½æ•¸
    
    Args:
        train_df, valid_df, test_df: è³‡æ–™æ¡†
        strategy: è™•ç†ç­–ç•¥
            - 'zero': å¡«è£œç‚º 0 (é è¨­)
            - 'mean': ç”¨è¨“ç·´é›†çš„å¹³å‡å€¼å¡«è£œ
            - 'median': ç”¨è¨“ç·´é›†çš„ä¸­ä½æ•¸å¡«è£œ
            - 'mode': ç”¨è¨“ç·´é›†çš„çœ¾æ•¸å¡«è£œ
            - 'drop': åˆªé™¤æœ‰ç¼ºå¤±å€¼çš„åˆ—
        target_col: ç›®æ¨™è®Šæ•¸æ¬„ä½åç¨±
    
    Returns:
        è™•ç†å¾Œçš„ train_df, valid_df, test_df
    """
    print(f"\n=== è™•ç†ç¼ºå¤±å€¼ (ç­–ç•¥: {strategy}) ===")
    
    # æª¢æŸ¥ç¼ºå¤±å€¼æƒ…æ³
    train_nan_count = train_df.isnull().sum().sum()
    valid_nan_count = valid_df.isnull().sum().sum()
    test_nan_count = test_df.isnull().sum().sum()
    
    print(f"è™•ç†å‰ç¼ºå¤±å€¼æ•¸é‡:")
    print(f"  Train: {train_nan_count}")
    print(f"  Valid: {valid_nan_count}")
    print(f"  Test: {test_nan_count}")
    
    if train_nan_count == 0 and valid_nan_count == 0 and test_nan_count == 0:
        print("âœ… æ²’æœ‰ç™¼ç¾ç¼ºå¤±å€¼ï¼Œç„¡éœ€è™•ç†")
        return train_df, valid_df, test_df
    
    # é¡¯ç¤ºæœ‰ç¼ºå¤±å€¼çš„æ¬„ä½
    all_dfs = {'Train': train_df, 'Valid': valid_df, 'Test': test_df}
    for name, df in all_dfs.items():
        nan_cols = df.columns[df.isnull().any()].tolist()
        if nan_cols:
            nan_counts = df[nan_cols].isnull().sum()
            print(f"  {name} æœ‰ç¼ºå¤±å€¼çš„æ¬„ä½:")
            for col in nan_cols:
                print(f"    {col}: {nan_counts[col]} å€‹ç¼ºå¤±å€¼")
    
    if strategy == 'zero':
        # ç­–ç•¥ 1: å¡«è£œç‚º 0
        train_df_clean = train_df.fillna(0)
        valid_df_clean = valid_df.fillna(0)
        test_df_clean = test_df.fillna(0)
        print("âœ… æ‰€æœ‰ç¼ºå¤±å€¼å·²å¡«è£œç‚º 0")
        
    elif strategy == 'mean':
        # ç­–ç•¥ 2: ç”¨è¨“ç·´é›†çš„å¹³å‡å€¼å¡«è£œ
        # åˆ†é›¢æ•¸å€¼æ¬„ä½å’Œéæ•¸å€¼æ¬„ä½
        numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
        if target_col in numeric_cols:
            numeric_cols.remove(target_col)  # ä¸å°ç›®æ¨™è®Šæ•¸åšå¡«è£œ
        
        # è¨ˆç®—è¨“ç·´é›†æ•¸å€¼æ¬„ä½çš„å¹³å‡å€¼
        fill_values = train_df[numeric_cols].mean()
        
        # å¡«è£œæ•¸å€¼æ¬„ä½
        train_df_clean = train_df.copy()
        valid_df_clean = valid_df.copy()
        test_df_clean = test_df.copy()
        
        for col in numeric_cols:
            if col in train_df_clean.columns:
                train_df_clean[col].fillna(fill_values[col], inplace=True)
            if col in valid_df_clean.columns:
                valid_df_clean[col].fillna(fill_values[col], inplace=True)
            if col in test_df_clean.columns:
                test_df_clean[col].fillna(fill_values[col], inplace=True)
        
        # éæ•¸å€¼æ¬„ä½ç”¨ 0 å¡«è£œ
        train_df_clean = train_df_clean.fillna(0)
        valid_df_clean = valid_df_clean.fillna(0)
        test_df_clean = test_df_clean.fillna(0)
        
        print("âœ… æ•¸å€¼æ¬„ä½ç”¨å¹³å‡å€¼å¡«è£œï¼Œéæ•¸å€¼æ¬„ä½ç”¨ 0 å¡«è£œ")
        
    elif strategy == 'median':
        # ç­–ç•¥ 3: ç”¨è¨“ç·´é›†çš„ä¸­ä½æ•¸å¡«è£œ
        numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
        if target_col in numeric_cols:
            numeric_cols.remove(target_col)
        
        fill_values = train_df[numeric_cols].median()
        
        train_df_clean = train_df.copy()
        valid_df_clean = valid_df.copy()
        test_df_clean = test_df.copy()
        
        for col in numeric_cols:
            if col in train_df_clean.columns:
                train_df_clean[col].fillna(fill_values[col], inplace=True)
            if col in valid_df_clean.columns:
                valid_df_clean[col].fillna(fill_values[col], inplace=True)
            if col in test_df_clean.columns:
                test_df_clean[col].fillna(fill_values[col], inplace=True)
        
        train_df_clean = train_df_clean.fillna(0)
        valid_df_clean = valid_df_clean.fillna(0)
        test_df_clean = test_df_clean.fillna(0)
        
        print("âœ… æ•¸å€¼æ¬„ä½ç”¨ä¸­ä½æ•¸å¡«è£œï¼Œéæ•¸å€¼æ¬„ä½ç”¨ 0 å¡«è£œ")
        
    elif strategy == 'drop':
        # ç­–ç•¥ 4: åˆªé™¤æœ‰ç¼ºå¤±å€¼çš„åˆ—
        print("âš ï¸ æ³¨æ„ï¼šåˆªé™¤æœ‰ç¼ºå¤±å€¼çš„åˆ—å¯èƒ½æœƒå½±éŸ¿æ¨¡å‹æ€§èƒ½")
        original_cols = len(train_df.columns)
        
        # æ‰¾å‡ºæ²’æœ‰ç¼ºå¤±å€¼çš„æ¬„ä½
        train_clean_cols = train_df.columns[~train_df.isnull().any()].tolist()
        valid_clean_cols = valid_df.columns[~valid_df.isnull().any()].tolist()
        test_clean_cols = test_df.columns[~test_df.isnull().any()].tolist()
        
        # å–äº¤é›†ï¼Œç¢ºä¿æ‰€æœ‰è³‡æ–™é›†éƒ½æœ‰é€™äº›æ¬„ä½
        common_clean_cols = list(set(train_clean_cols) & set(valid_clean_cols) & set(test_clean_cols))
        
        # ç¢ºä¿ç›®æ¨™è®Šæ•¸åœ¨è¨“ç·´å’Œé©—è­‰é›†ä¸­
        if target_col in train_df.columns and target_col not in common_clean_cols:
            common_clean_cols.append(target_col)
        
        train_df_clean = train_df[common_clean_cols]
        valid_df_clean = valid_df[common_clean_cols if target_col in valid_df.columns else [col for col in common_clean_cols if col != target_col]]
        test_df_clean = test_df[[col for col in common_clean_cols if col != target_col]]
        
        removed_cols = original_cols - len(common_clean_cols)
        print(f"âœ… åˆªé™¤äº† {removed_cols} å€‹æœ‰ç¼ºå¤±å€¼çš„æ¬„ä½")
        
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„ç­–ç•¥: {strategy}")
    
    # æœ€çµ‚é©—è­‰
    final_train_nan = train_df_clean.isnull().sum().sum()
    final_valid_nan = valid_df_clean.isnull().sum().sum()
    final_test_nan = test_df_clean.isnull().sum().sum()
    
    print(f"è™•ç†å¾Œç¼ºå¤±å€¼æ•¸é‡:")
    print(f"  Train: {final_train_nan}")
    print(f"  Valid: {final_valid_nan}")
    print(f"  Test: {final_test_nan}")
    
    if final_train_nan + final_valid_nan + final_test_nan == 0:
        print("ğŸ‰ æ‰€æœ‰ç¼ºå¤±å€¼å·²æˆåŠŸè™•ç†!")
    else:
        print("âš ï¸ ä»æœ‰ç¼ºå¤±å€¼å­˜åœ¨ï¼Œè«‹æª¢æŸ¥è™•ç†é‚è¼¯")
    
    return train_df_clean, valid_df_clean, test_df_clean

if __name__ == "__main__":
    # æ¸¬è©¦è¼‰å…¥è³‡æ–™
    try:
        train_df, valid_df, test_df = load_data()
        print("\næ¬„ä½è³‡è¨Š:")
        print("Train columns:", list(train_df.columns))
        print("Valid columns:", list(valid_df.columns))
        print("Test columns:", list(test_df.columns))
        
        # ç¯„ä¾‹: åˆªé™¤ç‰¹å®šæ¬„ä½ä¸¦å„²å­˜
        # columns_to_drop = ['column1', 'column2']  # è«‹æ ¹æ“šå¯¦éš›éœ€æ±‚ä¿®æ”¹
        # train_df, valid_df, test_df = preprocess_data(columns_to_drop, save_data=True)
        columns_to_drop = ['åœŸåœ°ä½ç½®å»ºç‰©é–€ç‰Œ',
                           'ééƒ½å¸‚åœŸåœ°ä½¿ç”¨åˆ†å€',
                           'ééƒ½å¸‚åœŸåœ°ä½¿ç”¨ç·¨å®š',
                           'ç§»è½‰å±¤æ¬¡',
                           'ä¸»è¦ç”¨é€”',
                           'ä¸»è¦å»ºæ', 
                           'å»ºç¯‰å®Œæˆå¹´æœˆ', 
                           'å»ºç‰©ç¾æ³æ ¼å±€-éš”é–“',
                           'æœ‰ç„¡ç®¡ç†çµ„ç¹”',
                           'è»Šä½é¡åˆ¥', 
                           'è»Šä½ç§»è½‰ç¸½é¢ç©å¹³æ–¹å…¬å°º',
                           'å‚™è¨»', 
                           'æ£ŸåŠè™Ÿ', 
                           'å»ºæ¡ˆåç¨±',
                           'è§£ç´„æƒ…å½¢']  
        train_df, valid_df, test_df = preprocess_data(columns_to_drop, save_data=True)
        
    except Exception as e:
        print(f"è®€å–è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def load_processed_data(suffix="processed"):
    """
    è¼‰å…¥å‰è™•ç†å¾Œçš„è³‡æ–™
    
    Args:
        suffix: æª”æ¡ˆåç¨±å¾Œç¶´
    
    Returns:
        train_df, valid_df, test_df
    """
    current_dir = Path(__file__).parent
    processed_dir = current_dir.parent / "Dataset" / "processed"
    
    train_path = processed_dir / f"train_{suffix}.csv"
    valid_path = processed_dir / f"valid_{suffix}.csv"
    test_path = processed_dir / f"test_{suffix}.csv"
    
    print(f"è®€å–å‰è™•ç†å¾Œçš„è¨“ç·´è³‡æ–™: {train_path}")
    train_df = pd.read_csv(train_path)
    
    print(f"è®€å–å‰è™•ç†å¾Œçš„é©—è­‰è³‡æ–™: {valid_path}")
    valid_df = pd.read_csv(valid_path)
    
    print(f"è®€å–å‰è™•ç†å¾Œçš„æ¸¬è©¦è³‡æ–™: {test_path}")
    test_df = pd.read_csv(test_path)
    
    return train_df, valid_df, test_df
