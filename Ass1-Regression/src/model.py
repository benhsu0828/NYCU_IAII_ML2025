import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import cross_val_score
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class RegressionModels:
    """å›æ­¸æ¨¡å‹é›†åˆé¡"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = {}
        self.trained_models = {}
        self.results = {}
        
    def get_tree_models(self):
        """å–å¾—æ¨¹æ¨¡å‹é…ç½®"""
        return {          
            'XGBoost': xgb.XGBRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=self.random_state,
                n_jobs=-1
            ),
            
            'LightGBM': lgb.LGBMRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=self.random_state,
                verbose=-1,
                n_jobs=-1
            ),
            
            'CatBoost': cb.CatBoostRegressor(
                iterations=100,
                depth=6,
                learning_rate=0.1,
                random_state=self.random_state,
                verbose=False,
                thread_count=-1
            ),
        }
    
    def get_linear_models(self):
        """å–å¾—ç·šæ€§æ¨¡å‹é…ç½®"""
        return {
            'LinearRegression': LinearRegression(),
            'Ridge': Ridge(alpha=1.0, random_state=self.random_state),
            'Lasso': Lasso(alpha=1.0, random_state=self.random_state)
        }
    
    def get_optimized_tree_models(self):
        """å–å¾—å„ªåŒ–å¾Œçš„æ¨¹æ¨¡å‹é…ç½®ï¼ˆæ›´å¥½çš„åƒæ•¸ï¼‰"""
        return {
            'XGBoost_Optimized': xgb.XGBRegressor(
                n_estimators=300,
                max_depth=8,
                learning_rate=0.05,
                subsample=0.85,
                colsample_bytree=0.85,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=self.random_state,
                n_jobs=-1
            ),
            
            'LightGBM_Optimized': lgb.LGBMRegressor(
                n_estimators=300,
                max_depth=8,
                learning_rate=0.05,
                subsample=0.85,
                colsample_bytree=0.85,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=self.random_state,
                verbose=-1,
                n_jobs=-1
            ),
        }
    
    def calculate_metrics(self, y_true, y_pred, model_name=""):
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        # è¨ˆç®— MAPE (Mean Absolute Percentage Error)
        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        
        metrics = {
            'Model': model_name,
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'R2': r2,
            'MAPE': mape
        }
        
        return metrics
    
    def train_single_model(self, model, model_name, X_train, y_train, X_valid, y_valid):
        """è¨“ç·´å–®ä¸€æ¨¡å‹"""
        import time
        
        print(f"\nğŸ”„ é–‹å§‹è¨“ç·´ {model_name}...")
        start_time = time.time()
        
        # è¨“ç·´æ¨¡å‹
        model.fit(X_train, y_train)
        
        training_time = time.time() - start_time
        
        # é æ¸¬
        print(f"   â±ï¸  è¨“ç·´æ™‚é–“: {training_time:.2f} ç§’")
        print(f"   ğŸ“Š é€²è¡Œé æ¸¬...")
        
        train_pred = model.predict(X_train)
        valid_pred = model.predict(X_valid)
        
        # è¨ˆç®—æŒ‡æ¨™
        train_metrics = self.calculate_metrics(y_train, train_pred, f"{model_name}_Train")
        valid_metrics = self.calculate_metrics(y_valid, valid_pred, f"{model_name}_Valid")
        
        # å„²å­˜çµæœ
        self.trained_models[model_name] = model
        self.results[model_name] = {
            'train': train_metrics,
            'valid': valid_metrics,
            'train_pred': train_pred,
            'valid_pred': valid_pred,
            'training_time': training_time
        }
        
        # è©³ç´°è¼¸å‡ºçµæœ
        print(f"   âœ… {model_name} è¨“ç·´å®Œæˆ!")
        print(f"      ğŸ“ˆ é©—è­‰é›† RMSE: {valid_metrics['RMSE']:.2f}")
        print(f"      ğŸ“ˆ é©—è­‰é›† RÂ²: {valid_metrics['R2']:.4f}")
        print(f"      ğŸ“ˆ é©—è­‰é›† MAE: {valid_metrics['MAE']:.2f}")
        if training_time > 60:
            print(f"      â±ï¸  è€—æ™‚: {training_time/60:.1f} åˆ†é˜")
        else:
            print(f"      â±ï¸  è€—æ™‚: {training_time:.1f} ç§’")
        
        return model
    
    def get_deep_learning_models(self):
        """å–å¾—æ·±åº¦å­¸ç¿’æ¨¡å‹é…ç½®"""
        try:
            import tensorflow as tf
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation
            from tensorflow.keras.optimizers import Adam
            from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
            from sklearn.preprocessing import StandardScaler
            
            print(f"ğŸ” TensorFlow ç‰ˆæœ¬: {tf.__version__}")
            
            # æª¢æŸ¥ GPU
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                print(f"ğŸš€ æª¢æ¸¬åˆ° {len(gpus)} å€‹ GPU")
                try:
                    for gpu in gpus:
                        tf.config.experimental.set_memory_growth(gpu, True)
                    print("âœ… GPU è¨˜æ†¶é«”å¢é•·å·²å•Ÿç”¨")
                except RuntimeError:
                    pass  # GPU å·²ç¶“åˆå§‹åŒ–
            else:
                print("ä½¿ç”¨ CPU è¨“ç·´")
            
            # ç°¡åŒ–çš„ç¥ç¶“ç¶²çµ¡åŒ…è£å™¨
            class KerasRegressorWrapper:
                def __init__(self, input_dim, epochs=50, batch_size=32): #CPU batch_size=16
                    self.input_dim = input_dim
                    self.epochs = epochs
                    self.batch_size = batch_size
                    self.model = None
                    self.scaler = StandardScaler()
                
                def _build_model(self):
                    model = Sequential([
                            Dense(256, activation='relu', input_shape=(self.input_dim,)),
                            Dropout(0.3),
                            Dense(512, activation='relu'),
                            Dropout(0.3),
                            Dense(256, activation='relu'),
                            Dropout(0.3),
                            Dense(128, activation='relu'),
                            Dropout(0.2),
                            Dense(64, activation='relu'),
                            Dense(1, activation='linear')
                        ])

                    # # V3: BatchNorm 
                    # model = Sequential([
                    #     Dense(512, input_shape=(self.input_dim,)),
                    #     BatchNormalization(),
                    #     Activation('relu'),
                    #     Dropout(0.3),
                        
                    #     Dense(256),
                    #     BatchNormalization(), 
                    #     Activation('relu'),
                    #     Dropout(0.3),
                        
                    #     Dense(128),
                    #     BatchNormalization(),
                    #     Activation('relu'), 
                    #     Dropout(0.2),
                        
                    #     Dense(64),
                    #     BatchNormalization(),
                    #     Activation('relu'),
                        
                    #     Dense(1, activation='linear')
                    # ])
                    
                    
                    model.compile(
                        optimizer=Adam(learning_rate=0.001),
                        loss='mse',
                        metrics=['mae']
                    )
                    return model
                
                def fit(self, X, y):
                    print(f"ğŸ§  é–‹å§‹è¨“ç·´ç¥ç¶“ç¶²çµ¡ - æ¨£æœ¬æ•¸: {len(X)}, ç‰¹å¾µæ•¸: {X.shape[1]}")
                    
                    # æ¨™æº–åŒ–ç‰¹å¾µ
                    X_scaled = self.scaler.fit_transform(X)
                    
                    # å»ºç«‹æ¨¡å‹
                    self.model = self._build_model()
                    
                    # è¨­å®šå›èª¿å‡½æ•¸
                    callbacks = [
                        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
                        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)
                    ]
                    
                    # è¨“ç·´æ¨¡å‹
                    self.model.fit(
                        X_scaled, y,
                        epochs=self.epochs,
                        batch_size=self.batch_size,
                        validation_split=0.2,
                        callbacks=callbacks,
                        verbose=1
                    )
                    
                    return self
                
                def predict(self, X):
                    if self.model is None:
                        raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´")
                    
                    X_scaled = self.scaler.transform(X)
                    return self.model.predict(X_scaled, verbose=0).flatten()
            
            # å‰µå»ºä¸€å€‹å·¥å» å‡½æ•¸ï¼Œåœ¨è¨“ç·´æ™‚å‹•æ…‹è¨­å®š input_dim
            def create_dl_models(input_dim):
                return {
                    'DeepLearning_NN': KerasRegressorWrapper(
                        input_dim=input_dim,
                        epochs=100,
                        batch_size=64
                    )
                }
            
            return create_dl_models
            
        except ImportError as e:
            print("âš ï¸ TensorFlow æœªå®‰è£ï¼Œè·³éæ·±åº¦å­¸ç¿’æ¨¡å‹")
            print(f"   éŒ¯èª¤: {e}")
            return {}
        except Exception as e:
            print(f"âŒ æ·±åº¦å­¸ç¿’æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
            return {}
    
    def train_multiple_models(self, models_dict, X_train, y_train, X_valid, y_valid):
        """è¨“ç·´å¤šå€‹æ¨¡å‹"""
        import time
        
        total_models = len(models_dict)
        print(f"\n{'='*60}")
        print(f"ğŸš€ é–‹å§‹è¨“ç·´ {total_models} å€‹æ¨¡å‹")
        print(f"{'='*60}")
        
        total_start_time = time.time()
        successful_models = 0
        failed_models = []
        
        for i, (model_name, model) in enumerate(models_dict.items(), 1):
            print(f"\nğŸ“Š é€²åº¦: {i}/{total_models}")
            try:
                # ç‰¹æ®Šè™•ç†æ·±åº¦å­¸ç¿’æ¨¡å‹
                if 'DeepLearning' in model_name and hasattr(model, 'input_dim'):
                    if model.input_dim is None:
                        model.input_dim = X_train.shape[1]
                
                self.train_single_model(model, model_name, X_train, y_train, X_valid, y_valid)
                successful_models += 1
                
            except Exception as e:
                print(f"   âŒ {model_name} è¨“ç·´å¤±æ•—: {e}")
                failed_models.append(model_name)
                if 'DeepLearning' in model_name:
                    print("     ğŸ’¡ æç¤ºï¼šæ·±åº¦å­¸ç¿’æ¨¡å‹éœ€è¦ TensorFlowï¼Œå¯è·³éæ­¤æ¨¡å‹")
                else:
                    import traceback
                    print("     ğŸ” è©³ç´°éŒ¯èª¤:")
                    traceback.print_exc()
        
        # ç¸½çµ
        total_time = time.time() - total_start_time
        print(f"\n{'='*60}")
        print(f"ğŸ‰ æ¨¡å‹è¨“ç·´å®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {successful_models}/{total_models} å€‹æ¨¡å‹")
        if failed_models:
            print(f"âŒ å¤±æ•—: {len(failed_models)} å€‹æ¨¡å‹ ({', '.join(failed_models)})")
        
        if total_time > 60:
            print(f"â±ï¸  ç¸½è€—æ™‚: {total_time/60:.1f} åˆ†é˜")
        else:
            print(f"â±ï¸  ç¸½è€—æ™‚: {total_time:.1f} ç§’")
        
        if successful_models > 0:
            avg_time = total_time / successful_models
            print(f"ğŸ“Š å¹³å‡æ¯æ¨¡å‹: {avg_time:.1f} ç§’")
        print(f"{'='*60}")
    
    def get_results_summary(self):
        """å–å¾—çµæœæ‘˜è¦"""
        summary_data = []
        
        for model_name, result in self.results.items():
            valid_metrics = result['valid']
            training_time = result.get('training_time', 0)
            
            summary_data.append({
                'Model': model_name,
                'Valid_RMSE': valid_metrics['RMSE'],
                'Valid_MAE': valid_metrics['MAE'],
                'Valid_R2': valid_metrics['R2'],
                'Valid_MAPE': valid_metrics['MAPE'],
                'Training_Time(s)': training_time
            })
        
        df_summary = pd.DataFrame(summary_data)
        df_summary = df_summary.sort_values('Valid_RMSE')
        
        # ç¾åŒ–è¼¸å‡º
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½æ’è¡Œæ¦œ (ä¾é©—è­‰é›† RMSE æ’åº)")
        print(f"{'='*80}")
        
        for i, row in df_summary.iterrows():
            rank = df_summary.index.get_loc(i) + 1
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else f"#{rank}"
            
            print(f"{medal} {row['Model']}")
            print(f"    RMSE: {row['Valid_RMSE']:.2f} | RÂ²: {row['Valid_R2']:.4f} | MAE: {row['Valid_MAE']:.2f}")
            print(f"    MAPE: {row['Valid_MAPE']:.2f}% | è¨“ç·´æ™‚é–“: {row['Training_Time(s)']:.1f}s")
            print()
        
        return df_summary
    
    def save_best_model(self, save_dir="models"):
        """å„²å­˜æœ€ä½³æ¨¡å‹"""
        if not self.results:
            print("æ²’æœ‰è¨“ç·´éçš„æ¨¡å‹!")
            return None, None
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹ï¼ˆValid RMSE æœ€ä½ï¼‰
        best_model_name = min(self.results.keys(), 
                            key=lambda x: self.results[x]['valid']['RMSE'])
        
        best_model = self.trained_models[best_model_name]
        
        # å»ºç«‹å„²å­˜ç›®éŒ„
        save_path = Path(save_dir)
        save_path.mkdir(exist_ok=True)
        
        # å„²å­˜æ¨¡å‹
        model_file = save_path / f"best_model_{best_model_name}.joblib"
        joblib.dump(best_model, model_file)
        
        print(f"æœ€ä½³æ¨¡å‹ {best_model_name} å·²å„²å­˜åˆ° {model_file}")
        
        return best_model_name, best_model
    
    def save_best_models_by_type(self, save_dir="models", timestamp=None):
        """åˆ†é¡åˆ¥å„²å­˜æœ€ä½³æ¨¡å‹"""
        if not self.results:
            print("æ²’æœ‰è¨“ç·´éçš„æ¨¡å‹!")
            return {}
        
        # å»ºç«‹å„²å­˜ç›®éŒ„
        save_path = Path(save_dir)
        save_path.mkdir(exist_ok=True)
        
        # å¦‚æœæ²’æœ‰æä¾›æ™‚é–“æˆ³è¨˜ï¼Œç”Ÿæˆä¸€å€‹
        if timestamp is None:
            from datetime import datetime
            timestamp = datetime.now().strftime("%m%d_%H%M")
        
        # åˆ†é¡æ¨¡å‹
        model_categories = {
            'tree': [],
            'linear': [],
            'deep_learning': []
        }
        
        for model_name in self.results.keys():
            if any(tree_type in model_name.lower() for tree_type in 
                   ['randomforest', 'xgboost', 'lightgbm', 'catboost', 'gradient']):
                model_categories['tree'].append(model_name)
            elif any(linear_type in model_name.lower() for linear_type in 
                     ['linear', 'ridge', 'lasso', 'elastic']):
                model_categories['linear'].append(model_name)
            elif 'deeplearning' in model_name.lower() or 'neural' in model_name.lower():
                model_categories['deep_learning'].append(model_name)
        
        best_models = {}
        
        # ç‚ºæ¯å€‹é¡åˆ¥æ‰¾å‡ºæœ€ä½³æ¨¡å‹ä¸¦å„²å­˜
        for category, model_names in model_categories.items():
            if not model_names:
                continue
                
            # æ‰¾å‡ºè©²é¡åˆ¥çš„æœ€ä½³æ¨¡å‹
            best_in_category = min(model_names, 
                                 key=lambda x: self.results[x]['valid']['RMSE'])
            
            best_model = self.trained_models[best_in_category]
            
            # ğŸ• ç”Ÿæˆå¸¶æ™‚é–“æˆ³è¨˜çš„æª”æ¡ˆåç¨±
            timestamped_filename = f"best_{category}_model_{best_in_category}_{timestamp}.joblib"
            model_file = save_path / timestamped_filename
            joblib.dump(best_model, model_file)
            
            best_models[category] = {
                'name': best_in_category,
                'timestamped_name': f"{best_in_category}_{timestamp}",
                'model': best_model,
                'rmse': self.results[best_in_category]['valid']['RMSE'],
                'r2': self.results[best_in_category]['valid']['R2'],
                'file': model_file,
                'timestamp': timestamp
            }
            
            print(f"æœ€ä½³{category}æ¨¡å‹ {best_in_category} å·²å„²å­˜åˆ° {model_file}")
            print(f"  RMSE: {self.results[best_in_category]['valid']['RMSE']:.2f}")
            print(f"  RÂ²: {self.results[best_in_category]['valid']['R2']:.4f}")
            print(f"  æ™‚é–“æˆ³è¨˜: {timestamp}")
        
        # æ‰¾å‡ºæ•´é«”æœ€ä½³æ¨¡å‹
        if best_models:
            overall_best_category = min(best_models.keys(), 
                                      key=lambda x: best_models[x]['rmse'])
            overall_best = best_models[overall_best_category]
            
            print(f"\nğŸ† æ•´é«”æœ€ä½³æ¨¡å‹: {overall_best['name']} ({overall_best_category})")
            print(f"   RMSE: {overall_best['rmse']:.2f}")
            print(f"   RÂ²: {overall_best['r2']:.4f}")
            
            # å„²å­˜æ•´é«”æœ€ä½³æ¨¡å‹çš„å‰¯æœ¬
            overall_best_file = save_path / f"overall_best_model_{overall_best['name']}.joblib"
            joblib.dump(overall_best['model'], overall_best_file)
            print(f"   æª”æ¡ˆ: {overall_best_file}")
            
            best_models['overall_best'] = overall_best
        
        return best_models
    
    def plot_predictions(self, model_names=None, figsize=(15, 10)):
        """ç¹ªè£½é æ¸¬çµæœåœ–"""
        if model_names is None:
            model_names = list(self.results.keys())
        
        n_models = len(model_names)
        cols = min(3, n_models)
        rows = (n_models + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=figsize)
        if n_models == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes.reshape(1, -1)
        
        for i, model_name in enumerate(model_names):
            if model_name not in self.results:
                continue
                
            row = i // cols
            col = i % cols
            ax = axes[row, col] if rows > 1 else axes[col]
            
            result = self.results[model_name]
            y_true = result['valid']['y_true'] if 'y_true' in result['valid'] else None
            y_pred = result['valid_pred']
            
            if y_true is not None:
                ax.scatter(y_true, y_pred, alpha=0.5)
                ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
                ax.set_xlabel('True Values')
                ax.set_ylabel('Predictions')
                ax.set_title(f'{model_name}\nRÂ² = {result["valid"]["R2"]:.4f}')
        
        plt.tight_layout()
        plt.show()
    
    def predict_test(self, model_name, X_test):
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹é æ¸¬æ¸¬è©¦é›†"""
        if model_name not in self.trained_models:
            raise ValueError(f"æ¨¡å‹ {model_name} å°šæœªè¨“ç·´!")
        
        model = self.trained_models[model_name]
        test_pred = model.predict(X_test)
        
        return test_pred