import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import cross_val_score
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ğŸ”§ å°‡ DNN åŒ…è£å™¨ç§»åˆ°æ¨¡çµ„é ‚å±¤ä»¥æ”¯æ´ pickle
from sklearn.base import BaseEstimator, RegressorMixin

class SerializableDNNWrapper(BaseEstimator, RegressorMixin):
    """å¯åºåˆ—åŒ–çš„ DNN åŒ…è£å™¨ - æ”¯æ´ pickle å’Œ scikit-learn"""
    
    def __init__(self, input_dim=None):
        self.input_dim = input_dim
        self.model = None
        self.scaler = None
        self.is_fitted = False
        self.model_weights = None  # ç”¨æ–¼å­˜å„²æ¬Šé‡è€Œä¸æ˜¯æ•´å€‹ TF æ¨¡å‹
    
    def _build_model(self):
        """æ§‹å»º DNN æ¨¡å‹"""
        try:
            import tensorflow as tf
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import Dense, Dropout
            
            model = Sequential([
                Dense(128, activation='relu', input_shape=(self.input_dim,)),
                Dropout(0.3),
                Dense(64, activation='relu'),
                Dropout(0.2),
                Dense(32, activation='relu'),
                Dense(1, activation='linear')
            ])
            
            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                loss='mse',
                metrics=['mae']
            )
            return model
        except ImportError:
            print("âŒ TensorFlow æœªå®‰è£ï¼Œç„¡æ³•å‰µå»º DNN")
            return None
    
    def fit(self, X, y):
        """è¨“ç·´æ¨¡å‹"""
        try:
            from sklearn.preprocessing import StandardScaler
            import tensorflow as tf
            import time
            
            print(f"      ğŸ§  è¨“ç·´æ–°çš„ DNN åŸºå­¸ç¿’å™¨ (ç‰¹å¾µæ•¸: {X.shape[1]}, æ¨£æœ¬æ•¸: {X.shape[0]})...")
            
            # è¨­ç½®è¼¸å…¥ç¶­åº¦
            if self.input_dim is None:
                self.input_dim = X.shape[1]
            
            # æ•¸æ“šæ¨™æº–åŒ–
            print(f"         ğŸ“Š æ­£åœ¨æ¨™æº–åŒ–ç‰¹å¾µ...")
            self.scaler = StandardScaler()
            X_scaled = self.scaler.fit_transform(X)
            
            # æ§‹å»ºæ¨¡å‹
            print(f"         ğŸ—ï¸ å»ºç«‹ç¥ç¶“ç¶²è·¯æ¶æ§‹...")
            self.model = self._build_model()
            if self.model is None:
                raise ValueError("ç„¡æ³•å‰µå»º DNN æ¨¡å‹")
            
            # ğŸ“Š Stacking DNN å°ˆç”¨é€²åº¦å›èª¿
            class StackingProgressCallback(tf.keras.callbacks.Callback):
                def __init__(self):
                    self.start_time = None
                    
                def on_train_begin(self, logs=None):
                    print(f"         ğŸš€ é–‹å§‹è¨“ç·´ {self.params['epochs']} epochs...")
                    self.start_time = time.time()
                    
                def on_epoch_end(self, epoch, logs=None):
                    current_epoch = epoch + 1
                    total_epochs = self.params['epochs']
                    
                    # æ¯5å€‹epochæˆ–å‰3å€‹epochæˆ–æœ€å¾Œ3å€‹epoché¡¯ç¤ºé€²åº¦
                    if (current_epoch <= 3 or 
                        current_epoch % 5 == 0 or 
                        current_epoch > total_epochs - 3):
                        
                        elapsed = time.time() - self.start_time
                        progress = current_epoch / total_epochs * 100
                        
                        # å‰µå»ºé€²åº¦æ¢
                        bar_length = 20
                        filled_length = int(bar_length * current_epoch / total_epochs)
                        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                        
                        # é ä¼°å‰©é¤˜æ™‚é–“
                        if current_epoch > 0:
                            eta = elapsed / current_epoch * (total_epochs - current_epoch)
                            eta_str = f", ETA: {eta:.0f}s"
                        else:
                            eta_str = ""
                        
                        print(f"         Epoch {current_epoch:2d}/{total_epochs} "
                              f"[{bar}] {progress:5.1f}% "
                              f"- loss: {logs.get('loss', 0):.4f} "
                              f"- val_loss: {logs.get('val_loss', 0):.4f} "
                              f"({elapsed:.0f}s{eta_str})")
                              
                def on_train_end(self, logs=None):
                    total_time = time.time() - self.start_time
                    print(f"         âœ… Stacking DNN è¨“ç·´å®Œæˆï¼Œç¸½è€—æ™‚: {total_time:.1f} ç§’")
            
            # è¨“ç·´è¨­ç½®
            callbacks = [
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_loss', patience=8, restore_best_weights=True, verbose=0
                ),
                tf.keras.callbacks.ReduceLROnPlateau(
                    monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=0
                ),
                StackingProgressCallback()  # ğŸ“Š æ·»åŠ é€²åº¦é¡¯ç¤ºå›èª¿
            ]
            
            # è¨“ç·´
            history = self.model.fit(
                X_scaled, y,
                epochs=40,             # å¢åŠ  epochs ä»¥ç²å¾—æ›´å¥½æ•ˆæœ
                batch_size=64,       
                validation_split=0.2,
                callbacks=callbacks,
                verbose=0  # é—œé–‰ TensorFlow é»˜èªè¼¸å‡ºï¼Œä½¿ç”¨è‡ªå®šç¾©é€²åº¦
            )
            
            # å„²å­˜æ¬Šé‡è€Œä¸æ˜¯æ•´å€‹æ¨¡å‹
            self.model_weights = self.model.get_weights()
            self.is_fitted = True
            
            final_loss = history.history['loss'][-1]
            final_val_loss = history.history['val_loss'][-1]
            print(f"      âœ… DNN è¨“ç·´å®Œæˆï¼Œæœ€çµ‚ loss: {final_loss:.4f}, val_loss: {final_val_loss:.4f}")
            return self
            
        except Exception as e:
            print(f"      âŒ DNN è¨“ç·´å¤±æ•—: {e}")
            # ä½¿ç”¨ Ridge ä½œç‚ºå‚™ç”¨
            from sklearn.linear_model import Ridge
            self.backup_model = Ridge(alpha=1.0)
            self.backup_model.fit(X, y)
            self.scaler = None
            self.is_fitted = True
            self.use_backup = True
            print("      ğŸ’¡ æ”¹ç”¨ Ridge å›æ­¸ä½œç‚ºå‚™ç”¨")
            return self
    
    def predict(self, X):
        """é æ¸¬"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´")
        
        # å¦‚æœä½¿ç”¨å‚™ç”¨æ¨¡å‹
        if hasattr(self, 'use_backup') and self.use_backup:
            return self.backup_model.predict(X)
        
        try:
            # é‡å»ºæ¨¡å‹ä¸¦è¼‰å…¥æ¬Šé‡
            if self.model is None and self.model_weights is not None:
                self.model = self._build_model()
                if self.model:
                    self.model.set_weights(self.model_weights)
            
            if self.model and self.scaler:
                X_scaled = self.scaler.transform(X)
                predictions = self.model.predict(X_scaled, verbose=0)
                return predictions.flatten()
            else:
                raise ValueError("æ¨¡å‹ç‹€æ…‹ç•°å¸¸")
                
        except Exception as e:
            print(f"âš ï¸  DNN é æ¸¬å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®é æ¸¬: {e}")
            # ç°¡å–®çš„å‚™ç”¨é æ¸¬
            return np.mean(X, axis=1) * 0.1
    
    def get_params(self, deep=True):
        """ç²å–åƒæ•¸ï¼ˆscikit-learn ç›¸å®¹æ€§ï¼‰"""
        return {'input_dim': self.input_dim}
    
    def set_params(self, **params):
        """è¨­ç½®åƒæ•¸ï¼ˆscikit-learn ç›¸å®¹æ€§ï¼‰"""
        for key, value in params.items():
            setattr(self, key, value)
        return self
    
    def score(self, X, y):
        """è¨ˆç®— RÂ² åˆ†æ•¸ï¼ˆscikit-learn å›æ­¸å™¨å¿…éœ€ï¼‰"""
        try:
            from sklearn.metrics import r2_score
            y_pred = self.predict(X)
            return r2_score(y, y_pred)
        except Exception as e:
            print(f"âš ï¸  è¨ˆç®—åˆ†æ•¸å¤±æ•—: {e}")
            return 0.0
    
    def __getstate__(self):
        """è‡ªå®šç¾©åºåˆ—åŒ– - åªä¿å­˜æ¬Šé‡ï¼Œä¸ä¿å­˜ TensorFlow ç‰©ä»¶"""
        state = self.__dict__.copy()
        # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„ TensorFlow æ¨¡å‹ç‰©ä»¶
        state['model'] = None
        return state
    
    def __setstate__(self, state):
        """è‡ªå®šç¾©ååºåˆ—åŒ–"""
        self.__dict__.update(state)
        # æ¨¡å‹æœƒåœ¨éœ€è¦æ™‚é‡æ–°å‰µå»º

class RegressionModels:
    """å›æ­¸æ¨¡å‹é›†åˆé¡"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = {}
        self.trained_models = {}
        self.results = {}
        
    def get_tree_models(self):
        """å–å¾—æ¨¹æ¨¡å‹é…ç½®"""
        return {          
            'XGBoost': xgb.XGBRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=self.random_state,
                n_jobs=-1
            ),
            
            'LightGBM': lgb.LGBMRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=self.random_state,
                verbose=-1,
                n_jobs=-1
            ),
            
            'CatBoost': cb.CatBoostRegressor(
                iterations=100,
                depth=6,
                learning_rate=0.1,
                random_state=self.random_state,
                verbose=False,
                thread_count=-1
            ),
        }
    
    def get_linear_models(self):
        """å–å¾—ç·šæ€§æ¨¡å‹é…ç½®"""
        return {
            'LinearRegression': LinearRegression(),
            'Ridge': Ridge(alpha=1.0, random_state=self.random_state),
            'Lasso': Lasso(alpha=1.0, random_state=self.random_state)
        }
    
    def get_optimized_tree_models(self):
        """å–å¾—å„ªåŒ–å¾Œçš„æ¨¹æ¨¡å‹é…ç½®ï¼ˆæ›´å¥½çš„åƒæ•¸ï¼‰"""
        return {
            'XGBoost_Optimized': xgb.XGBRegressor(
                n_estimators=300,
                max_depth=8,
                learning_rate=0.05,
                subsample=0.85,
                colsample_bytree=0.85,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=self.random_state,
                n_jobs=-1
            ),
            
            'LightGBM_Optimized': lgb.LGBMRegressor(
                n_estimators=300,
                max_depth=8,
                learning_rate=0.05,
                subsample=0.85,
                colsample_bytree=0.85,
                reg_alpha=0.1,
                reg_lambda=1.0,
                random_state=self.random_state,
                verbose=-1,
                n_jobs=-1
            ),
        }
    
    def calculate_metrics(self, y_true, y_pred, model_name=""):
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        # è¨ˆç®— MAPE (Mean Absolute Percentage Error)
        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        
        metrics = {
            'Model': model_name,
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'R2': r2,
            'MAPE': mape
        }
        
        return metrics
    
    def train_single_model(self, model, model_name, X_train, y_train, X_valid, y_valid):
        """è¨“ç·´å–®ä¸€æ¨¡å‹"""
        import time
        
        print(f"\nğŸ”„ é–‹å§‹è¨“ç·´ {model_name}...")
        start_time = time.time()
        
        # è¨“ç·´æ¨¡å‹
        model.fit(X_train, y_train)
        
        training_time = time.time() - start_time
        
        # é æ¸¬
        print(f"   â±ï¸  è¨“ç·´æ™‚é–“: {training_time:.2f} ç§’")
        print(f"   ğŸ“Š é€²è¡Œé æ¸¬...")
        
        train_pred = model.predict(X_train)
        valid_pred = model.predict(X_valid)
        
        # è¨ˆç®—æŒ‡æ¨™
        train_metrics = self.calculate_metrics(y_train, train_pred, f"{model_name}_Train")
        valid_metrics = self.calculate_metrics(y_valid, valid_pred, f"{model_name}_Valid")
        
        # å„²å­˜çµæœ
        self.trained_models[model_name] = model
        self.results[model_name] = {
            'train': train_metrics,
            'valid': valid_metrics,
            'train_pred': train_pred,
            'valid_pred': valid_pred,
            'training_time': training_time
        }
        
        # è©³ç´°è¼¸å‡ºçµæœ
        print(f"   âœ… {model_name} è¨“ç·´å®Œæˆ!")
        print(f"      ğŸ“ˆ é©—è­‰é›† RMSE: {valid_metrics['RMSE']:.2f}")
        print(f"      ğŸ“ˆ é©—è­‰é›† RÂ²: {valid_metrics['R2']:.4f}")
        print(f"      ğŸ“ˆ é©—è­‰é›† MAE: {valid_metrics['MAE']:.2f}")
        if training_time > 60:
            print(f"      â±ï¸  è€—æ™‚: {training_time/60:.1f} åˆ†é˜")
        else:
            print(f"      â±ï¸  è€—æ™‚: {training_time:.1f} ç§’")
        
        return model
    
    def get_deep_learning_models(self):
        """å–å¾—æ·±åº¦å­¸ç¿’æ¨¡å‹é…ç½®"""
        try:
            import tensorflow as tf
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation
            from tensorflow.keras.optimizers import Adam
            from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
            from sklearn.preprocessing import StandardScaler
            
            print(f"ğŸ” TensorFlow ç‰ˆæœ¬: {tf.__version__}")
            
            # æª¢æŸ¥ GPU
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                print(f"ğŸš€ æª¢æ¸¬åˆ° {len(gpus)} å€‹ GPU")
                try:
                    for gpu in gpus:
                        tf.config.experimental.set_memory_growth(gpu, True)
                    print("âœ… GPU è¨˜æ†¶é«”å¢é•·å·²å•Ÿç”¨")
                except RuntimeError:
                    pass  # GPU å·²ç¶“åˆå§‹åŒ–
            else:
                print("ä½¿ç”¨ CPU è¨“ç·´")
            
            # ç°¡åŒ–çš„ç¥ç¶“ç¶²çµ¡åŒ…è£å™¨
            class KerasRegressorWrapper:
                def __init__(self, input_dim, epochs=50, batch_size=32): #CPU batch_size=16
                    self.input_dim = input_dim
                    self.epochs = epochs
                    self.batch_size = batch_size
                    self.model = None
                    self.scaler = StandardScaler()
                
                def _build_model(self):
                    model = Sequential([
                            Dense(256, activation='relu', input_shape=(self.input_dim,)),
                            Dropout(0.3),
                            Dense(512, activation='relu'),
                            Dropout(0.3),
                            Dense(256, activation='relu'),
                            Dropout(0.3),
                            Dense(512, activation='relu'),
                            Dropout(0.3),
                            Dense(128, activation='relu'),
                            Dropout(0.2),
                            Dense(64, activation='relu'),
                            Dense(1, activation='linear')
                        ])

                    # # V3: BatchNorm 
                    # model = Sequential([
                    #     Dense(512, input_shape=(self.input_dim,)),
                    #     BatchNormalization(),
                    #     Activation('relu'),
                    #     Dropout(0.3),
                        
                    #     Dense(256),
                    #     BatchNormalization(), 
                    #     Activation('relu'),
                    #     Dropout(0.3),
                        
                    #     Dense(128),
                    #     BatchNormalization(),
                    #     Activation('relu'), 
                    #     Dropout(0.2),
                        
                    #     Dense(64),
                    #     BatchNormalization(),
                    #     Activation('relu'),
                        
                    #     Dense(1, activation='linear')
                    # ])
                    
                    
                    model.compile(
                        optimizer=Adam(learning_rate=0.001),
                        loss='mse',
                        metrics=['mae']
                    )
                    return model
                
                def fit(self, X, y):
                    print(f"ğŸ§  é–‹å§‹è¨“ç·´ç¥ç¶“ç¶²çµ¡ - æ¨£æœ¬æ•¸: {len(X)}, ç‰¹å¾µæ•¸: {X.shape[1]}")
                    
                    # æ¨™æº–åŒ–ç‰¹å¾µ
                    X_scaled = self.scaler.fit_transform(X)
                    
                    # å»ºç«‹æ¨¡å‹
                    self.model = self._build_model()
                    
                    # è¨­å®šå›èª¿å‡½æ•¸
                    callbacks = [
                        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
                        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)
                    ]
                    
                    # è¨“ç·´æ¨¡å‹
                    self.model.fit(
                        X_scaled, y,
                        epochs=self.epochs,
                        batch_size=self.batch_size,
                        validation_split=0.2,
                        callbacks=callbacks,
                        verbose=1
                    )
                    
                    return self
                
                def predict(self, X):
                    if self.model is None:
                        raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´")
                    
                    X_scaled = self.scaler.transform(X)
                    return self.model.predict(X_scaled, verbose=0).flatten()
            
            # å‰µå»ºä¸€å€‹å·¥å» å‡½æ•¸ï¼Œåœ¨è¨“ç·´æ™‚å‹•æ…‹è¨­å®š input_dim
            def create_dl_models(input_dim):
                return {
                    'DeepLearning_NN': KerasRegressorWrapper(
                        input_dim=input_dim,
                        epochs=100,
                        batch_size=64
                    )
                }
            
            return create_dl_models
            
        except ImportError as e:
            print("âš ï¸ TensorFlow æœªå®‰è£ï¼Œè·³éæ·±åº¦å­¸ç¿’æ¨¡å‹")
            print(f"   éŒ¯èª¤: {e}")
            return {}
        except Exception as e:
            print(f"âŒ æ·±åº¦å­¸ç¿’æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
            return {}
    
    def train_multiple_models(self, models_dict, X_train, y_train, X_valid, y_valid):
        """è¨“ç·´å¤šå€‹æ¨¡å‹"""
        import time
        
        total_models = len(models_dict)
        print(f"\n{'='*60}")
        print(f"ğŸš€ é–‹å§‹è¨“ç·´ {total_models} å€‹æ¨¡å‹")
        print(f"{'='*60}")
        
        total_start_time = time.time()
        successful_models = 0
        failed_models = []
        
        for i, (model_name, model) in enumerate(models_dict.items(), 1):
            print(f"\nğŸ“Š é€²åº¦: {i}/{total_models}")
            try:
                # ç‰¹æ®Šè™•ç†æ·±åº¦å­¸ç¿’æ¨¡å‹
                if 'DeepLearning' in model_name and hasattr(model, 'input_dim'):
                    if model.input_dim is None:
                        model.input_dim = X_train.shape[1]
                
                self.train_single_model(model, model_name, X_train, y_train, X_valid, y_valid)
                successful_models += 1
                
            except Exception as e:
                print(f"   âŒ {model_name} è¨“ç·´å¤±æ•—: {e}")
                failed_models.append(model_name)
                if 'DeepLearning' in model_name:
                    print("     ğŸ’¡ æç¤ºï¼šæ·±åº¦å­¸ç¿’æ¨¡å‹éœ€è¦ TensorFlowï¼Œå¯è·³éæ­¤æ¨¡å‹")
                else:
                    import traceback
                    print("     ğŸ” è©³ç´°éŒ¯èª¤:")
                    traceback.print_exc()
        
        # ç¸½çµ
        total_time = time.time() - total_start_time
        print(f"\n{'='*60}")
        print(f"ğŸ‰ æ¨¡å‹è¨“ç·´å®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {successful_models}/{total_models} å€‹æ¨¡å‹")
        if failed_models:
            print(f"âŒ å¤±æ•—: {len(failed_models)} å€‹æ¨¡å‹ ({', '.join(failed_models)})")
        
        if total_time > 60:
            print(f"â±ï¸  ç¸½è€—æ™‚: {total_time/60:.1f} åˆ†é˜")
        else:
            print(f"â±ï¸  ç¸½è€—æ™‚: {total_time:.1f} ç§’")
        
        if successful_models > 0:
            avg_time = total_time / successful_models
            print(f"ğŸ“Š å¹³å‡æ¯æ¨¡å‹: {avg_time:.1f} ç§’")
        print(f"{'='*60}")
    
    def get_results_summary(self):
        """å–å¾—çµæœæ‘˜è¦"""
        summary_data = []
        
        for model_name, result in self.results.items():
            valid_metrics = result['valid']
            training_time = result.get('training_time', 0)
            
            summary_data.append({
                'Model': model_name,
                'Valid_RMSE': valid_metrics['RMSE'],
                'Valid_MAE': valid_metrics['MAE'],
                'Valid_R2': valid_metrics['R2'],
                'Valid_MAPE': valid_metrics['MAPE'],
                'Training_Time(s)': training_time
            })
        
        df_summary = pd.DataFrame(summary_data)
        df_summary = df_summary.sort_values('Valid_RMSE')
        
        # ç¾åŒ–è¼¸å‡º
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½æ’è¡Œæ¦œ (ä¾é©—è­‰é›† RMSE æ’åº)")
        print(f"{'='*80}")
        
        for i, row in df_summary.iterrows():
            rank = df_summary.index.get_loc(i) + 1
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else f"#{rank}"
            
            print(f"{medal} {row['Model']}")
            print(f"    RMSE: {row['Valid_RMSE']:.2f} | RÂ²: {row['Valid_R2']:.4f} | MAE: {row['Valid_MAE']:.2f}")
            print(f"    MAPE: {row['Valid_MAPE']:.2f}% | è¨“ç·´æ™‚é–“: {row['Training_Time(s)']:.1f}s")
            print()
        
        return df_summary
    
    def save_best_model(self, save_dir="models"):
        """å„²å­˜æœ€ä½³æ¨¡å‹"""
        if not self.results:
            print("æ²’æœ‰è¨“ç·´éçš„æ¨¡å‹!")
            return None, None
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹ï¼ˆValid RMSE æœ€ä½ï¼‰
        best_model_name = min(self.results.keys(), 
                            key=lambda x: self.results[x]['valid']['RMSE'])
        
        best_model = self.trained_models[best_model_name]
        
        # å»ºç«‹å„²å­˜ç›®éŒ„
        save_path = Path(save_dir)
        save_path.mkdir(exist_ok=True)
        
        # å„²å­˜æ¨¡å‹
        model_file = save_path / f"best_model_{best_model_name}.joblib"
        joblib.dump(best_model, model_file)
        
        print(f"æœ€ä½³æ¨¡å‹ {best_model_name} å·²å„²å­˜åˆ° {model_file}")
        
        return best_model_name, best_model
    
    def save_best_models_by_type(self, save_dir="models", timestamp=None):
        """åˆ†é¡åˆ¥å„²å­˜æœ€ä½³æ¨¡å‹"""
        if not self.results:
            print("æ²’æœ‰è¨“ç·´éçš„æ¨¡å‹!")
            return {}
        
        # å»ºç«‹å„²å­˜ç›®éŒ„
        save_path = Path(save_dir)
        save_path.mkdir(exist_ok=True)
        
        # å¦‚æœæ²’æœ‰æä¾›æ™‚é–“æˆ³è¨˜ï¼Œç”Ÿæˆä¸€å€‹
        if timestamp is None:
            from datetime import datetime
            timestamp = datetime.now().strftime("%m%d_%H%M")
        
        # åˆ†é¡æ¨¡å‹
        model_categories = {
            'tree': [],
            'linear': [],
            'deep_learning': []
        }
        
        for model_name in self.results.keys():
            if any(tree_type in model_name.lower() for tree_type in 
                   ['randomforest', 'xgboost', 'lightgbm', 'catboost', 'gradient']):
                model_categories['tree'].append(model_name)
            elif any(linear_type in model_name.lower() for linear_type in 
                     ['linear', 'ridge', 'lasso', 'elastic']):
                model_categories['linear'].append(model_name)
            elif 'deeplearning' in model_name.lower() or 'neural' in model_name.lower():
                model_categories['deep_learning'].append(model_name)
        
        best_models = {}
        
        # ç‚ºæ¯å€‹é¡åˆ¥æ‰¾å‡ºæœ€ä½³æ¨¡å‹ä¸¦å„²å­˜
        for category, model_names in model_categories.items():
            if not model_names:
                continue
                
            # æ‰¾å‡ºè©²é¡åˆ¥çš„æœ€ä½³æ¨¡å‹
            best_in_category = min(model_names, 
                                 key=lambda x: self.results[x]['valid']['RMSE'])
            
            best_model = self.trained_models[best_in_category]
            
            # ğŸ• ç”Ÿæˆå¸¶æ™‚é–“æˆ³è¨˜çš„æª”æ¡ˆåç¨±
            timestamped_filename = f"best_{category}_model_{best_in_category}_{timestamp}.joblib"
            model_file = save_path / timestamped_filename
            joblib.dump(best_model, model_file)
            
            best_models[category] = {
                'name': best_in_category,
                'timestamped_name': f"{best_in_category}_{timestamp}",
                'model': best_model,
                'rmse': self.results[best_in_category]['valid']['RMSE'],
                'r2': self.results[best_in_category]['valid']['R2'],
                'file': model_file,
                'timestamp': timestamp
            }
            
            print(f"æœ€ä½³{category}æ¨¡å‹ {best_in_category} å·²å„²å­˜åˆ° {model_file}")
            print(f"  RMSE: {self.results[best_in_category]['valid']['RMSE']:.2f}")
            print(f"  RÂ²: {self.results[best_in_category]['valid']['R2']:.4f}")
            print(f"  æ™‚é–“æˆ³è¨˜: {timestamp}")
        
        # æ‰¾å‡ºæ•´é«”æœ€ä½³æ¨¡å‹
        if best_models:
            overall_best_category = min(best_models.keys(), 
                                      key=lambda x: best_models[x]['rmse'])
            overall_best = best_models[overall_best_category]
            
            print(f"\nğŸ† æ•´é«”æœ€ä½³æ¨¡å‹: {overall_best['name']} ({overall_best_category})")
            print(f"   RMSE: {overall_best['rmse']:.2f}")
            print(f"   RÂ²: {overall_best['r2']:.4f}")
            
            # å„²å­˜æ•´é«”æœ€ä½³æ¨¡å‹çš„å‰¯æœ¬
            overall_best_file = save_path / f"overall_best_model_{overall_best['name']}.joblib"
            joblib.dump(overall_best['model'], overall_best_file)
            print(f"   æª”æ¡ˆ: {overall_best_file}")
            
            best_models['overall_best'] = overall_best
        
        return best_models
    
    def plot_predictions(self, model_names=None, figsize=(15, 10)):
        """ç¹ªè£½é æ¸¬çµæœåœ–"""
        if model_names is None:
            model_names = list(self.results.keys())
        
        n_models = len(model_names)
        cols = min(3, n_models)
        rows = (n_models + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=figsize)
        if n_models == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes.reshape(1, -1)
        
        for i, model_name in enumerate(model_names):
            if model_name not in self.results:
                continue
                
            row = i // cols
            col = i % cols
            ax = axes[row, col] if rows > 1 else axes[col]
            
            result = self.results[model_name]
            y_true = result['valid']['y_true'] if 'y_true' in result['valid'] else None
            y_pred = result['valid_pred']
            
            if y_true is not None:
                ax.scatter(y_true, y_pred, alpha=0.5)
                ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
                ax.set_xlabel('True Values')
                ax.set_ylabel('Predictions')
                ax.set_title(f'{model_name}\nRÂ² = {result["valid"]["R2"]:.4f}')
        
        plt.tight_layout()
        plt.show()
    
    def get_stacking_models(self, use_pretrained_dnn=True, X_sample=None):
        """å–å¾— Stacking é›†æˆæ¨¡å‹é…ç½® - ä½¿ç”¨å¯åºåˆ—åŒ–DNN
        
        Args:
            use_pretrained_dnn (bool): æ˜¯å¦ä½¿ç”¨é è¨“ç·´çš„ DNN æ¨¡å‹ï¼ˆå·²å»¢æ£„ï¼Œä¸€å¾‹é‡æ–°è¨“ç·´ï¼‰
            X_sample (array-like): æ¨£æœ¬è³‡æ–™ç”¨æ–¼æª¢æŸ¥ç‰¹å¾µç¶­åº¦
        """
        try:
            from sklearn.ensemble import StackingRegressor, VotingRegressor
            from sklearn.model_selection import KFold
            
            # ï¿½ ç¸½æ˜¯ä½¿ç”¨æ–°çš„å¯åºåˆ—åŒ– DNN åŒ…è£å™¨
            # ğŸ”„ æ ¹æ“šç”¨æˆ¶é¸æ“‡æ±ºå®š DNN ç­–ç•¥
            expected_features = X_sample.shape[1] if X_sample is not None else None
            
            if use_pretrained_dnn:
                # å˜—è©¦è¼‰å…¥é è¨“ç·´ DNN
                pretrained_dnn = self._load_pretrained_dnn(expected_features=expected_features)
                if pretrained_dnn:
                    print("âœ… æˆåŠŸè¼‰å…¥é è¨“ç·´ DNN æ¨¡å‹ç”¨æ–¼ Stacking")
                    dnn_estimator = pretrained_dnn
                else:
                    print("âš ï¸  é è¨“ç·´ DNN æ¨¡å‹ä¸å¯ç”¨æˆ–ä¸ç›¸å®¹")
                    print("ğŸ’¡ å°‡ä½¿ç”¨è¼•é‡ç´š Ridge å›æ­¸ä»£æ›¿ DNN")
                    dnn_estimator = Ridge(alpha=2.0, random_state=self.random_state)
            else:
                # å‰µå»ºæ–°çš„å¯åºåˆ—åŒ– DNN åŒ…è£å™¨
                try:
                    print("ğŸ§  å»ºç«‹æ–°çš„å¯åºåˆ—åŒ–DNNæ¨¡å‹ (å°‡é‡æ–°è¨“ç·´)...")
                    dnn_estimator = SerializableDNNWrapper(input_dim=expected_features)
                    print("âœ… æˆåŠŸå»ºç«‹å¯åºåˆ—åŒ–DNNæ¨¡å‹ç”¨æ–¼ Stacking")
                except Exception as e:
                    print(f"âŒ å»ºç«‹DNNå¤±æ•—: {e}")
                    print("ğŸ’¡ å°‡ä½¿ç”¨è¼•é‡ç´š Ridge å›æ­¸ä»£æ›¿ DNN")
                    dnn_estimator = Ridge(alpha=2.0, random_state=self.random_state)
            
            # åŸºå­¸ç¿’å™¨ï¼šDNN + XGBoost + LightGBM (è³‡æºå„ªåŒ–ç‰ˆ)
            base_learners = [
                ('xgb', xgb.XGBRegressor(
                    n_estimators=100,      # å¤§å¹…æ¸›å°‘æ¨¹æ•¸é‡
                    max_depth=5,           # æ¸›å°‘æ·±åº¦
                    learning_rate=0.1,     # æé«˜å­¸ç¿’ç‡è£œå„Ÿæ¨¹æ•¸é‡æ¸›å°‘
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_alpha=0.1,
                    reg_lambda=1.0,
                    random_state=self.random_state,
                    n_jobs=5,              # é™åˆ¶CPUæ ¸å¿ƒæ•¸
                    tree_method='hist',    # ä½¿ç”¨è¨˜æ†¶é«”å‹å–„çš„æ–¹æ³•
                    max_bin=64            # å¤§å¹…æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
                )),
                ('lgb', lgb.LGBMRegressor(
                    n_estimators=100,      # å¤§å¹…æ¸›å°‘æ¨¹æ•¸é‡
                    max_depth=5,           # æ¸›å°‘æ·±åº¦
                    learning_rate=0.1,     # æé«˜å­¸ç¿’ç‡
                    subsample=0.8,         # æ¨£æœ¬æ¡æ¨£æ¯”ä¾‹
                    colsample_bytree=0.8,  # ç‰¹å¾µæ¡æ¨£æ¯”ä¾‹
                    reg_alpha=0.1,         # L1 æ­£å‰‡åŒ–
                    reg_lambda=1.0,        # L2 æ­£å‰‡åŒ–
                    random_state=self.random_state,  # ä½¿ç”¨çµ±ä¸€çš„éš¨æ©Ÿç¨®å­
                    verbose=-1,            # ä¸é¡¯ç¤ºè¨“ç·´è³‡è¨Š
                    n_jobs=5,              # é™åˆ¶CPUæ ¸å¿ƒæ•¸
                    max_bin=64,            # å¤§å¹…æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
                    min_data_in_leaf=20,   # å¢åŠ æœ€å°è‘‰å­æ¨£æœ¬æ•¸
                    feature_fraction=0.8   # æ¸›å°‘ç‰¹å¾µä½¿ç”¨é‡
                )),
                ('dnn', dnn_estimator)  # ä½¿ç”¨å¯åºåˆ—åŒ–DNN
            ]
            
            return {
                # ğŸ† æ–¹æ¡ˆAï¼šDNN + æ¨¹æ¨¡å‹åŸºå­¸ç¿’å™¨ + Ridgeæœ€çµ‚å­¸ç¿’å™¨ (å¯åºåˆ—åŒ–ç‰ˆ)
                'Stacking_DNN_Trees_Ridge': StackingRegressor(
                    estimators=base_learners,
                    final_estimator=Ridge(alpha=1.0, random_state=self.random_state),
                    cv=KFold(n_splits=3, shuffle=True, random_state=self.random_state),  # æ¸›å°‘ CV fold
                    n_jobs=1,               # é™åˆ¶ä¸¦è¡Œè™•ç†é¿å…è³‡æºè¡çª
                    passthrough=False       # ä¸å‚³éåŸå§‹ç‰¹å¾µï¼Œæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
                ),
                
                # ğŸ¯ å‚™é¸æ–¹æ¡ˆï¼šä½¿ç”¨ç·šæ€§å›æ­¸ä½œç‚ºæœ€çµ‚å­¸ç¿’å™¨ (æ›´è¼•é‡)
                'Stacking_DNN_Trees_Linear': StackingRegressor(
                    estimators=base_learners,
                    final_estimator=LinearRegression(),
                    cv=KFold(n_splits=3, shuffle=True, random_state=self.random_state),  # æ¸›å°‘ CV fold
                    n_jobs=1,               # é™åˆ¶ä¸¦è¡Œè™•ç†
                    passthrough=False       # ä¸å‚³éåŸå§‹ç‰¹å¾µ
                ),
                
                # ğŸš€ Votingç‰ˆæœ¬ï¼šç°¡å–®æŠ•ç¥¨çµ„åˆ (æœ€è¼•é‡)
                'Voting_DNN_Trees': VotingRegressor(
                    estimators=base_learners,
                    n_jobs=1                # é™åˆ¶ä¸¦è¡Œè™•ç†
                )
            }
            
        except ImportError as e:
            print(f"âš ï¸  TensorFlow æœªå®‰è£ï¼Œç„¡æ³•ä½¿ç”¨ Stacking æ¨¡å‹: {e}")
            return {}
        except Exception as e:
            print(f"âŒ Stacking æ¨¡å‹é…ç½®å‡ºéŒ¯: {e}")
            return {}
            
        except ImportError as e:
            print(f"âš ï¸  TensorFlow æœªå®‰è£ï¼Œç„¡æ³•ä½¿ç”¨ Stacking æ¨¡å‹: {e}")
            return {}
        except Exception as e:
            print(f"âŒ Stacking æ¨¡å‹é…ç½®å‡ºéŒ¯: {e}")
            return {}
    
    def _load_pretrained_dnn(self, expected_features=None):
        """è¼‰å…¥é è¨“ç·´çš„ DNN æ¨¡å‹ç”¨æ–¼ Stacking
        
        Args:
            expected_features: æœŸæœ›çš„ç‰¹å¾µæ•¸é‡ï¼Œå¦‚æœæä¾›æœƒé€²è¡Œç›¸å®¹æ€§æª¢æŸ¥
        """
        try:
            import tensorflow as tf
            import joblib
            from pathlib import Path
            from sklearn.base import BaseEstimator, RegressorMixin
            
            model_dir = Path("../models")
            
            # ğŸ” å°‹æ‰¾æœ€æ–°çš„ DNN æ¨¡å‹
            keras_dirs = [d for d in model_dir.iterdir() 
                         if d.is_dir() and d.name.endswith('_keras')]
            
            if not keras_dirs:
                print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½• DNN æ¨¡å‹ç›®éŒ„")
                return None
            
            # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œé¸æ“‡æœ€æ–°çš„
            latest_keras_dir = max(keras_dirs, key=lambda x: x.stat().st_mtime)
            base_name = latest_keras_dir.name.replace('_keras', '')
            scaler_file = model_dir / f"{base_name}_scaler.joblib"
            
            if not scaler_file.exists():
                print(f"âš ï¸  æœªæ‰¾åˆ°å°æ‡‰çš„ scaler æª”æ¡ˆ: {scaler_file}")
                return None
            
            print(f"ğŸ”„ è¼‰å…¥é è¨“ç·´ DNN æ¨¡å‹: {base_name}")
            
            # è¼‰å…¥æ¨¡å‹å’Œ scaler
            model = tf.keras.models.load_model(latest_keras_dir)
            scaler = joblib.load(scaler_file)
            
            # æª¢æŸ¥ç‰¹å¾µç›¸å®¹æ€§
            if expected_features is not None:
                model_features = scaler.n_features_in_
                if model_features != expected_features:
                    print(f"âš ï¸  ç‰¹å¾µç¶­åº¦ä¸åŒ¹é…:")
                    print(f"   é è¨“ç·´æ¨¡å‹: {model_features} å€‹ç‰¹å¾µ")
                    print(f"   ç•¶å‰è³‡æ–™: {expected_features} å€‹ç‰¹å¾µ")
                    print(f"ğŸ’¡ å°‡ä½¿ç”¨ Ridge ä»£æ›¿ä¸ç›¸å®¹çš„é è¨“ç·´ DNN")
                    return None
                else:
                    print(f"âœ… ç‰¹å¾µç¶­åº¦åŒ¹é…: {model_features} å€‹ç‰¹å¾µ")
            
            # å‰µå»ºåŒ…è£å™¨
            class PretrainedDNNWrapper(BaseEstimator, RegressorMixin):
                """é è¨“ç·´ DNN æ¨¡å‹åŒ…è£å™¨ - æ”¯æ´ scikit-learn clone"""
                
                def __init__(self, model_path=None, scaler_path=None, model=None, scaler=None):
                    # å„²å­˜è·¯å¾‘è€Œä¸æ˜¯ç‰©ä»¶ï¼Œé¿å… deepcopy å•é¡Œ
                    self.model_path = model_path or latest_keras_dir
                    self.scaler_path = scaler_path or scaler_file
                    self._model = model
                    self._scaler = scaler
                    self.is_fitted = True
                
                def _load_model_if_needed(self):
                    """å»¶é²è¼‰å…¥æ¨¡å‹å’Œ scaler"""
                    if self._model is None or self._scaler is None:
                        try:
                            import tensorflow as tf
                            import joblib
                            self._model = tf.keras.models.load_model(self.model_path)
                            self._scaler = joblib.load(self.scaler_path)
                        except Exception as e:
                            print(f"âŒ é‡æ–°è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
                            raise
                
                def fit(self, X, y):
                    """å·²ç¶“æ˜¯é è¨“ç·´æ¨¡å‹ï¼Œæª¢æŸ¥ç‰¹å¾µç¶­åº¦ç›¸å®¹æ€§"""
                    print("      ğŸ”„ ä½¿ç”¨é è¨“ç·´ DNN æ¨¡å‹ (è·³éè¨“ç·´éšæ®µ)")
                    
                    # æª¢æŸ¥ç‰¹å¾µç¶­åº¦ç›¸å®¹æ€§
                    try:
                        self._load_model_if_needed()
                        expected_features = self._scaler.n_features_in_
                        actual_features = X.shape[1]
                        
                        if expected_features != actual_features:
                            print(f"      âš ï¸  ç‰¹å¾µç¶­åº¦ä¸åŒ¹é…ï¼")
                            print(f"         é è¨“ç·´æ¨¡å‹æœŸæœ›: {expected_features} å€‹ç‰¹å¾µ")
                            print(f"         ç•¶å‰è³‡æ–™å…·æœ‰: {actual_features} å€‹ç‰¹å¾µ")
                            print(f"      ğŸ’¡ å°‡æ¨™è¨˜ç‚ºä¸ç›¸å®¹ï¼ŒStacking æœƒè‡ªå‹•è™•ç†")
                            self._is_compatible = False
                        else:
                            print(f"      âœ… ç‰¹å¾µç¶­åº¦åŒ¹é… ({actual_features} å€‹ç‰¹å¾µ)")
                            self._is_compatible = True
                            
                    except Exception as e:
                        print(f"      âŒ é è¨“ç·´æ¨¡å‹æª¢æŸ¥å¤±æ•—: {e}")
                        self._is_compatible = False
                    
                    return self
                
                def predict(self, X):
                    """é æ¸¬ - å¦‚æœç‰¹å¾µä¸ç›¸å®¹å‰‡ä½¿ç”¨ç°¡å–®å‚™ç”¨ç­–ç•¥"""
                    import numpy as np
                    
                    # æª¢æŸ¥æ˜¯å¦ç›¸å®¹
                    if not hasattr(self, '_is_compatible'):
                        # é¦–æ¬¡èª¿ç”¨ï¼Œé€²è¡Œæª¢æŸ¥
                        try:
                            self._load_model_if_needed()
                            expected_features = self._scaler.n_features_in_
                            actual_features = X.shape[1]
                            self._is_compatible = (expected_features == actual_features)
                        except:
                            self._is_compatible = False
                    
                    if not self._is_compatible:
                        # ç‰¹å¾µä¸ç›¸å®¹ï¼Œä½¿ç”¨ç°¡å–®çš„ç·šæ€§é æ¸¬ä½œç‚ºå‚™ç”¨
                        print("      âš ï¸  ç‰¹å¾µç¶­åº¦ä¸ç›¸å®¹ï¼Œä½¿ç”¨å‚™ç”¨é æ¸¬ç­–ç•¥")
                        # ç°¡å–®çš„ç·šæ€§çµ„åˆä½œç‚ºå‚™ç”¨
                        if not hasattr(self, '_backup_weights'):
                            np.random.seed(42)
                            self._backup_weights = np.random.randn(X.shape[1]) * 0.1
                        
                        predictions = X @ self._backup_weights + np.random.randn(len(X)) * 0.01
                        return predictions.flatten()
                    
                    # æ­£å¸¸é æ¸¬
                    self._load_model_if_needed()
                    X_scaled = self._scaler.transform(X)
                    predictions = self._model.predict(X_scaled, verbose=0)
                    return predictions.flatten()
                
                def get_params(self, deep=True):
                    """ç²å–åƒæ•¸ - è¿”å›è·¯å¾‘è€Œä¸æ˜¯ç‰©ä»¶"""
                    return {
                        'model_path': self.model_path,
                        'scaler_path': self.scaler_path,
                        'model': None,  # ä¸è¿”å› TensorFlow ç‰©ä»¶
                        'scaler': None
                    }
                
                def set_params(self, **params):
                    """è¨­ç½®åƒæ•¸"""
                    for key, value in params.items():
                        if key in ['model_path', 'scaler_path']:
                            setattr(self, key, value)
                        elif key in ['model', 'scaler']:
                            setattr(self, f'_{key}', value)
                    return self
                
                def __deepcopy__(self, memo):
                    """è‡ªå®šç¾©æ·±æ‹·è²è¡Œç‚º - é¿å… TensorFlow ç‰©ä»¶æ‹·è²"""
                    # å‰µå»ºæ–°å¯¦ä¾‹ï¼Œåªæ‹·è²è·¯å¾‘
                    new_instance = PretrainedDNNWrapper(
                        model_path=self.model_path,
                        scaler_path=self.scaler_path,
                        model=None,  # ä¸æ‹·è² TensorFlow ç‰©ä»¶
                        scaler=None
                    )
                    return new_instance
            
            wrapper = PretrainedDNNWrapper(
                model_path=latest_keras_dir,
                scaler_path=scaler_file,
                model=model,
                scaler=scaler
            )
            print(f"âœ… æˆåŠŸè¼‰å…¥é è¨“ç·´ DNN: {base_name}")
            return wrapper
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥é è¨“ç·´ DNN å¤±æ•—: {e}")
            return None
    
    def list_available_dnn_models(self):
        """åˆ—å‡ºå¯ç”¨çš„é è¨“ç·´ DNN æ¨¡å‹"""
        try:
            from pathlib import Path
            import os
            
            model_dir = Path("../models")
            
            # å°‹æ‰¾ DNN æ¨¡å‹
            keras_dirs = [d for d in model_dir.iterdir() 
                         if d.is_dir() and d.name.endswith('_keras')]
            
            if not keras_dirs:
                print("ğŸ“ æœªæ‰¾åˆ°ä»»ä½•é è¨“ç·´ DNN æ¨¡å‹")
                return []
            
            print("ğŸ“‹ å¯ç”¨çš„é è¨“ç·´ DNN æ¨¡å‹:")
            available_models = []
            
            for i, keras_dir in enumerate(sorted(keras_dirs, key=lambda x: x.stat().st_mtime, reverse=True), 1):
                base_name = keras_dir.name.replace('_keras', '')
                scaler_file = model_dir / f"{base_name}_scaler.joblib"
                info_file = model_dir / f"{base_name}_info.txt"
                
                # æª¢æŸ¥æª”æ¡ˆå®Œæ•´æ€§
                if scaler_file.exists():
                    # ç²å–ä¿®æ”¹æ™‚é–“
                    mod_time = os.path.getmtime(keras_dir)
                    import datetime
                    mod_time_str = datetime.datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M')
                    
                    # å˜—è©¦è®€å–æ€§èƒ½è³‡è¨Š
                    performance_info = ""
                    if info_file.exists():
                        try:
                            with open(info_file, 'r', encoding='utf-8') as f:
                                content = f.read()
                                if 'MAE:' in content:
                                    mae_line = [line for line in content.split('\n') if 'MAE:' in line]
                                    if mae_line:
                                        performance_info = f" | {mae_line[0].split('MAE:')[-1].strip()}"
                        except:
                            pass
                    
                    print(f"{i}. {base_name} (ä¿®æ”¹æ™‚é–“: {mod_time_str}{performance_info})")
                    available_models.append({
                        'name': base_name,
                        'keras_dir': keras_dir,
                        'scaler_file': scaler_file,
                        'mod_time': mod_time
                    })
                else:
                    print(f"{i}. {base_name} âŒ (ç¼ºå°‘ scaler æª”æ¡ˆ)")
            
            return available_models
            
        except Exception as e:
            print(f"âŒ åˆ—å‡º DNN æ¨¡å‹æ™‚å‡ºéŒ¯: {e}")
            return []
    
    def _get_stacking_dnn_estimator(self, input_dim):
        """ç²å–ç”¨æ–¼ Stacking çš„ DNN ä¼°è¨ˆå™¨ - ä½¿ç”¨å¯åºåˆ—åŒ–åŒ…è£å™¨"""
        try:
            print(f"ğŸ§  å‰µå»ºå¯åºåˆ—åŒ– DNN ä¼°è¨ˆå™¨ (ç‰¹å¾µæ•¸: {input_dim})...")
            return SerializableDNNWrapper(input_dim=input_dim)
        except Exception as e:
            print(f"âš ï¸  ç„¡æ³•å‰µå»º DNN ä¼°è¨ˆå™¨: {e}")
            print("ï¿½ ä½¿ç”¨ Ridge å›æ­¸ä½œç‚ºæ›¿ä»£")
            return Ridge(alpha=1.0, random_state=self.random_state)

    def predict_test(self, model_name, X_test):
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹é æ¸¬æ¸¬è©¦é›†"""
        if model_name not in self.trained_models:
            raise ValueError(f"æ¨¡å‹ {model_name} å°šæœªè¨“ç·´!")
        
        model = self.trained_models[model_name]
        test_pred = model.predict(X_test)
        
        return test_pred