#!/usr/bin/env python3
"""
æ•¸æ“šé è™•ç† V2 - åŒ…å«ç‰¹å¾µé—œä¿‚åˆ†æå’Œå¯è‡ªå®šç¾©çš„æ•¸æ“šè™•ç†
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import os
import sys
from scipy.stats import pearsonr, spearmanr
from sklearn.feature_selection import mutual_info_regression
from sklearn.preprocessing import LabelEncoder
from datetime import datetime

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

# åŠ å…¥ src ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))
from data_preprocess import load_data

class DataAnalysisV2:
    """æ•¸æ“šåˆ†æå’Œé è™•ç† V2"""
    
    def __init__(self):
        self.train_df = None
        self.valid_df = None
        self.test_df = None
        self.analysis_results = {}
        
    def load_data(self):
        """è¼‰å…¥åŸå§‹æ•¸æ“š"""
        print("=== è¼‰å…¥åŸå§‹æ•¸æ“š ===")
        self.train_df, self.valid_df, self.test_df = load_data()
        print(f"âœ… æ•¸æ“šè¼‰å…¥å®Œæˆ!")
        print(f"   è¨“ç·´é›†: {self.train_df.shape}")
        print(f"   é©—è­‰é›†: {self.valid_df.shape}")
        print(f"   æ¸¬è©¦é›†: {self.test_df.shape}")
        
    def analyze_data_relationship(self, target_column='ç¸½åƒ¹å…ƒ', save_results=True):
        """åˆ†ææ•¸æ“šç‰¹å¾µèˆ‡ç›®æ¨™è®Šæ•¸çš„é—œä¿‚"""
        if self.train_df is None:
            print("âŒ è«‹å…ˆè¼‰å…¥æ•¸æ“š!")
            return None
            
        print("\n=== é–‹å§‹ç‰¹å¾µé—œä¿‚åˆ†æ ===")
        
        # 1. åŸºæœ¬çµ±è¨ˆ
        print("\n1. ç›®æ¨™è®Šæ•¸åŸºæœ¬çµ±è¨ˆ:")
        target_stats = {
            'count': self.train_df[target_column].count(),
            'mean': self.train_df[target_column].mean(),
            'median': self.train_df[target_column].median(),
            'std': self.train_df[target_column].std(),
            'min': self.train_df[target_column].min(),
            'max': self.train_df[target_column].max(),
            'skewness': self.train_df[target_column].skew()
        }
        
        for key, value in target_stats.items():
            print(f"   {key}: {value:,.2f}")
        
        # 2. ç‰¹å¾µé¡å‹åˆ†æ
        print(f"\n2. ç‰¹å¾µé¡å‹åˆ†æ:")
        numeric_features = []
        categorical_features = []
        
        for col in self.train_df.columns:
            if col == target_column:
                continue
                
            if self.train_df[col].dtype in ['int64', 'float64']:
                # æª¢æŸ¥æ˜¯å¦ç‚ºé¡åˆ¥å‹æ•¸å€¼ (å”¯ä¸€å€¼å¾ˆå°‘)
                unique_vals = self.train_df[col].nunique()
                if unique_vals <= 20:  # å¯èƒ½æ˜¯é¡åˆ¥å‹
                    print(f"   ğŸ” {col}: æ•¸å€¼å‹ä½†å”¯ä¸€å€¼å°‘ ({unique_vals}å€‹), å¯èƒ½æ˜¯é¡åˆ¥å‹")
                numeric_features.append(col)
            else:
                categorical_features.append(col)
        
        print(f"   ğŸ“Š æ•¸å€¼å‹ç‰¹å¾µ: {len(numeric_features)}")
        print(f"   ğŸ“‹ é¡åˆ¥å‹ç‰¹å¾µ: {len(categorical_features)}")
        
        # 3. ç›¸é—œæ€§åˆ†æ
        print(f"\n3. ç›¸é—œæ€§åˆ†æ:")
        correlations = self._calculate_correlations(numeric_features, target_column)
        
        # 4. äº’ä¿¡æ¯åˆ†æ
        print(f"\n4. äº’ä¿¡æ¯åˆ†æ:")
        mi_scores = self._calculate_mutual_information(numeric_features, target_column)
        
        # 5. ç¶œåˆæ’åº
        print(f"\n5. ç¶œåˆç‰¹å¾µé‡è¦æ€§:")
        ranked_features = self._rank_features(correlations, mi_scores)
        
        # 6. ç¼ºå¤±å€¼åˆ†æ
        print(f"\n6. ç¼ºå¤±å€¼åˆ†æ:")
        missing_analysis = self._analyze_missing_values()
        
        # å„²å­˜çµæœ
        self.analysis_results = {
            'target_stats': target_stats,
            'numeric_features': numeric_features,
            'categorical_features': categorical_features,
            'correlations': correlations,
            'mutual_info': mi_scores,
            'ranked_features': ranked_features,
            'missing_analysis': missing_analysis
        }
        
        # ç”Ÿæˆå ±å‘Š
        if save_results:
            self._generate_analysis_report()
            self._plot_feature_importance()
        
        return self.analysis_results
    
    def _calculate_correlations(self, numeric_features, target_column):
        """è¨ˆç®—ç›¸é—œæ€§"""
        correlations = {}
        
        print(f"   ğŸ“ˆ è¨ˆç®—çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸...")
        for feature in numeric_features:
            try:
                # ç§»é™¤ç¼ºå¤±å€¼
                mask = ~(self.train_df[feature].isna() | self.train_df[target_column].isna())
                if mask.sum() < 10:  # æ¨£æœ¬å¤ªå°‘
                    continue
                    
                x = self.train_df.loc[mask, feature]
                y = self.train_df.loc[mask, target_column]
                
                pearson_r, pearson_p = pearsonr(x, y)
                spearman_r, spearman_p = spearmanr(x, y)
                
                correlations[feature] = {
                    'pearson_r': pearson_r,
                    'pearson_p': pearson_p,
                    'spearman_r': spearman_r,
                    'spearman_p': spearman_p,
                    'abs_pearson': abs(pearson_r)
                }
                
            except Exception as e:
                print(f"      è¨ˆç®— {feature} ç›¸é—œæ€§æ™‚å‡ºéŒ¯: {e}")
        
        # æŒ‰çµ•å°å€¼æ’åº
        sorted_corr = sorted(correlations.items(), key=lambda x: x[1]['abs_pearson'], reverse=True)
        
        print(f"   ğŸ” ç›¸é—œæ€§ Top 10:")
        for i, (feature, stats) in enumerate(sorted_corr[:10]):
            significance = "***" if stats['pearson_p'] < 0.001 else "**" if stats['pearson_p'] < 0.01 else "*" if stats['pearson_p'] < 0.05 else ""
            print(f"      {i+1:2d}. {feature[:30]:30s}: {stats['pearson_r']:7.4f} {significance}")
        
        return correlations
    
    def _calculate_mutual_information(self, numeric_features, target_column):
        """è¨ˆç®—äº’ä¿¡æ¯"""
        mi_scores = {}
        
        try:
            print(f"   ğŸ§  è¨ˆç®—äº’ä¿¡æ¯åˆ†æ•¸...")
            
            # æº–å‚™æ•¸æ“š
            feature_data = []
            feature_names = []
            
            for feature in numeric_features:
                if feature in self.train_df.columns:
                    # å¡«å……ç¼ºå¤±å€¼
                    values = self.train_df[feature].fillna(self.train_df[feature].median())
                    feature_data.append(values)
                    feature_names.append(feature)
            
            if not feature_data:
                print(f"      âŒ æ²’æœ‰å¯ç”¨çš„æ•¸å€¼ç‰¹å¾µ")
                return mi_scores
            
            X = np.column_stack(feature_data)
            y = self.train_df[target_column]
            
            # è¨ˆç®—äº’ä¿¡æ¯
            mi_values = mutual_info_regression(X, y, random_state=42)
            
            for feature, score in zip(feature_names, mi_values):
                mi_scores[feature] = score
            
            # æŒ‰åˆ†æ•¸æ’åº
            sorted_mi = sorted(mi_scores.items(), key=lambda x: x[1], reverse=True)
            
            print(f"   ğŸ” äº’ä¿¡æ¯ Top 10:")
            for i, (feature, score) in enumerate(sorted_mi[:10]):
                print(f"      {i+1:2d}. {feature[:30]:30s}: {score:.6f}")
                
        except Exception as e:
            print(f"   âŒ äº’ä¿¡æ¯è¨ˆç®—å‡ºéŒ¯: {e}")
        
        return mi_scores
    
    def _rank_features(self, correlations, mi_scores):
        """ç¶œåˆæ’åºç‰¹å¾µ"""
        combined_scores = {}
        
        # æ­£è¦åŒ–äº’ä¿¡æ¯åˆ†æ•¸
        max_mi = max(mi_scores.values()) if mi_scores else 1
        
        for feature in correlations.keys():
            pearson_score = correlations[feature]['abs_pearson']
            mi_score = mi_scores.get(feature, 0) / max_mi
            
            # ç¶œåˆåˆ†æ•¸ (60% ç›¸é—œæ€§ + 40% äº’ä¿¡æ¯)
            combined_score = 0.6 * pearson_score + 0.4 * mi_score
            
            combined_scores[feature] = {
                'combined_score': combined_score,
                'pearson_r': correlations[feature]['pearson_r'],
                'pearson_abs': pearson_score,
                'mutual_info': mi_scores.get(feature, 0),
                'p_value': correlations[feature]['pearson_p']
            }
        
        # æ’åº
        ranked = sorted(combined_scores.items(), key=lambda x: x[1]['combined_score'], reverse=True)
        
        print(f"   ğŸ† ç¶œåˆé‡è¦æ€§ Top 15:")
        print(f"   {'æ’å':<4} {'ç‰¹å¾µåç¨±':<30} {'ç¶œåˆåˆ†æ•¸':<10} {'ç›¸é—œä¿‚æ•¸':<10} {'äº’ä¿¡æ¯':<10}")
        print(f"   {'-'*70}")
        
        for i, (feature, scores) in enumerate(ranked[:15]):
            print(f"   {i+1:<4} {feature[:30]:<30} {scores['combined_score']:<10.4f} "
                  f"{scores['pearson_r']:<10.4f} {scores['mutual_info']:<10.6f}")
        
        return ranked
    
    def _analyze_missing_values(self):
        """åˆ†æç¼ºå¤±å€¼"""
        missing_info = {}
        
        print(f"   ğŸ” ç¼ºå¤±å€¼çµ±è¨ˆ:")
        total_samples = len(self.train_df)
        
        for col in self.train_df.columns:
            missing_count = self.train_df[col].isna().sum()
            missing_pct = (missing_count / total_samples) * 100
            
            if missing_count > 0:
                missing_info[col] = {
                    'count': missing_count,
                    'percentage': missing_pct
                }
        
        if missing_info:
            # æŒ‰ç¼ºå¤±æ¯”ä¾‹æ’åº
            sorted_missing = sorted(missing_info.items(), key=lambda x: x[1]['percentage'], reverse=True)
            
            print(f"      ç™¼ç¾ {len(missing_info)} å€‹ç‰¹å¾µæœ‰ç¼ºå¤±å€¼:")
            for feature, info in sorted_missing[:10]:  # åªé¡¯ç¤ºå‰10å€‹
                print(f"      â€¢ {feature[:30]:30s}: {info['count']:4d} ({info['percentage']:5.1f}%)")
        else:
            print(f"      âœ… æ²’æœ‰ç™¼ç¾ç¼ºå¤±å€¼")
        
        return missing_info
    
    def _generate_analysis_report(self):
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        timestamp = datetime.now().strftime("%m%d_%H%M")
        
        # å»ºç«‹çµæœç›®éŒ„
        results_dir = Path("../results")
        results_dir.mkdir(exist_ok=True)
        
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("ğŸ  æˆ¿åœ°ç”¢æ•¸æ“šç‰¹å¾µé—œä¿‚åˆ†æå ±å‘Š V2")
        report_lines.append("=" * 60)
        report_lines.append(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # åŸºæœ¬çµ±è¨ˆ
        target_stats = self.analysis_results['target_stats']
        report_lines.append(f"\nğŸ“Š ç›®æ¨™è®Šæ•¸çµ±è¨ˆ (ç¸½åƒ¹å…ƒ):")
        report_lines.append(f"  æ¨£æœ¬æ•¸: {target_stats['count']:,}")
        report_lines.append(f"  å¹³å‡å€¼: {target_stats['mean']:,.0f}")
        report_lines.append(f"  ä¸­ä½æ•¸: {target_stats['median']:,.0f}")
        report_lines.append(f"  æ¨™æº–å·®: {target_stats['std']:,.0f}")
        report_lines.append(f"  ç¯„åœ: {target_stats['min']:,.0f} ~ {target_stats['max']:,.0f}")
        
        # ç‰¹å¾µçµ±è¨ˆ
        report_lines.append(f"\nğŸ“ˆ ç‰¹å¾µçµ±è¨ˆ:")
        report_lines.append(f"  æ•¸å€¼å‹ç‰¹å¾µ: {len(self.analysis_results['numeric_features'])}")
        report_lines.append(f"  é¡åˆ¥å‹ç‰¹å¾µ: {len(self.analysis_results['categorical_features'])}")
        
        # æœ€é‡è¦ç‰¹å¾µ
        report_lines.append(f"\nğŸ† æœ€é‡è¦çš„15å€‹ç‰¹å¾µ:")
        for i, (feature, scores) in enumerate(self.analysis_results['ranked_features'][:15]):
            report_lines.append(f"  {i+1:2d}. {feature:<30} (ç¶œåˆåˆ†æ•¸: {scores['combined_score']:.4f})")
        
        # é«˜ç›¸é—œç‰¹å¾µ
        report_lines.append(f"\nğŸ”— é«˜åº¦ç›¸é—œç‰¹å¾µ (|r| > 0.3):")
        high_corr = []
        for feature, stats in self.analysis_results['correlations'].items():
            if abs(stats['pearson_r']) > 0.3:
                high_corr.append((feature, stats['pearson_r']))
        
        if high_corr:
            high_corr.sort(key=lambda x: abs(x[1]), reverse=True)
            for feature, corr in high_corr:
                direction = "æ­£ç›¸é—œ" if corr > 0 else "è² ç›¸é—œ"
                report_lines.append(f"  â€¢ {feature:<30}: {corr:7.4f} ({direction})")
        else:
            report_lines.append("  æ²’æœ‰ç™¼ç¾é«˜åº¦ç›¸é—œç‰¹å¾µ")
        
        # ç¼ºå¤±å€¼åˆ†æ
        missing_info = self.analysis_results['missing_analysis']
        if missing_info:
            report_lines.append(f"\nâš ï¸ ç¼ºå¤±å€¼åˆ†æ:")
            sorted_missing = sorted(missing_info.items(), key=lambda x: x[1]['percentage'], reverse=True)
            for feature, info in sorted_missing[:10]:
                report_lines.append(f"  â€¢ {feature:<30}: {info['percentage']:5.1f}% ({info['count']} ç­†)")
        
        # å„²å­˜å ±å‘Š
        report_text = "\n".join(report_lines)
        report_file = results_dir / f"data_analysis_report_v2_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"\nğŸ“„ åˆ†æå ±å‘Šå·²å„²å­˜: {report_file}")
        
        # ä¹Ÿæ‰“å°åˆ°æ§åˆ¶å°
        print(f"\n{report_text}")
    
    def _plot_feature_importance(self):
        """ç¹ªè£½ç‰¹å¾µé‡è¦æ€§åœ–è¡¨"""
        try:
            ranked_features = self.analysis_results['ranked_features']
            
            # å–å‰15å€‹ç‰¹å¾µ
            top_15 = ranked_features[:15]
            features = [item[0][:25] for item, _ in enumerate(top_15)]  # æˆªçŸ­ç‰¹å¾µå
            scores = [item[1]['combined_score'] for item in top_15]
            correlations = [item[1]['pearson_r'] for item in top_15]
            
            # å‰µå»ºåœ–è¡¨
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
            
            # åœ–1: ç¶œåˆé‡è¦æ€§åˆ†æ•¸
            y_pos = np.arange(len(features))
            bars1 = ax1.barh(y_pos, scores, color='skyblue', alpha=0.7)
            ax1.set_yticks(y_pos)
            ax1.set_yticklabels([item[0][:25] for item in top_15])
            ax1.set_xlabel('ç¶œåˆé‡è¦æ€§åˆ†æ•¸')
            ax1.set_title('ç‰¹å¾µé‡è¦æ€§æ’åº (ç¶œåˆåˆ†æ•¸)')
            ax1.grid(axis='x', alpha=0.3)
            
            # åœ¨æ¢å½¢ä¸Šé¡¯ç¤ºæ•¸å€¼
            for i, bar in enumerate(bars1):
                width = bar.get_width()
                ax1.text(width + 0.001, bar.get_y() + bar.get_height()/2, 
                        f'{width:.3f}', ha='left', va='center', fontsize=9)
            
            # åœ–2: ç›¸é—œä¿‚æ•¸
            colors = ['red' if x < 0 else 'green' for x in correlations]
            bars2 = ax2.barh(y_pos, correlations, color=colors, alpha=0.7)
            ax2.set_yticks(y_pos)
            ax2.set_yticklabels([item[0][:25] for item in top_15])
            ax2.set_xlabel('çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸')
            ax2.set_title('ç‰¹å¾µèˆ‡å”®åƒ¹ç›¸é—œæ€§')
            ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)
            ax2.grid(axis='x', alpha=0.3)
            
            # åœ¨æ¢å½¢ä¸Šé¡¯ç¤ºæ•¸å€¼
            for i, bar in enumerate(bars2):
                width = bar.get_width()
                ax2.text(width + (0.01 if width >= 0 else -0.01), 
                        bar.get_y() + bar.get_height()/2, 
                        f'{width:.3f}', 
                        ha='left' if width >= 0 else 'right', va='center', fontsize=9)
            
            plt.tight_layout()
            
            # å„²å­˜åœ–è¡¨
            timestamp = datetime.now().strftime("%m%d_%H%M")
            plot_file = Path("../results") / f"feature_importance_v2_{timestamp}.png"
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            
            print(f"ğŸ“Š ç‰¹å¾µé‡è¦æ€§åœ–è¡¨å·²å„²å­˜: {plot_file}")
            plt.show()
            
        except Exception as e:
            print(f"âŒ ç¹ªåœ–å‡ºéŒ¯: {e}")
    
    def process_data_custom(self):
        """è‡ªå®šç¾©æ•¸æ“šè™•ç† - å¯ä»¥æ ¹æ“šåˆ†æçµæœé€²è¡Œç‰¹å¾µå·¥ç¨‹"""
        if self.train_df is None:
            print("âŒ è«‹å…ˆè¼‰å…¥æ•¸æ“š!")
            return None
            
        print("\n=== é–‹å§‹è‡ªå®šç¾©æ•¸æ“šè™•ç† ===")
        
        # è¤‡è£½æ•¸æ“šé˜²æ­¢ä¿®æ”¹åŸå§‹æ•¸æ“š
        train_processed = self.train_df.copy()
        valid_processed = self.valid_df.copy()
        test_processed = self.test_df.copy()
        
        print("ğŸ“ ä»¥ä¸‹æ˜¯éœ€è¦ä½ è‡ªè¡Œä¿®æ”¹çš„æ•¸æ“šè™•ç†éƒ¨åˆ†:")
        print("-" * 50)
        
        # ============ åœ¨é€™è£¡æ·»åŠ ä½ çš„æ•¸æ“šè™•ç†é‚è¼¯ ============
        
        # 1. è™•ç†ç¼ºå¤±å€¼
        print("1. è™•ç†ç¼ºå¤±å€¼:")
        print("   # TODO: æ ¹æ“šåˆ†æçµæœè™•ç†ç¼ºå¤±å€¼")
        print("   # ç¯„ä¾‹:")
        print("   # train_processed['æŸæ¬„ä½'].fillna(train_processed['æŸæ¬„ä½'].median(), inplace=True)")
        
        # 2. ç‰¹å¾µå·¥ç¨‹ - æ ¹æ“šåˆ†æçµæœå‰µå»ºæ–°ç‰¹å¾µ
        print("\n2. ç‰¹å¾µå·¥ç¨‹:")
        print("   # TODO: æ ¹æ“šç›¸é—œæ€§åˆ†æçµæœå‰µå»ºæ–°ç‰¹å¾µ")
        print("   # ç¯„ä¾‹:")
        print("   # train_processed['æ–°ç‰¹å¾µ'] = train_processed['ç‰¹å¾µ1'] * train_processed['ç‰¹å¾µ2']")
        
        # 3. é¡åˆ¥å‹ç‰¹å¾µç·¨ç¢¼
        print("\n3. é¡åˆ¥å‹ç‰¹å¾µç·¨ç¢¼:")
        print("   # TODO: å°é¡åˆ¥å‹ç‰¹å¾µé€²è¡Œç·¨ç¢¼")
        categorical_features = self.analysis_results.get('categorical_features', [])
        if categorical_features:
            print(f"   # ç™¼ç¾çš„é¡åˆ¥å‹ç‰¹å¾µ: {categorical_features[:5]}...")
            print("   # ç¯„ä¾‹:")
            print("   # le = LabelEncoder()")
            print("   # train_processed['æŸé¡åˆ¥æ¬„ä½'] = le.fit_transform(train_processed['æŸé¡åˆ¥æ¬„ä½'])")
        
        # 4. æ•¸å€¼ç‰¹å¾µè™•ç†
        print("\n4. æ•¸å€¼ç‰¹å¾µè™•ç†:")
        if hasattr(self, 'analysis_results') and 'ranked_features' in self.analysis_results:
            top_features = [item[0] for item in self.analysis_results['ranked_features'][:10]]
            print(f"   # é‡è¦æ•¸å€¼ç‰¹å¾µ (å‰10å€‹): {top_features}")
            print("   # TODO: å°é‡è¦ç‰¹å¾µé€²è¡Œè®Šæ›æˆ–æ­£è¦åŒ–")
            print("   # ç¯„ä¾‹:")
            print("   # train_processed['é‡è¦ç‰¹å¾µ_log'] = np.log1p(train_processed['é‡è¦ç‰¹å¾µ'])")
        
        # 5. ç•°å¸¸å€¼è™•ç†
        print("\n5. ç•°å¸¸å€¼è™•ç†:")
        print("   # TODO: æ ¹æ“šéœ€è¦è™•ç†ç•°å¸¸å€¼")
        print("   # ç¯„ä¾‹:")
        print("   # Q1 = train_processed['æŸæ¬„ä½'].quantile(0.25)")
        print("   # Q3 = train_processed['æŸæ¬„ä½'].quantile(0.75)")
        print("   # IQR = Q3 - Q1")
        print("   # train_processed = train_processed[~((train_processed['æŸæ¬„ä½'] < (Q1 - 1.5 * IQR)) | (train_processed['æŸæ¬„ä½'] > (Q3 + 1.5 * IQR)))]")
        
        # 6. ç‰¹å¾µé¸æ“‡
        print("\n6. ç‰¹å¾µé¸æ“‡:")
        print("   # TODO: æ ¹æ“šåˆ†æçµæœé¸æ“‡é‡è¦ç‰¹å¾µ")
        print("   # ç¯„ä¾‹:")
        print("   # selected_features = ['é‡è¦ç‰¹å¾µ1', 'é‡è¦ç‰¹å¾µ2', ...]")
        print("   # train_processed = train_processed[selected_features + ['ç¸½åƒ¹å…ƒ']]")
        
        print("\n" + "=" * 50)
        print("ğŸ’¡ ä¿®æ”¹å»ºè­°:")
        if hasattr(self, 'analysis_results'):
            if self.analysis_results.get('missing_analysis'):
                print("â€¢ å„ªå…ˆè™•ç†ç¼ºå¤±å€¼è¼ƒå¤šçš„ç‰¹å¾µ")
            
            ranked_features = self.analysis_results.get('ranked_features', [])
            if ranked_features:
                print(f"â€¢ é‡é»é—œæ³¨å‰10å€‹é‡è¦ç‰¹å¾µ: {[item[0] for item in ranked_features[:10]]}")
            
            high_corr_features = []
            correlations = self.analysis_results.get('correlations', {})
            for feature, stats in correlations.items():
                if abs(stats['pearson_r']) > 0.3:
                    high_corr_features.append(feature)
            
            if high_corr_features:
                print(f"â€¢ è€ƒæ…®å°é«˜ç›¸é—œç‰¹å¾µé€²è¡Œç‰¹å¾µå·¥ç¨‹: {high_corr_features[:5]}...")
        
        print("\nâš ï¸  æ³¨æ„: è«‹åœ¨ä¸Šé¢çš„ TODO éƒ¨åˆ†æ·»åŠ ä½ çš„æ•¸æ“šè™•ç†ä»£ç¢¼")
        print("ä¿®æ”¹å®Œæˆå¾Œï¼Œå¯ä»¥èª¿ç”¨ save_processed_data() å„²å­˜è™•ç†å¾Œçš„æ•¸æ“š")
        
        return train_processed, valid_processed, test_processed
    
    def save_processed_data(self, train_df, valid_df, test_df):
        """å„²å­˜è™•ç†å¾Œçš„æ•¸æ“š"""
        print("\n=== å„²å­˜è™•ç†å¾Œçš„æ•¸æ“š ===")
        
        # å»ºç«‹ç›®éŒ„
        processed_dir = Path("../Dataset/processed")
        processed_dir.mkdir(exist_ok=True)
        
        # å„²å­˜
        train_df.to_csv(processed_dir / "train_processed_v2.csv", index=False, encoding='utf-8-sig')
        valid_df.to_csv(processed_dir / "valid_processed_v2.csv", index=False, encoding='utf-8-sig')
        test_df.to_csv(processed_dir / "test_processed_v2.csv", index=False, encoding='utf-8-sig')
        
        print(f"âœ… è™•ç†å¾Œæ•¸æ“šå·²å„²å­˜åˆ°: {processed_dir}")
        print(f"   - train_processed_v2.csv: {train_df.shape}")
        print(f"   - valid_processed_v2.csv: {valid_df.shape}")
        print(f"   - test_processed_v2.csv: {test_df.shape}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ  æˆ¿åœ°ç”¢æ•¸æ“šåˆ†æå’Œé è™•ç† V2")
    print("=" * 50)
    
    analyzer = DataAnalysisV2()
    
    while True:
        print("\nè«‹é¸æ“‡æ“ä½œ:")
        print("1. è¼‰å…¥æ•¸æ“š")
        print("2. åˆ†ææ•¸æ“šé—œä¿‚")
        print("3. è‡ªå®šç¾©æ•¸æ“šè™•ç†")
        print("0. é€€å‡º")
        
        try:
            choice = input("\nè«‹è¼¸å…¥é¸æ“‡ (0-3): ").strip()
            
            if choice == '0':
                print("ğŸ‘‹ ç¨‹åºçµæŸ")
                break
            elif choice == '1':
                analyzer.load_data()
            elif choice == '2':
                if analyzer.train_df is None:
                    print("âŒ è«‹å…ˆè¼‰å…¥æ•¸æ“š!")
                    continue
                analyzer.analyze_data_relationship()
            elif choice == '3':
                if analyzer.train_df is None:
                    print("âŒ è«‹å…ˆè¼‰å…¥æ•¸æ“š!")
                    continue
                
                # å¦‚æœé‚„æ²’æœ‰åˆ†æçµæœï¼Œå…ˆé€²è¡Œåˆ†æ
                if not analyzer.analysis_results:
                    print("ğŸ“Š å…ˆé€²è¡Œæ•¸æ“šé—œä¿‚åˆ†æ...")
                    analyzer.analyze_data_relationship(save_results=False)
                
                processed_data = analyzer.process_data_custom()
                if processed_data:
                    save_choice = input("\næ˜¯å¦å„²å­˜è™•ç†å¾Œçš„æ•¸æ“š? (y/n): ").strip().lower()
                    if save_choice == 'y':
                        analyzer.save_processed_data(*processed_data)
            else:
                print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç¨‹åºå·²ä¸­æ­¢")
            break
        except Exception as e:
            print(f"âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()