#!/usr/bin/env python3

import torch

print("=== GPU å¯ç”¨æ€§æª¢æŸ¥ ===")
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"GPU æ•¸é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"  è¨˜æ†¶é«”: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
    
    # æ¸¬è©¦ GPU é‹ç®—
    try:
        x = torch.randn(1000, 1000).cuda()
        y = torch.randn(1000, 1000).cuda()
        z = torch.matmul(x, y)
        print("âœ… GPU é‹ç®—æ¸¬è©¦é€šéï¼")
        print(f"å»ºè­°ä½¿ç”¨: --device cuda")
    except Exception as e:
        print(f"âŒ GPU é‹ç®—æ¸¬è©¦å¤±æ•—: {e}")
        print(f"å»ºè­°ä½¿ç”¨: --device cpu")
else:
    print("âŒ CUDA ä¸å¯ç”¨")
    print(f"å»ºè­°ä½¿ç”¨: --device cpu")

print("\n=== æ¨è–¦è¨­å®š ===")
if torch.cuda.is_available():
    print("ğŸš€ ä½ çš„ç³»çµ±æ”¯æ´ GPU åŠ é€Ÿï¼")
    print("åŸ·è¡ŒæŒ‡ä»¤ï¼špython main_finetune.py --device cuda")
else:
    print("ğŸ’» ä½¿ç”¨ CPU è¨“ç·´ï¼ˆé€Ÿåº¦è¼ƒæ…¢ï¼‰")
    print("åŸ·è¡ŒæŒ‡ä»¤ï¼špython main_finetune.py --device cpu")