#!/usr/bin/env python
"""
è³‡æ–™å¢å¼·æ€§èƒ½æ¯”è¼ƒå·¥å…·
"""

import os
import time
from pathlib import Path
import platform

def benchmark_cpu_vs_gpu():
    """æ¯”è¼ƒ CPU å’Œ GPU å¢å¼·æ€§èƒ½"""
    
    print("âš¡ è³‡æ–™å¢å¼·æ€§èƒ½æ¯”è¼ƒ")
    print("=" * 50)
    
    # æª¢æŸ¥ GPU å¯ç”¨æ€§
    gpu_available = False
    try:
        import torch
        gpu_available = torch.cuda.is_available()
        if gpu_available:
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸ® GPU: {gpu_name} ({gpu_memory:.1f} GB)")
        else:
            print("âŒ GPU ä¸å¯ç”¨")
    except:
        print("âŒ PyTorch æœªå®‰è£")
    
    # CPU ä¿¡æ¯
    try:
        import multiprocessing
        cpu_count = multiprocessing.cpu_count()
        print(f"ğŸ’» CPU: {cpu_count} æ ¸å¿ƒ")
    except:
        cpu_count = "æœªçŸ¥"
    
    print("\nğŸ“Š æ€§èƒ½é ä¼° (1000å¼µåœ–ç‰‡):")
    print("-" * 50)
    
    methods = [
        {
            "name": "åŸå§‹ CPU (é€å¼µ)",
            "file": "data_augmentation.py", 
            "estimated_time": "15-20 åˆ†é˜",
            "memory": "2-4 GB",
            "pros": ["ä½è¨˜æ†¶é«”éœ€æ±‚", "ç©©å®šå¯é "],
            "cons": ["é€Ÿåº¦è¼ƒæ…¢", "CPU åˆ©ç”¨ç‡ä½"]
        },
        {
            "name": "GPU åŠ é€Ÿ (æ‰¹æ¬¡)",
            "file": "gpu_augmentation.py",
            "estimated_time": "3-5 åˆ†é˜" if gpu_available else "ä¸å¯ç”¨",
            "memory": "4-8 GB",
            "pros": ["é€Ÿåº¦å¿« 3-5å€", "æ‰¹æ¬¡è™•ç†", "ä¸¦è¡Œè¨ˆç®—"],
            "cons": ["éœ€è¦ GPU", "è¨˜æ†¶é«”éœ€æ±‚é«˜"]
        },
        {
            "name": "CPU å¤šæ ¸ (ä¸¦è¡Œ)",
            "file": "æº–å‚™é–‹ç™¼ä¸­...",
            "estimated_time": "8-12 åˆ†é˜",
            "memory": "3-6 GB", 
            "pros": ["ä¸­ç­‰é€Ÿåº¦", "å……åˆ†åˆ©ç”¨å¤šæ ¸"],
            "cons": ["è¤‡é›œåº¦é«˜", "è¨˜æ†¶é«”éœ€æ±‚ä¸­ç­‰"]
        }
    ]
    
    for i, method in enumerate(methods, 1):
        print(f"\n{i}. {method['name']}")
        print(f"   ğŸ“ è…³æœ¬: {method['file']}")
        print(f"   â±ï¸  é ä¼°æ™‚é–“: {method['estimated_time']}")
        print(f"   ğŸ’¾ è¨˜æ†¶é«”: {method['memory']}")
        print(f"   âœ… å„ªé»: {', '.join(method['pros'])}")
        print(f"   âŒ ç¼ºé»: {', '.join(method['cons'])}")

def recommend_method():
    """æ¨è–¦æœ€é©åˆçš„æ–¹æ³•"""
    print(f"\nğŸ¯ æ–¹æ³•æ¨è–¦:")
    print("-" * 30)
    
    # æª¢æŸ¥ GPU
    has_gpu = False
    gpu_memory = 0
    try:
        import torch
        has_gpu = torch.cuda.is_available()
        if has_gpu:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    except:
        pass
    
    if has_gpu and gpu_memory >= 4:
        print("ğŸ¥‡ æ¨è–¦: GPU åŠ é€Ÿæ–¹æ³•")
        print("   ç†ç”±: ä½ æœ‰è¶³å¤ çš„ GPU è¨˜æ†¶é«”")
        print("   è…³æœ¬: python quick_gpu_augment.py")
        print("   é æœŸæ•ˆæœ: é€Ÿåº¦æå‡ 3-5 å€")
    elif has_gpu and gpu_memory < 4:
        print("ğŸ¥ˆ æ¨è–¦: GPU åŠ é€Ÿæ–¹æ³• (å°æ‰¹æ¬¡)")
        print("   ç†ç”±: GPU å¯ç”¨ä½†è¨˜æ†¶é«”æœ‰é™")
        print("   è…³æœ¬: python quick_gpu_augment.py")
        print("   æ³¨æ„: æœƒè‡ªå‹•èª¿æ•´æ‰¹æ¬¡å¤§å°")
    else:
        print("ğŸ¥‰ æ¨è–¦: åŸå§‹ CPU æ–¹æ³•")
        print("   ç†ç”±: GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ç©©å®šçš„ CPU æ–¹æ³•")
        print("   è…³æœ¬: python quick_augment.py") 
        print("   ç‰¹é»: ç©©å®šå¯é ï¼Œè¨˜æ†¶é«”éœ€æ±‚ä½")

def optimization_tips():
    """å„ªåŒ–å»ºè­°"""
    print(f"\nğŸ’¡ å„ªåŒ–å»ºè­°:")
    print("-" * 30)
    
    tips = [
        "ğŸ”§ å¦‚æœ GPU è¨˜æ†¶é«”ä¸è¶³ï¼Œé™ä½æ‰¹æ¬¡å¤§å° (--batch_size 4)",
        "âš¡ é—œé–‰ä¸å¿…è¦çš„ç¨‹åºé‡‹æ”¾ VRAM",
        "ğŸ’¾ ç¢ºä¿æœ‰è¶³å¤ çš„ç¡¬ç¢Ÿç©ºé–“ (å¢å¼·å¾Œç´„ 4 å€å¤§å°)",
        "ğŸŒ¡ï¸  é•·æ™‚é–“é‹è¡Œæ³¨æ„ GPU æº«åº¦",
        "ğŸ”„ å¯ä»¥åˆ†æ‰¹è™•ç†é¡åˆ¥é¿å…è¨˜æ†¶é«”å•é¡Œ",
        "ğŸ“Š å…ˆç”¨å°é‡è³‡æ–™æ¸¬è©¦ç¢ºå®šæœ€ä½³åƒæ•¸"
    ]
    
    for tip in tips:
        print(f"   {tip}")

def speed_calculator():
    """é€Ÿåº¦è¨ˆç®—å™¨"""
    print(f"\nğŸ§® é€Ÿåº¦è¨ˆç®—å™¨:")
    print("-" * 30)
    
    try:
        # ç²å–è³‡æ–™é‡
        is_wsl = "microsoft" in platform.uname().release.lower()
        if is_wsl:
            input_dir = "/mnt/e/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset/preprocessed/train"
        else:
            input_dir = r"E:\NYCU\NYCU_IAII_ML2025\Ass2-Classification\Dataset\preprocessed\train"
        
        if os.path.exists(input_dir):
            class_dirs = [d for d in Path(input_dir).iterdir() if d.is_dir()]
            total_images = 0
            for class_dir in class_dirs:
                image_files = []
                for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
                    image_files.extend(list(class_dir.glob(ext)))
                total_images += len(image_files)
            
            print(f"ğŸ“Š ä½ çš„è³‡æ–™é‡: {total_images} å¼µåŸå§‹åœ–ç‰‡")
            print(f"ğŸ“ˆ å¢å¼·å¾Œæ•¸é‡: ~{total_images * 4} å¼µ")
            
            # æ™‚é–“é ä¼°
            cpu_time = total_images * 0.8  # æ¯å¼µç´„ 0.8 ç§’
            gpu_time = total_images * 0.2  # æ¯å¼µç´„ 0.2 ç§’ (GPUåŠ é€Ÿ)
            
            print(f"\nâ±ï¸  é ä¼°è™•ç†æ™‚é–“:")
            print(f"   CPU æ–¹æ³•: {cpu_time/60:.1f} åˆ†é˜")
            print(f"   GPU æ–¹æ³•: {gpu_time/60:.1f} åˆ†é˜ (ç¯€çœ {(cpu_time-gpu_time)/60:.1f} åˆ†é˜)")
            
        else:
            print("ğŸ“ æ‰¾ä¸åˆ°é è™•ç†è³‡æ–™ï¼Œç„¡æ³•è¨ˆç®—")
            
    except Exception as e:
        print(f"âŒ è¨ˆç®—å¤±æ•—: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    benchmark_cpu_vs_gpu()
    recommend_method()
    optimization_tips()
    speed_calculator()
    
    print(f"\nğŸš€ æº–å‚™é–‹å§‹å¢å¼·:")
    print("1. GPU åŠ é€Ÿ (æ¨è–¦): python quick_gpu_augment.py")
    print("2. CPU ç©©å®šç‰ˆ: python quick_augment.py")
    print("3. æ‰‹å‹•åƒæ•¸ç‰ˆ: python gpu_augmentation.py --help")

if __name__ == "__main__":
    main()