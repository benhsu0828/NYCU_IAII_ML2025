import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import os
import glob
import numpy as np
import platform
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import json
from vit_pytorch.learnable_memory_vit import ViT, Adapter

def get_best_data_path():
    """
    ä½¿ç”¨å›ºå®šçš„è³‡æ–™è·¯å¾‘é…ç½®ï¼š
    - è¨“ç·´è³‡æ–™ï¼šaugmented/train/
    - é©—è­‰è³‡æ–™ï¼špreprocessed/val/
    """
    # æª¢æ¸¬ç’°å¢ƒ
    is_wsl = "microsoft" in platform.uname().release.lower() or "WSL" in os.environ.get("WSL_DISTRO_NAME", "")
    
    if is_wsl:
        base_path = "/mnt/e/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset"
        augmented_train = f"{base_path}/augmented/train"
        preprocessed_val = f"{base_path}/preprocessed/val"
    else:
        base_path = "E:/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset"
        augmented_train = f"{base_path}/augmented/train"
        preprocessed_val = f"{base_path}/preprocessed/val"
    
    # æª¢æŸ¥å¿…è¦çš„è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨
    if os.path.exists(augmented_train) and os.path.exists(preprocessed_val):
        return {
            'train': augmented_train,
            'val': preprocessed_val,
            'use_existing_split': True
        }, "ğŸ¨ ä½¿ç”¨å¢å¼·è¨“ç·´è³‡æ–™ + é è™•ç†é©—è­‰è³‡æ–™"
    else:
        missing = []
        if not os.path.exists(augmented_train):
            missing.append(f"augmented/train: {augmented_train}")
        if not os.path.exists(preprocessed_val):
            missing.append(f"preprocessed/val: {preprocessed_val}")
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å¿…è¦çš„è³‡æ–™å¤¾: {', '.join(missing)}")

class CharacterDataset(Dataset):
    """50é¡è§’è‰²åˆ†é¡è³‡æ–™é›†"""
    def __init__(self, image_paths, labels, class_to_idx, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.class_to_idx = class_to_idx
        self.transform = transform
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
            
        label = self.labels[idx]
        return image, label

class MemoryViTCharacterClassifier:
    def __init__(self, num_classes=50, image_size=224, device='cuda'):
        self.device = device
        self.num_classes = num_classes
        self.image_size = image_size
        
        # è³‡æ–™å¢å¼·ç­–ç•¥
        self.train_transform = transforms.Compose([
            transforms.Resize((image_size + 32, image_size + 32)),
            transforms.RandomCrop(image_size),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        self.val_transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # å‰µå»ºåŸºç¤ ViT æ¨¡å‹
        self.base_vit = ViT(
            image_size=image_size,
            patch_size=16,
            num_classes=1000,  # é è¨“ç·´é¡åˆ¥æ•¸ï¼ˆå¾ŒçºŒæœƒè¢« Adapter è¦†è“‹ï¼‰
            dim=768,           # æ¨™æº– ViT-Base ç¶­åº¦
            depth=12,          # 12 å±¤ Transformer
            heads=12,          # 12 å€‹æ³¨æ„åŠ›é ­
            mlp_dim=3072,      # MLP ç¶­åº¦
            dropout=0.1,
            emb_dropout=0.1
        ).to(device)
        
        # å‰µå»ºè§’è‰²åˆ†é¡ Adapter
        self.character_adapter = None  # å°‡åœ¨æº–å‚™è³‡æ–™å¾Œå‰µå»º
        
    def prepare_data(self, data_paths, test_size=0.15, val_size=0.15):
        """æº–å‚™ 50 é¡è§’è‰²åˆ†é¡è³‡æ–™"""
        print("ğŸ“‚ æº–å‚™è§’è‰²åˆ†é¡è³‡æ–™...")
        
        train_path = data_paths['train']
        val_path = data_paths['val']
        use_existing_split = data_paths['use_existing_split']
        
        if use_existing_split and val_path:
            print("âœ… ä½¿ç”¨å·²æœ‰çš„ train/val åˆ†å‰²")
            return self._prepare_data_with_split(train_path, val_path, test_size)
        else:
            print("ğŸ”„ å¾ train è³‡æ–™å¤¾é‡æ–°åˆ†å‰²")
            return self._prepare_data_from_single_folder(train_path, test_size, val_size)
    
    def _prepare_data_with_split(self, train_path, val_path, test_size=0.15):
        """ä½¿ç”¨å·²æœ‰çš„ train/val åˆ†å‰²"""
        # æ”¶é›†è¨“ç·´è³‡æ–™
        print(f"ğŸ“ è¼‰å…¥è¨“ç·´è³‡æ–™: {train_path}")
        train_image_paths = self._collect_images(train_path)
        
        # æ”¶é›†é©—è­‰è³‡æ–™
        print(f"ğŸ“ è¼‰å…¥é©—è­‰è³‡æ–™: {val_path}")
        val_image_paths = self._collect_images(val_path)
        
        # å»ºç«‹çµ±ä¸€çš„é¡åˆ¥æ˜ å°„
        all_image_paths = train_image_paths + val_image_paths
        all_classes = sorted(list(set([
            os.path.basename(os.path.dirname(path)) 
            for path in all_image_paths
        ])))
        
        if len(all_classes) != self.num_classes:
            print(f"âš ï¸ è­¦å‘Šï¼šç™¼ç¾ {len(all_classes)} å€‹é¡åˆ¥ï¼Œä½†æœŸæœ› {self.num_classes} å€‹")
            self.num_classes = len(all_classes)
        
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(all_classes)}
        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()}
        
        # å‰µå»ºæ¨™ç±¤
        train_labels = [self.class_to_idx[os.path.basename(os.path.dirname(path))] 
                       for path in train_image_paths]
        val_labels = [self.class_to_idx[os.path.basename(os.path.dirname(path))] 
                     for path in val_image_paths]
        
        # å¾é©—è­‰é›†ä¸­åˆ†å‡ºæ¸¬è©¦é›†
        if test_size > 0:
            val_paths_split, test_paths, val_labels_split, test_labels = train_test_split(
                val_image_paths, val_labels,
                test_size=test_size,
                stratify=val_labels,
                random_state=42
            )
        else:
            val_paths_split = val_image_paths
            val_labels_split = val_labels
            test_paths = []
            test_labels = []
        
        print(f"âœ… ç™¼ç¾ {len(all_classes)} å€‹è§’è‰²é¡åˆ¥")
        print(f"âœ… è¨“ç·´è³‡æ–™: {len(train_image_paths)} å¼µ")
        print(f"âœ… é©—è­‰è³‡æ–™: {len(val_paths_split)} å¼µ")
        print(f"âœ… æ¸¬è©¦è³‡æ–™: {len(test_paths)} å¼µ")
        
        # æª¢æŸ¥æ¯é¡åˆ¥çš„åœ–ç‰‡æ•¸é‡
        self._print_class_distribution(train_labels + val_labels_split, "Train + Val")
        
        # å‰µå»ºè³‡æ–™é›†
        self.train_dataset = CharacterDataset(
            train_image_paths, train_labels, self.class_to_idx, self.train_transform
        )
        self.val_dataset = CharacterDataset(
            val_paths_split, val_labels_split, self.class_to_idx, self.val_transform
        )
        
        if test_paths:
            self.test_dataset = CharacterDataset(
                test_paths, test_labels, self.class_to_idx, self.val_transform
            )
        else:
            # å¦‚æœæ²’æœ‰æ¸¬è©¦é›†ï¼Œä½¿ç”¨é©—è­‰é›†çš„ä¸€éƒ¨åˆ†ä½œç‚ºæ¸¬è©¦é›†
            self.test_dataset = self.val_dataset
        
        self._create_adapter_and_save_mapping()
        
        return self.train_dataset, self.val_dataset, self.test_dataset
    
    def _prepare_data_from_single_folder(self, data_path, test_size, val_size):
        """å¾å–®ä¸€è³‡æ–™å¤¾é‡æ–°åˆ†å‰²è³‡æ–™"""
        # æ”¶é›†æ‰€æœ‰åœ–ç‰‡
        all_image_paths = self._collect_images(data_path)
        
        # å»ºç«‹é¡åˆ¥æ˜ å°„
        all_classes = sorted(list(set([
            os.path.basename(os.path.dirname(path)) 
            for path in all_image_paths
        ])))
        
        if len(all_classes) != self.num_classes:
            print(f"âš ï¸ è­¦å‘Šï¼šç™¼ç¾ {len(all_classes)} å€‹é¡åˆ¥ï¼Œä½†æœŸæœ› {self.num_classes} å€‹")
            self.num_classes = len(all_classes)
        
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(all_classes)}
        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()}
        
        # å‰µå»ºæ¨™ç±¤
        labels = [self.class_to_idx[os.path.basename(os.path.dirname(path))] 
                 for path in all_image_paths]
        
        print(f"âœ… ç™¼ç¾ {len(all_classes)} å€‹è§’è‰²é¡åˆ¥")
        print(f"âœ… ç¸½å…± {len(all_image_paths)} å¼µåœ–ç‰‡")
        
        # æª¢æŸ¥æ¯é¡åˆ¥çš„åœ–ç‰‡æ•¸é‡
        self._print_class_distribution(labels, "All Data")
        
        # åˆ†å‰²è³‡æ–™é›†
        train_paths, temp_paths, train_labels, temp_labels = train_test_split(
            all_image_paths, labels, 
            test_size=(test_size + val_size), 
            stratify=labels, 
            random_state=42
        )
        
        val_paths, test_paths, val_labels, test_labels = train_test_split(
            temp_paths, temp_labels,
            test_size=(test_size / (test_size + val_size)),
            stratify=temp_labels,
            random_state=42
        )
        
        # å‰µå»ºè³‡æ–™é›†
        self.train_dataset = CharacterDataset(
            train_paths, train_labels, self.class_to_idx, self.train_transform
        )
        self.val_dataset = CharacterDataset(
            val_paths, val_labels, self.class_to_idx, self.val_transform
        )
        self.test_dataset = CharacterDataset(
            test_paths, test_labels, self.class_to_idx, self.val_transform
        )
        
        print(f"ğŸ“Š è³‡æ–™åˆ†å‰²:")
        print(f"  è¨“ç·´é›†: {len(self.train_dataset)} å¼µ")
        print(f"  é©—è­‰é›†: {len(self.val_dataset)} å¼µ")
        print(f"  æ¸¬è©¦é›†: {len(self.test_dataset)} å¼µ")
        
        self._create_adapter_and_save_mapping()
        
        return self.train_dataset, self.val_dataset, self.test_dataset
    
    def _collect_images(self, data_path):
        """æ”¶é›†æŒ‡å®šè·¯å¾‘ä¸‹çš„æ‰€æœ‰åœ–ç‰‡"""
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif']
        all_image_paths = []
        
        for ext in image_extensions:
            all_image_paths.extend(glob.glob(os.path.join(data_path, '**', ext), recursive=True))
        
        return all_image_paths
    
    def _print_class_distribution(self, labels, data_name):
        """æ‰“å°é¡åˆ¥åˆ†å¸ƒ"""
        class_counts = {}
        for label in labels:
            class_name = self.idx_to_class[label]
            class_counts[class_name] = class_counts.get(class_name, 0) + 1
        
        print(f"ğŸ“Š {data_name} - æ¯é¡åˆ¥åœ–ç‰‡æ•¸é‡:")
        for cls_name, count in sorted(class_counts.items()):
            print(f"  {cls_name}: {count} å¼µ")
    
    def _create_adapter_and_save_mapping(self):
        """å‰µå»º Adapter ä¸¦ä¿å­˜é¡åˆ¥æ˜ å°„"""
        # å‰µå»ºè§’è‰²åˆ†é¡ Adapter
        self.character_adapter = Adapter(
            vit=self.base_vit,
            num_classes=self.num_classes,
            num_memories_per_layer=20  # é‡å° 50 é¡å¢åŠ è¨˜æ†¶æ•¸é‡
        ).to(self.device)
        
        print(f"âœ… è§’è‰²åˆ†é¡ Adapter å‰µå»ºå®Œæˆ ({self.num_classes} é¡)")
        
        # ä¿å­˜é¡åˆ¥æ˜ å°„
        class_mapping = {
            'class_to_idx': self.class_to_idx,
            'idx_to_class': self.idx_to_class,
            'num_classes': self.num_classes
        }
        
        with open('character_class_mapping.json', 'w', encoding='utf-8') as f:
            json.dump(class_mapping, f, ensure_ascii=False, indent=2)
    
    def find_optimal_batch_size(self, max_batch_size=128, start_batch_size=16):
        """
        æ™ºæ…§æ‰¾åˆ°æœ€ä½³ batch size
        æ¸¬è©¦ä¸åŒçš„ batch size ç›´åˆ°è¨˜æ†¶é«”ç”¨ç›¡ï¼Œæ‰¾åˆ°æœ€ä½³é…ç½®
        """
        print("ğŸ” æ­£åœ¨å°‹æ‰¾æœ€ä½³ batch size...")
        print(f"   èµ·å§‹å¤§å°: {start_batch_size}, æœ€å¤§æ¸¬è©¦: {max_batch_size}")
        
        if not torch.cuda.is_available():
            print("   âš ï¸ æœªæª¢æ¸¬åˆ° GPUï¼Œå»ºè­°ä½¿ç”¨ batch_size=4")
            return 4
        
        # é¡¯ç¤º GPU è³‡è¨Š
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   ğŸ–¥ï¸ GPU: {gpu_name}")
        print(f"   ğŸ“Š ç¸½è¨˜æ†¶é«”: {gpu_memory:.1f} GB")
        
        optimal_batch_size = start_batch_size
        best_throughput = 0
        
        # ç¢ºä¿æ¨¡å‹å·²ç¶“å‰µå»º
        if self.character_adapter is None:
            print("   âŒ è«‹å…ˆæº–å‚™è³‡æ–™ä»¥å‰µå»ºæ¨¡å‹")
            return start_batch_size
        
        # æ¸¬è©¦ä¸åŒçš„ batch size
        test_sizes = [16, 24, 32, 48, 64, 80, 96, 128, 160, 192, 224, 256]
        test_sizes = [size for size in test_sizes if size <= max_batch_size]
        
        for batch_size in test_sizes:
            try:
                print(f"   ğŸ“ æ¸¬è©¦ batch size: {batch_size}")
                
                # æ¸…ç©º GPU å¿«å–
                torch.cuda.empty_cache()
                
                # å‰µå»ºæ¸¬è©¦æ‰¹æ¬¡
                test_batch = torch.randn(
                    batch_size, 3, self.image_size, self.image_size, 
                    device=self.device, dtype=torch.float32
                )
                
                # æ¸¬è©¦å‰å‘å‚³æ’­
                self.character_adapter.eval()
                with torch.no_grad():
                    # é ç†±
                    for _ in range(3):
                        _ = self.character_adapter(test_batch[:min(4, batch_size)])
                    
                    # æ­£å¼æ¸¬è©¦
                    torch.cuda.synchronize()
                    start_time = torch.cuda.Event(enable_timing=True)
                    end_time = torch.cuda.Event(enable_timing=True)
                    
                    start_time.record()
                    outputs = self.character_adapter(test_batch)
                    end_time.record()
                    
                    torch.cuda.synchronize()
                    elapsed_time = start_time.elapsed_time(end_time)  # æ¯«ç§’
                
                # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨
                memory_used = torch.cuda.max_memory_allocated() / 1024**3  # GB
                memory_percent = (memory_used / gpu_memory) * 100
                
                # è¨ˆç®—æ•ˆèƒ½æŒ‡æ¨™
                throughput = (batch_size / elapsed_time) * 1000  # images/second
                time_per_image = elapsed_time / batch_size  # ms per image
                
                print(f"      âœ… æˆåŠŸ - è¨˜æ†¶é«”: {memory_percent:.1f}% ({memory_used:.1f}GB)")
                print(f"      â±ï¸ è™•ç†æ™‚é–“: {elapsed_time:.1f}ms")
                print(f"      ğŸš€ ååé‡: {throughput:.1f} images/sec")
                print(f"      ğŸ“Š æ¯å¼µåœ–ç‰‡: {time_per_image:.2f}ms")
                
                # è¨ˆç®—æ•ˆç‡åˆ†æ•¸ (ç¶œåˆè€ƒæ…®ååé‡å’Œè¨˜æ†¶é«”ä½¿ç”¨)
                memory_efficiency = min(memory_percent / 80.0, 1.0)  # 80%ä»¥ä¸‹æ•ˆç‡è¼ƒé«˜
                throughput_efficiency = throughput / (batch_size * 50)  # æ­¸ä¸€åŒ–ååé‡
                
                # ç¶œåˆæ•ˆç‡åˆ†æ•¸ (è¨˜æ†¶é«”ä½¿ç”¨ç‡è¶Šä½è¶Šå¥½ï¼Œååé‡è¶Šé«˜è¶Šå¥½)
                efficiency_score = throughput_efficiency * (2.0 - memory_efficiency)
                
                print(f"      ğŸ“ˆ æ•ˆç‡åˆ†æ•¸: {efficiency_score:.3f}")
                
                # å¦‚æœè¨˜æ†¶é«”ä½¿ç”¨è¶…é 85%ï¼Œåœæ­¢å¢åŠ ï¼ˆä¿ç•™ä¸€äº›å®‰å…¨é‚Šç•Œï¼‰
                if memory_percent > 85:
                    print(f"      âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨éé«˜ ({memory_percent:.1f}%)ï¼Œåœæ­¢å¢åŠ ")
                    break
                
                # æ›´æ–°æœ€ä½³é…ç½® - æ”¹ç”¨æ•ˆç‡åˆ†æ•¸è€Œéå–®ç´”ååé‡
                if efficiency_score > best_throughput:
                    best_throughput = efficiency_score
                    optimal_batch_size = batch_size
                elif batch_size > 32 and time_per_image > 15.0:  # å¦‚æœæ¯å¼µåœ–ç‰‡è¶…é15msä¸”batch>32ï¼Œåœæ­¢
                    print(f"      âš ï¸ å–®å¼µåœ–ç‰‡è™•ç†æ™‚é–“éé•· ({time_per_image:.2f}ms)ï¼Œæ•ˆç‡é–‹å§‹ä¸‹é™")
                    break
                
                # æ¸…ç†æ¸¬è©¦è³‡æ–™
                del test_batch, outputs
                torch.cuda.empty_cache()
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print(f"      âŒ è¨˜æ†¶é«”ä¸è¶³ (OOM)")
                    break
                else:
                    print(f"      âŒ å…¶ä»–éŒ¯èª¤: {e}")
                    break
        
        # æ¸…ç©ºå¿«å–
        torch.cuda.empty_cache()
        
        print(f"\nğŸ¯ æª¢æ¸¬çµæœ:")
        print(f"   æœ€ä½³ batch size: {optimal_batch_size}")
        print(f"   æœ€ä½³æ•ˆç‡åˆ†æ•¸: {best_throughput:.3f}")
        
        # åˆ†æç‚ºä»€éº¼é¸æ“‡é€™å€‹ batch size
        print(f"\nğŸ“Š åˆ†æ:")
        if optimal_batch_size == 16:
            print("   ğŸ” é¸æ“‡ batch_size=16 çš„å¯èƒ½åŸå› :")
            print("     â€¢ ViT æ³¨æ„åŠ›æ©Ÿåˆ¶è¤‡é›œåº¦é«˜ï¼Œè¼ƒå° batch æ›´é«˜æ•ˆ")
            print("     â€¢ GPU è¨˜æ†¶é«”é »å¯¬é™åˆ¶ï¼Œå¤§ batch æ²’æœ‰å¸¶ä¾†é€Ÿåº¦æå‡")
            print("     â€¢ æ¨¡å‹è¨ˆç®—å¯†é›†ï¼Œå—è¨ˆç®—è¤‡é›œåº¦å½±éŸ¿å¤§æ–¼è¨˜æ†¶é«”é‡")
        elif optimal_batch_size <= 32:
            print("   âš¡ é©ä¸­çš„ batch sizeï¼Œå¹³è¡¡äº†è¨˜æ†¶é«”ä½¿ç”¨å’Œè¨ˆç®—æ•ˆç‡")
        else:
            print("   ğŸ’ª è¼ƒå¤§çš„ batch sizeï¼Œæ‚¨çš„ GPU æ€§èƒ½å¼·å‹")
        
        # æä¾›ä¸åŒå ´æ™¯çš„å»ºè­°
        print(f"\nğŸ’¡ ä¸åŒå ´æ™¯å»ºè­°:")
        print(f"     ğŸš€ æœ€å¤§ååé‡: batch_size={optimal_batch_size}")
        print(f"     ğŸ¯ ç©©å®šè¨“ç·´: batch_size={max(16, optimal_batch_size // 2)}")
        print(f"     ğŸ›¡ï¸ ä¿å®ˆå®‰å…¨: batch_size=16")
        
        # é¡å¤–å»ºè­°
        if optimal_batch_size >= 64:
            print("\n   ğŸ’¡ æ‚¨çš„ GPU è¨˜æ†¶é«”å……è¶³ï¼Œå¯ä»¥è€ƒæ…®:")
            print("      â€¢ ä½¿ç”¨æ›´å¤§çš„åœ–åƒå°ºå¯¸ (256x256)")
            print("      â€¢ å˜—è©¦æ›´è¤‡é›œçš„è³‡æ–™å¢å¼·")
        elif optimal_batch_size >= 32:
            print("\n   ğŸ‘ GPU è¨˜æ†¶é«”é©ä¸­ï¼Œç•¶å‰è¨­å®šå¾ˆå¥½")
        else:
            print("\n   âš ï¸ ç‚ºäº†æ›´å¥½çš„æ•ˆèƒ½ï¼Œå¯ä»¥è€ƒæ…®:")
            print("      â€¢ é™ä½åœ–åƒå°ºå¯¸ (192x192)")
            print("      â€¢ ä½¿ç”¨æ¢¯åº¦ç´¯ç©æŠ€è¡“")
            print("      â€¢ æ··åˆç²¾åº¦è¨“ç·´ (FP16)")
        
        return optimal_batch_size
    
    def train(self, batch_size=None, epochs=50, lr=1e-4, warmup_epochs=10, auto_batch_size=True, use_mixed_precision=True):
        """è¨“ç·´ MemoryViT è§’è‰²åˆ†é¡æ¨¡å‹"""
        print("ğŸš€ é–‹å§‹è¨“ç·´ MemoryViT è§’è‰²åˆ†é¡æ¨¡å‹...")
        
        # æª¢æŸ¥æ··åˆç²¾åº¦æ”¯æ´
        if use_mixed_precision and torch.cuda.is_available():
            print("âš¡ å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´ (FP16) - é æœŸæå‡ 30-50% é€Ÿåº¦")
            scaler = torch.cuda.amp.GradScaler()
        else:
            print("ğŸ“ ä½¿ç”¨æ¨™æº–ç²¾åº¦è¨“ç·´ (FP32)")
            scaler = None
            use_mixed_precision = False
        
        # è‡ªå‹•æª¢æ¸¬æœ€ä½³ batch size
        if batch_size is None and auto_batch_size:
            print("\nğŸ” å•Ÿç”¨è‡ªå‹• batch size æª¢æ¸¬...")
            batch_size = self.find_optimal_batch_size()
            print(f"âœ… è‡ªå‹•é¸æ“‡ batch size: {batch_size}")
        elif batch_size is None:
            batch_size = 16  # é è¨­å€¼
            print(f"ğŸ“ ä½¿ç”¨é è¨­ batch size: {batch_size}")
        else:
            print(f"ğŸ“ ä½¿ç”¨æŒ‡å®š batch size: {batch_size}")
        
        # è³‡æ–™è¼‰å…¥å™¨
        train_loader = DataLoader(
            self.train_dataset, 
            batch_size=batch_size, 
            shuffle=True, 
            num_workers=6,
            pin_memory=True
        )
        val_loader = DataLoader(
            self.val_dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=6,
            pin_memory=True
        )
        
        # æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # æ¨™ç±¤å¹³æ»‘
        
        # åªè¨“ç·´ Adapter åƒæ•¸ï¼Œå‡çµåŸºç¤ ViT
        adapter_params = [p for p in self.character_adapter.parameters() if p.requires_grad]
        optimizer = optim.AdamW(adapter_params, lr=lr, weight_decay=0.05)
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        total_steps = len(train_loader) * epochs
        warmup_steps = len(train_loader) * warmup_epochs
        
        def lr_lambda(step):
            if step < warmup_steps:
                return step / warmup_steps
            else:
                return 0.5 * (1 + np.cos(np.pi * (step - warmup_steps) / (total_steps - warmup_steps)))
        
        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        
        # è¨“ç·´è¨˜éŒ„
        history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': [],
            'learning_rate': []
        }
        
        best_val_acc = 0.0
        patience = 15
        patience_counter = 0
        
        print(f"ğŸ“Š è¨“ç·´é…ç½®:")
        print(f"  ç¸½åƒæ•¸: {sum(p.numel() for p in self.character_adapter.parameters()):,}")
        print(f"  å¯è¨“ç·´åƒæ•¸: {sum(p.numel() for p in adapter_params):,}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  å­¸ç¿’ç‡: {lr}")
        print(f"  ç¸½è¼ªæ•¸: {epochs}")
        
        for epoch in range(epochs):
            # è¨“ç·´éšæ®µ
            self.character_adapter.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')
            for batch_idx, (images, labels) in enumerate(pbar):
                images, labels = images.to(self.device), labels.to(self.device)
                
                optimizer.zero_grad()
                
                # æ··åˆç²¾åº¦å‰å‘å‚³æ’­
                if use_mixed_precision:
                    with torch.cuda.amp.autocast():
                        outputs = self.character_adapter(images)
                        loss = criterion(outputs, labels)
                    
                    # æ··åˆç²¾åº¦åå‘å‚³æ’­
                    scaler.scale(loss).backward()
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(adapter_params, max_norm=1.0)
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    # æ¨™æº–ç²¾åº¦è¨“ç·´
                    outputs = self.character_adapter(images)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(adapter_params, max_norm=1.0)
                    optimizer.step()
                
                scheduler.step()
                
                # çµ±è¨ˆ
                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                # æ›´æ–°é€²åº¦æ¢
                current_lr = optimizer.param_groups[0]['lr']
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100.*train_correct/train_total:.2f}%',
                    'LR': f'{current_lr:.2e}'
                })
            
            # é©—è­‰éšæ®µ
            self.character_adapter.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for images, labels in val_loader:
                    images, labels = images.to(self.device), labels.to(self.device)
                    
                    # æ··åˆç²¾åº¦é©—è­‰
                    if use_mixed_precision:
                        with torch.cuda.amp.autocast():
                            outputs = self.character_adapter(images)
                            loss = criterion(outputs, labels)
                    else:
                        outputs = self.character_adapter(images)
                        loss = criterion(outputs, labels)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
            
            # è¨ˆç®—å¹³å‡å€¼
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            train_acc = 100. * train_correct / train_total
            val_acc = 100. * val_correct / val_total
            current_lr = optimizer.param_groups[0]['lr']
            
            # è¨˜éŒ„æ­·å²
            history['train_loss'].append(avg_train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(avg_val_loss)
            history['val_acc'].append(val_acc)
            history['learning_rate'].append(current_lr)
            
            # è¼¸å‡ºçµæœ
            print(f'Epoch {epoch+1}/{epochs}:')
            print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')
            print(f'  Learning Rate: {current_lr:.2e}')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                
                # å¢å¼·çš„æ¨¡å‹å­˜æª”
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': self.character_adapter.state_dict(),
                    'base_vit_state_dict': self.base_vit.state_dict(),  # ä¹Ÿä¿å­˜åŸºç¤ ViT
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'scaler_state_dict': scaler.state_dict() if scaler else None,
                    'val_acc': val_acc,
                    'train_acc': train_acc,
                    'val_loss': avg_val_loss,
                    'train_loss': avg_train_loss,
                    'best_val_acc': best_val_acc,
                    'learning_rate': current_lr,
                    'class_mapping': {
                        'class_to_idx': self.class_to_idx,
                        'idx_to_class': self.idx_to_class,
                        'num_classes': self.num_classes
                    },
                    'training_config': {
                        'batch_size': batch_size,
                        'epochs': epochs,
                        'lr': lr,
                        'warmup_epochs': warmup_epochs,
                        'use_mixed_precision': use_mixed_precision,
                        'image_size': self.image_size
                    },
                    'history': history.copy()
                }
                
                torch.save(checkpoint, 'best_memory_vit_character_classifier.pth')
                print(f'  ğŸ¯ æ–°çš„æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}% (å·²ä¿å­˜æ¨¡å‹)')
                
                # åŒæ™‚ä¿å­˜ä¸€å€‹è¼•é‡ç‰ˆæœ¬ (åªæœ‰æ¨¡å‹æ¬Šé‡)
                torch.save({
                    'model_state_dict': self.character_adapter.state_dict(),
                    'class_mapping': {
                        'class_to_idx': self.class_to_idx,
                        'idx_to_class': self.idx_to_class,
                        'num_classes': self.num_classes
                    },
                    'val_acc': val_acc,
                    'training_config': {
                        'image_size': self.image_size,
                        'num_classes': self.num_classes
                    }
                }, f'memoryvit_model_acc{val_acc:.1f}.pth')
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f'  â° Early stopping triggered after {patience} epochs without improvement')
                    break
            
            print('-' * 60)
        
        print(f"âœ… è¨“ç·´å®Œæˆï¼æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
        return history
    
    def evaluate(self, batch_size=32):
        """è©•ä¼°æ¨¡å‹"""
        print("ğŸ“Š è©•ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # è¼‰å…¥æœ€ä½³æ¨¡å‹
        checkpoint = torch.load('best_memory_vit_character_classifier.pth')
        self.character_adapter.load_state_dict(checkpoint['model_state_dict'])
        
        test_loader = DataLoader(
            self.test_dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=4
        )
        
        self.character_adapter.eval()
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for images, labels in tqdm(test_loader, desc='è©•ä¼°ä¸­'):
                images = images.to(self.device)
                outputs = self.character_adapter(images)
                _, predicted = torch.max(outputs, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.numpy())
        
        # è¨ˆç®—æº–ç¢ºç‡
        accuracy = accuracy_score(all_labels, all_predictions)
        print(f"ğŸ¯ æ¸¬è©¦é›†æº–ç¢ºç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # åˆ†é¡å ±å‘Š
        class_names = [self.idx_to_class[i] for i in range(self.num_classes)]
        report = classification_report(
            all_labels, all_predictions, 
            target_names=class_names, 
            output_dict=True
        )
        
        print("\nğŸ“ˆ è©³ç´°åˆ†é¡å ±å‘Š:")
        print(classification_report(all_labels, all_predictions, target_names=class_names))
        
        # ä¿å­˜çµæœ
        with open('evaluation_results.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return all_predictions, all_labels, accuracy
    
    def plot_training_history(self, history):
        """ç¹ªè£½è¨“ç·´æ­·å²"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æå¤±
        axes[0, 0].plot(history['train_loss'], label='è¨“ç·´æå¤±', color='blue')
        axes[0, 0].plot(history['val_loss'], label='é©—è­‰æå¤±', color='red')
        axes[0, 0].set_title('æ¨¡å‹æå¤±')
        axes[0, 0].set_ylabel('æå¤±')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # æº–ç¢ºç‡
        axes[0, 1].plot(history['train_acc'], label='è¨“ç·´æº–ç¢ºç‡', color='blue')
        axes[0, 1].plot(history['val_acc'], label='é©—è­‰æº–ç¢ºç‡', color='red')
        axes[0, 1].set_title('æ¨¡å‹æº–ç¢ºç‡')
        axes[0, 1].set_ylabel('æº–ç¢ºç‡ (%)')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # å­¸ç¿’ç‡
        axes[1, 0].plot(history['learning_rate'], color='green')
        axes[1, 0].set_title('å­¸ç¿’ç‡è®ŠåŒ–')
        axes[1, 0].set_ylabel('å­¸ç¿’ç‡')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True)
        
        # é©—è­‰æº–ç¢ºç‡æ”¾å¤§
        axes[1, 1].plot(history['val_acc'], color='red', linewidth=2)
        axes[1, 1].set_title('é©—è­‰æº–ç¢ºç‡è©³ç´°')
        axes[1, 1].set_ylabel('é©—è­‰æº–ç¢ºç‡ (%)')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def predict_single_image(self, image_path, top_k=5):
        """é æ¸¬å–®å¼µåœ–ç‰‡"""
        self.character_adapter.eval()
        
        # è¼‰å…¥ä¸¦é è™•ç†åœ–ç‰‡
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.val_transform(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            outputs = self.character_adapter(image_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            top_probs, top_indices = torch.topk(probabilities, top_k)
        
        # è½‰æ›çµæœ
        results = []
        for i in range(top_k):
            class_idx = top_indices[0][i].item()
            class_name = self.idx_to_class[class_idx]
            prob = top_probs[0][i].item()
            results.append((class_name, prob))
        
        return results

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ­ MemoryViT 50é¡è§’è‰²åˆ†é¡å™¨")
    print("=" * 50)
    
    # è¨­å®šè£ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è£ç½®: {device}")
    
    # æ™ºæ…§é¸æ“‡è³‡æ–™è·¯å¾‘
    data_paths, data_type = get_best_data_path()
    

    if data_paths is None:
        print("âŒ æ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™ï¼")
        print("è«‹ç¢ºèªä»¥ä¸‹è·¯å¾‘å­˜åœ¨:")
        print("  - augmented/train (å¢å¼·è¨“ç·´è³‡æ–™)")
        print("  - preprocessed/val (é è™•ç†é©—è­‰è³‡æ–™)")
        return
    
    print(f"ğŸ“‚ ä½¿ç”¨è³‡æ–™: {data_type}")
    print(f"ğŸ“ è¨“ç·´è·¯å¾‘: {data_paths['train']}")
    print(f"ğŸ“ é©—è­‰è·¯å¾‘: {data_paths['val']}")
    print("âœ… ä½¿ç”¨æ‚¨å·²åˆ†å‰²å¥½çš„ train/val è³‡æ–™")
    
    # çµ±è¨ˆè³‡æ–™é‡
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif']
    total_images = 0
    
    # çµ±è¨ˆ train è³‡æ–™
    for ext in image_extensions:
        total_images += len(glob.glob(os.path.join(data_paths['train'], '**', ext), recursive=True))
    
    # å¦‚æœæœ‰ val è³‡æ–™ï¼Œä¹Ÿçµ±è¨ˆé€²å»
    if data_paths['val'] and os.path.exists(data_paths['val']):
        for ext in image_extensions:
            total_images += len(glob.glob(os.path.join(data_paths['val'], '**', ext), recursive=True))
    
    print(f"ğŸ“Š ç¸½åœ–ç‰‡æ•¸: {total_images} å¼µ")
    
    if total_images == 0:
        print("âŒ æ‰¾ä¸åˆ°ä»»ä½•åœ–ç‰‡æª”æ¡ˆï¼")
        return
    
    # åˆå§‹åŒ–åˆ†é¡å™¨
    classifier = MemoryViTCharacterClassifier(
        num_classes=50,  # ä½ çš„ 50 å€‹è§’è‰²é¡åˆ¥
        device=device
    )
    
    try:
        # æº–å‚™è³‡æ–™
        train_dataset, val_dataset, test_dataset = classifier.prepare_data(data_paths)
        
        # è©¢å•æ˜¯å¦è¦è‡ªå‹•æª¢æ¸¬æœ€ä½³ batch size
        print("\nâš™ï¸ Batch Size è¨­å®š:")
        print("1. è‡ªå‹•æª¢æ¸¬æœ€ä½³ batch size (æ¨è–¦)")
        print("2. æ‰‹å‹•æŒ‡å®š batch size")
        
        choice = input("è«‹é¸æ“‡ (1/2ï¼Œé è¨­1): ").strip()
        
        if choice == "2":
            batch_size = int(input("è«‹è¼¸å…¥ batch size (å»ºè­° 16-64): ") or "32")
            auto_batch_size = False
            print(f"âœ… ä½¿ç”¨æ‰‹å‹•æŒ‡å®š batch size: {batch_size}")
        else:
            batch_size = None  # å°‡ç”±è‡ªå‹•æª¢æ¸¬æ±ºå®š
            auto_batch_size = True
            print("âœ… å°‡è‡ªå‹•æª¢æ¸¬æœ€ä½³ batch size")
        
        # å…¶ä»–è¨“ç·´åƒæ•¸
        epochs = int(input("è¨“ç·´è¼ªæ•¸ (é è¨­ 50): ") or "50")
        lr = float(input("å­¸ç¿’ç‡ (é è¨­ 1e-4): ") or "1e-4")
        warmup_epochs = int(input("ç†±èº«è¼ªæ•¸ (é è¨­ 10): ") or "10")
        
        # æ··åˆç²¾åº¦é¸é …
        if torch.cuda.is_available():
            use_mixed_precision = input("ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´ (FP16) åŠ é€Ÿ? (y/nï¼Œé è¨­ y): ").strip().lower()
            use_mixed_precision = use_mixed_precision not in ['n', 'no', 'false']
            if use_mixed_precision:
                print("âœ… å°‡ä½¿ç”¨æ··åˆç²¾åº¦ (FP16) - é æœŸæå‡ 30-50% é€Ÿåº¦")
            else:
                print("ğŸ“ å°‡ä½¿ç”¨æ¨™æº–ç²¾åº¦ (FP32)")
        else:
            use_mixed_precision = False
            print("âš ï¸ CPU æ¨¡å¼ï¼Œç„¡æ³•ä½¿ç”¨æ··åˆç²¾åº¦")
        
        # è¨“ç·´æ¨¡å‹
        print("\nğŸš€ é–‹å§‹è¨“ç·´...")
        history = classifier.train(
            batch_size=batch_size,
            epochs=epochs,
            lr=lr,
            warmup_epochs=warmup_epochs,
            auto_batch_size=auto_batch_size,
            use_mixed_precision=use_mixed_precision
        )
        
        # ç¹ªè£½è¨“ç·´æ­·å²
        classifier.plot_training_history(history)
        
        # è©•ä¼°æ¨¡å‹
        print("\nğŸ“Š è©•ä¼°æ¨¡å‹...")
        predictions, true_labels, accuracy = classifier.evaluate()
        
        print(f"\nğŸ¯ æœ€çµ‚çµæœ:")
        print(f"  æ¸¬è©¦é›†æº–ç¢ºç‡: {accuracy*100:.2f}%")
        print(f"  æ¨¡å‹å·²ä¿å­˜è‡³: best_memory_vit_character_classifier.pth")
        print(f"  é¡åˆ¥æ˜ å°„å·²ä¿å­˜è‡³: character_class_mapping.json")
        
    except Exception as e:
        print(f"âŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()