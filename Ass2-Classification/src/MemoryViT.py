import torch
import torch
from vit_pytorch.learnable_memory_vit import ViT, Adapter

def test_memory_vit_concept():
    """
    MemoryViT æ¦‚å¿µæ¸¬è©¦
    
    é€™å€‹ç¯„ä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¸€å€‹é è¨“ç·´çš„ ViT æ¨¡å‹
    é…åˆå¤šå€‹ Adapter ä¾†è™•ç†ä¸åŒçš„åˆ†é¡ä»»å‹™
    """
    print("ğŸ§  MemoryViT æ¦‚å¿µæ¼”ç¤º")
    print("=" * 50)
    
    # æ­¥é©Ÿ 1: å‰µå»ºåŸºç¤ ViT æ¨¡å‹ï¼ˆé€šå¸¸åœ¨å¤§è¦æ¨¡è³‡æ–™ä¸Šé è¨“ç·´ï¼‰
    print("ğŸ“Œ æ­¥é©Ÿ 1: å‰µå»ºåŸºç¤ ViT æ¨¡å‹")
    base_vit = ViT(
        image_size=256,
        patch_size=16,
        num_classes=1000,    # é è¨“ç·´ä»»å‹™çš„é¡åˆ¥æ•¸ï¼ˆå¦‚ ImageNetï¼‰
        dim=1024,
        depth=6,
        heads=8,
        mlp_dim=2048,
        dropout=0.1,
        emb_dropout=0.1
    )
    
    print(f"âœ… åŸºç¤ ViT å‰µå»ºå®Œæˆï¼Œåƒæ•¸é‡: {sum(p.numel() for p in base_vit.parameters()):,}")
    
    # æ¸¬è©¦åŸºç¤æ¨¡å‹
    img = torch.randn(4, 3, 256, 256)
    logits = base_vit(img)  # (4, 1000)
    print(f"âœ… åŸºç¤æ¨¡å‹è¼¸å‡ºå½¢ç‹€: {logits.shape}")
    
    # åœ¨é€™è£¡é€²è¡Œä½ çš„é è¨“ç·´...
    print("ğŸ”„ [é€™è£¡é€²è¡Œå¤§è¦æ¨¡é è¨“ç·´...]")
    
    # æ­¥é©Ÿ 2: ä»»å‹™ 1 - è§’è‰²æ€§åˆ¥åˆ†é¡ï¼ˆ2 é¡ï¼‰
    print("\nğŸ“Œ æ­¥é©Ÿ 2: å‰µå»ºæ€§åˆ¥åˆ†é¡ Adapter")
    gender_adapter = Adapter(
        vit=base_vit,               # ä½¿ç”¨é è¨“ç·´çš„ ViTï¼ˆåƒæ•¸å‡çµï¼‰
        num_classes=2,              # ç”·æ€§ vs å¥³æ€§
        num_memories_per_layer=5    # æ¯å±¤ 5 å€‹å¯å­¸ç¿’è¨˜æ†¶
    )
    
    # æ¸¬è©¦æ€§åˆ¥åˆ†é¡
    gender_output = gender_adapter(img)  # (4, 2)
    print(f"âœ… æ€§åˆ¥åˆ†é¡è¼¸å‡ºå½¢ç‹€: {gender_output.shape}")
    
    # æ­¥é©Ÿ 3: ä»»å‹™ 2 - è§’è‰²æƒ…ç·’åˆ†é¡ï¼ˆ5 é¡ï¼‰
    print("\nğŸ“Œ æ­¥é©Ÿ 3: å‰µå»ºæƒ…ç·’åˆ†é¡ Adapter") 
    emotion_adapter = Adapter(
        vit=base_vit,               # åŒæ¨£çš„é è¨“ç·´ ViT
        num_classes=5,              # 5 ç¨®æƒ…ç·’
        num_memories_per_layer=8    # æ›´è¤‡é›œçš„ä»»å‹™éœ€è¦æ›´å¤šè¨˜æ†¶
    )
    
    # æ¸¬è©¦æƒ…ç·’åˆ†é¡
    emotion_output = emotion_adapter(img)  # (4, 5)
    print(f"âœ… æƒ…ç·’åˆ†é¡è¼¸å‡ºå½¢ç‹€: {emotion_output.shape}")
    
    # æ­¥é©Ÿ 4: ä»»å‹™ 3 - 50 é¡è§’è‰²åˆ†é¡
    print("\nğŸ“Œ æ­¥é©Ÿ 4: å‰µå»º 50 é¡è§’è‰²åˆ†é¡ Adapter")
    character_adapter = Adapter(
        vit=base_vit,               # åŒæ¨£çš„é è¨“ç·´ ViT
        num_classes=50,             # 50 å€‹è§’è‰²é¡åˆ¥
        num_memories_per_layer=20   # æ›´å¤šé¡åˆ¥éœ€è¦æ›´å¤šè¨˜æ†¶
    )
    
    # æ¸¬è©¦è§’è‰²åˆ†é¡
    character_output = character_adapter(img)  # (4, 50)
    print(f"âœ… è§’è‰²åˆ†é¡è¼¸å‡ºå½¢ç‹€: {character_output.shape}")
    
    # åƒæ•¸é‡æ¯”è¼ƒ
    print("\nğŸ“Š åƒæ•¸é‡æ¯”è¼ƒ:")
    base_params = sum(p.numel() for p in base_vit.parameters())
    gender_params = sum(p.numel() for p in gender_adapter.parameters() if p.requires_grad)
    emotion_params = sum(p.numel() for p in emotion_adapter.parameters() if p.requires_grad)
    character_params = sum(p.numel() for p in character_adapter.parameters() if p.requires_grad)
    
    print(f"  åŸºç¤ ViT: {base_params:,} åƒæ•¸")
    print(f"  æ€§åˆ¥ Adapter: {gender_params:,} åƒæ•¸ ({gender_params/base_params*100:.2f}%)")
    print(f"  æƒ…ç·’ Adapter: {emotion_params:,} åƒæ•¸ ({emotion_params/base_params*100:.2f}%)")
    print(f"  è§’è‰² Adapter: {character_params:,} åƒæ•¸ ({character_params/base_params*100:.2f}%)")
    
    print("\nğŸ’¡ MemoryViT çš„å„ªå‹¢:")
    print("  âœ… ä¸€å€‹åŸºç¤æ¨¡å‹æœå‹™å¤šå€‹ä»»å‹™")
    print("  âœ… å…±äº«è¦–è¦ºç‰¹å¾µè¡¨ç¤º")
    print("  âœ… Adapter åƒæ•¸é‡å¾ˆå°")
    print("  âœ… é¿å…ç½é›£æ€§éºå¿˜")
    print("  âœ… æ–°å¢ä»»å‹™åªéœ€è¨“ç·´ Adapter")
    
    return base_vit, gender_adapter, emotion_adapter, character_adapter

if __name__ == "__main__":
    # é‹è¡Œæ¦‚å¿µæ¼”ç¤º
    base_vit, gender_adapter, emotion_adapter, character_adapter = test_memory_vit_concept()
    
    print("\nğŸš€ ç¾åœ¨ä½ å¯ä»¥:")
    print("  1. ä½¿ç”¨ MemoryViT_character_classifier.py è¨“ç·´ 50 é¡è§’è‰²åˆ†é¡")
    print("  2. åŒæ™‚å‰µå»ºå…¶ä»–ä»»å‹™çš„ Adapterï¼ˆæ€§åˆ¥ã€æƒ…ç·’ç­‰ï¼‰")
    print("  3. æ‰€æœ‰ä»»å‹™å…±äº«åŒä¸€å€‹åŸºç¤ ViT çš„è¦–è¦ºç‰¹å¾µ")

# do your usual training with ViT
# ...


# then, to finetune, just pass the ViT into the Adapter class
# you can do this for multiple Adapters, as shown below

adapter1 = Adapter(
    vit = v,
    num_classes = 2,               # number of output classes for this specific task
    num_memories_per_layer = 5     # number of learnable memories per layer, 10 was sufficient in paper
)

logits1 = adapter1(img) # (4, 2) - predict 2 classes off frozen ViT backbone with learnable memories and task specific head

# yet another task to finetune on, this time with 4 classes

adapter2 = Adapter(
    vit = v,
    num_classes = 4,
    num_memories_per_layer = 10
)

logits2 = adapter2(img) # (4, 4) - predict 4 classes off frozen ViT backbone with learnable memories and task specific head
