#!/usr/bin/env python3
"""
ğŸ”® EfficientNet æ¨¡å‹æ¨ç†å·¥å…· - Simpson è§’è‰²é æ¸¬

åŠŸèƒ½ï¼š
- è¼‰å…¥è¨“ç·´å¥½çš„ EfficientNet æ¨¡å‹
- å°å–®å¼µåœ–ç‰‡æˆ–æ‰¹é‡åœ–ç‰‡é€²è¡Œé æ¸¬
- æ”¯æ´ä¿¡å¿ƒåˆ†æ•¸å’Œå‰ N é æ¸¬
- é«˜æ•ˆæ¨ç†ï¼Œé©åˆéƒ¨ç½²ä½¿ç”¨
"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import timm
import os
import glob
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
import json
from pathlib import Path
import argparse
from tqdm import tqdm

class EfficientNetInference:
    """
    EfficientNet æ¨¡å‹æ¨ç†å™¨
    """
    
    def __init__(self, model_path, device=None):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        
        Args:
            model_path: æ¨¡å‹æª”æ¡ˆè·¯å¾‘ (.pth)
            device: è¨ˆç®—è¨­å‚™
        """
        # Windows è¨­å‚™å„ªå…ˆç´šï¼šCUDA > CPU  
        if device is None:
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                # é¡¯ç¤º GPU è³‡è¨Š
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"ğŸ® åµæ¸¬åˆ° GPU: {gpu_name} ({gpu_memory:.1f} GB)")
            else:
                self.device = torch.device("cpu")
                print("âš ï¸ æœªåµæ¸¬åˆ° GPUï¼Œä½¿ç”¨ CPU")
        else:
            self.device = device
            
        self.model = None
        self.class_to_idx = {}
        self.idx_to_class = {}
        self.model_name = None
        
        print(f"ğŸ”® EfficientNet æ¨ç†å™¨")
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # è¼‰å…¥æ¨¡å‹
        self.load_model(model_path)
        
        # æº–å‚™è®Šæ›
        self.transform = self._get_inference_transform()
        
    def load_model(self, model_path):
        """è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {model_path}")
        
        print(f"ğŸ“‚ è¼‰å…¥æ¨¡å‹: {model_path}")
        
        # è¼‰å…¥ checkpoint
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # ç²å–æ¨¡å‹è³‡è¨Š
        self.model_name = checkpoint['model_name']
        num_classes = checkpoint['num_classes']
        self.class_to_idx = checkpoint['class_to_idx']
        self.idx_to_class = checkpoint['idx_to_class']
        
        print(f"ğŸ¯ æ¨¡å‹: {self.model_name}")
        print(f"ğŸ“ é¡åˆ¥æ•¸: {num_classes}")
        
        # é‡å»ºæ¨¡å‹æ¶æ§‹
        self.model = timm.create_model(
            self.model_name,
            pretrained=False,  # ä¸éœ€è¦é è¨“ç·´æ¬Šé‡
            num_classes=num_classes
        )
        
        # è¼‰å…¥æ¬Šé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()  # è¨­ç‚ºæ¨ç†æ¨¡å¼
        
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    
    def _get_inference_transform(self):
        """ç²å–æ¨ç†ç”¨çš„åœ–ç‰‡è®Šæ›"""
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def predict_single(self, image_path, top_k=1):
        """
        é æ¸¬å–®å¼µåœ–ç‰‡
        
        Args:
            image_path: åœ–ç‰‡è·¯å¾‘
            top_k: è¿”å›å‰ k å€‹é æ¸¬çµæœ
            
        Returns:
            str æˆ– list: é æ¸¬çš„é¡åˆ¥åç¨±
        """
        try:
            # è¼‰å…¥ä¸¦é è™•ç†åœ–ç‰‡
            image = Image.open(image_path).convert('RGB')
            
            # è®Šæ›åœ–ç‰‡
            input_tensor = self.transform(image).unsqueeze(0).to(self.device)
            
            # GPU æ¨ç†
            with torch.no_grad():
                if self.device.type == 'cuda':
                    # GPU è¨˜æ†¶é«”å„ªåŒ–
                    torch.cuda.empty_cache()
                
                outputs = self.model(input_tensor)
                probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
                
                # ç²å– top-k çµæœ
                top_prob, top_indices = torch.topk(probabilities, top_k)
                
                if top_k == 1:
                    predicted_idx = top_indices[0].item()
                    predicted_class = self.idx_to_class[predicted_idx]
                    return predicted_class
                else:
                    results = []
                    for i in range(top_k):
                        idx = top_indices[i].item()
                        prob = top_prob[i].item()
                        class_name = self.idx_to_class[idx]
                        results.append({
                            'class': class_name,
                            'confidence': prob
                        })
                    return results
            
        except Exception as e:
            print(f"âŒ é æ¸¬å¤±æ•— {image_path}: {e}")
            return "unknown" if top_k == 1 else [{'class': 'unknown', 'confidence': 0.0}]
    
    def predict_batch(self, image_paths, batch_size=32):
        """
        æ‰¹æ¬¡é æ¸¬ - GPU åŠ é€Ÿç‰ˆæœ¬
        
        Args:
            image_paths: åœ–ç‰‡è·¯å¾‘åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            list: é æ¸¬çµæœåˆ—è¡¨
        """
        predictions = []
        
        # è¨ˆç®—ç¸½æ‰¹æ¬¡æ•¸
        total_batches = (len(image_paths) + batch_size - 1) // batch_size
        
        # æ·»åŠ é€²åº¦æ¢
        with tqdm(total=len(image_paths), desc="GPU æ‰¹æ¬¡æ¨ç†é€²åº¦", unit="å¼µ") as pbar:
            for i in range(0, len(image_paths), batch_size):
                batch_paths = image_paths[i:i+batch_size]
                batch_images = []
                valid_indices = []
                
                # è¼‰å…¥æ‰¹æ¬¡åœ–ç‰‡
                for idx, path in enumerate(batch_paths):
                    try:
                        image = Image.open(path).convert('RGB')
                        tensor = self.transform(image)
                        batch_images.append(tensor)
                        valid_indices.append(idx)
                    except Exception as e:
                        print(f"âŒ è¼‰å…¥å¤±æ•— {path}: {e}")
                        predictions.append("unknown")
                
                if batch_images:
                    # æ‰¹æ¬¡æ¨ç†
                    try:
                        batch_tensor = torch.stack(batch_images).to(self.device)
                        
                        with torch.no_grad():
                            if self.device.type == 'cuda':
                                torch.cuda.empty_cache()
                            
                            outputs = self.model(batch_tensor)
                            _, predicted = torch.max(outputs.data, 1)
                            
                            # è½‰æ›ç‚ºé¡åˆ¥åç¨±
                            for j, pred_idx in enumerate(predicted.cpu().numpy()):
                                if j < len(valid_indices):
                                    predicted_class = self.idx_to_class[pred_idx]
                                    # æ’å…¥æ­£ç¢ºä½ç½®
                                    while len(predictions) <= i + valid_indices[j]:
                                        predictions.append("unknown")
                                    predictions[i + valid_indices[j]] = predicted_class
                    
                    except Exception as e:
                        print(f"âŒ æ‰¹æ¬¡æ¨ç†å¤±æ•—: {e}")
                        # å›é€€åˆ°å–®å¼µæ¨ç†
                        for path in batch_paths:
                            predictions.append(self.predict_single(path))
                
                # æ›´æ–°é€²åº¦æ¢
                pbar.update(len(batch_paths))
        
        return predictions
    
    def predict_test_dataset(self, test_dir="Dataset/test", output_file="predictions.csv", batch_size=32, use_gpu_batch=True):
        """
        å°æ¸¬è©¦è³‡æ–™é›†é€²è¡Œæ‰¹é‡é æ¸¬ä¸¦è¼¸å‡º CSV - GPU å„ªåŒ–ç‰ˆæœ¬
        
        Args:
            test_dir: æ¸¬è©¦åœ–ç‰‡ç›®éŒ„
            output_file: è¼¸å‡º CSV æª”æ¡ˆåç¨±
            batch_size: æ‰¹æ¬¡å¤§å° (GPU æ™‚å»ºè­° 32-64)
            use_gpu_batch: æ˜¯å¦ä½¿ç”¨ GPU æ‰¹æ¬¡æ¨ç†
            
        Returns:
            pd.DataFrame: é æ¸¬çµæœ
        """
        print(f"\nğŸ“ é–‹å§‹è™•ç†æ¸¬è©¦è³‡æ–™é›†: {test_dir}")
        
        # æ”¯æ´çš„åœ–ç‰‡æ ¼å¼
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif', '*.webp']
        
        # æ”¶é›†æ‰€æœ‰åœ–ç‰‡è·¯å¾‘
        image_paths = []
        for ext in image_extensions:
            pattern = os.path.join(test_dir, ext)
            image_paths.extend(glob.glob(pattern))
        
        # æŒ‰æª”åæ•¸å­—æ’åº
        def sort_key(path):
            filename = os.path.basename(path)
            # æå–æ•¸å­—éƒ¨åˆ†é€²è¡Œæ’åº
            try:
                number = int(filename.split('.')[0])
                return number
            except:
                return 0
        
        image_paths.sort(key=sort_key)
        
        if not image_paths:
            print("âŒ æ‰¾ä¸åˆ°ä»»ä½•åœ–ç‰‡ï¼")
            return None
        
        print(f"ğŸ” æ‰¾åˆ° {len(image_paths)} å¼µåœ–ç‰‡")
        
        # æ ¹æ“šè¨­å‚™é¸æ“‡æ¨ç†æ–¹å¼
        if self.device.type == 'cuda' and use_gpu_batch:
            print(f"ğŸ® ä½¿ç”¨ GPU æ‰¹æ¬¡æ¨ç† (æ‰¹æ¬¡å¤§å°: {batch_size})")
            predicted_classes = self.predict_batch(image_paths, batch_size)
        else:
            print(f"ğŸ’» ä½¿ç”¨é€å¼µæ¨ç†")
            predicted_classes = []
            for image_path in tqdm(image_paths, desc="é€å¼µæ¨ç†é€²åº¦", unit="å¼µ"):
                predicted_class = self.predict_single(image_path)
                predicted_classes.append(predicted_class)
        
        # æº–å‚™çµæœ
        results = []
        for image_path, predicted_class in zip(image_paths, predicted_classes):
            filename = os.path.basename(image_path)
            # ç§»é™¤å‰¯æª”å (.jpg, .png ç­‰)
            id_name = os.path.splitext(filename)[0]
            
            results.append({
                'id': id_name,
                'character': predicted_class
            })
        
        # å‰µå»º DataFrame
        df = pd.DataFrame(results)
        
        # ä¿å­˜ CSV
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"ğŸ’¾ é æ¸¬çµæœå·²ä¿å­˜è‡³: {output_file}")
        
        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        print(f"\nğŸ“Š é æ¸¬çµ±è¨ˆ:")
        print(f"   ç¸½åœ–ç‰‡æ•¸: {len(df)}")
        print(f"   ä½¿ç”¨è¨­å‚™: {self.device}")
        if self.device.type == 'cuda':
            print(f"   GPU è¨˜æ†¶é«”ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
        print(f"   é æ¸¬é¡åˆ¥åˆ†å¸ƒ:")
        for class_name, count in df['character'].value_counts().head(10).items():
            print(f"     {class_name}: {count}")
        
        return df
    
    def predict_and_show(self, image_path, save_plot=True):
        """
        é æ¸¬åœ–ç‰‡ä¸¦è¦–è¦ºåŒ–çµæœ
        
        Args:
            image_path: åœ–ç‰‡è·¯å¾‘
            save_plot: æ˜¯å¦ä¿å­˜çµæœåœ–ç‰‡
        """
        # é æ¸¬
        result = self.predict_single(image_path, top_k=5)
        
        # è¼‰å…¥åŸåœ–
        image = Image.open(image_path).convert('RGB')
        
        # ç¹ªè£½çµæœ
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # é¡¯ç¤ºåŸåœ–
        ax1.imshow(image)
        ax1.set_title(f'åŸåœ–: {os.path.basename(image_path)}')
        ax1.axis('off')
        
        # é¡¯ç¤ºé æ¸¬çµæœ
        predictions = result['predictions']
        class_names = [pred['class_name'] for pred in predictions]
        confidences = [pred['confidence'] for pred in predictions]
        
        bars = ax2.barh(range(len(class_names)), confidences)
        ax2.set_yticks(range(len(class_names)))
        ax2.set_yticklabels(class_names)
        ax2.set_xlabel('ä¿¡å¿ƒåˆ†æ•¸')
        ax2.set_title('é æ¸¬çµæœ (å‰5å)')
        ax2.set_xlim(0, 1)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i, (bar, conf) in enumerate(zip(bars, confidences)):
            ax2.text(conf + 0.01, i, f'{conf:.3f}', 
                    va='center', fontsize=10)
        
        # æ¨™è¨˜æœ€ä½³é æ¸¬
        if confidences:
            bars[0].set_color('gold')
            ax2.text(0.5, len(class_names), 
                    f'æœ€ä½³é æ¸¬: {class_names[0]} ({confidences[0]:.3f})',
                    ha='center', fontweight='bold', fontsize=12)
        
        plt.tight_layout()
        
        if save_plot:
            output_name = f"prediction_{os.path.splitext(os.path.basename(image_path))[0]}.png"
            plt.savefig(output_name, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š çµæœåœ–å·²ä¿å­˜: {output_name}")
        
        plt.show()
        
        # æ‰“å°çµæœ
        print(f"\nğŸ¯ é æ¸¬çµæœ:")
        for i, pred in enumerate(predictions):
            icon = "ğŸ†" if i == 0 else f"{i+1}."
            print(f"{icon} {pred['class_name']}: {pred['confidence']:.3f}")
        
        return result
    
    def get_model_info(self):
        """ç²å–æ¨¡å‹è³‡è¨Š"""
        total_params = sum(p.numel() for p in self.model.parameters())
        
        info = {
            'model_name': self.model_name,
            'num_classes': len(self.idx_to_class),
            'total_parameters': f"{total_params/1e6:.1f}M",
            'device': str(self.device),
            'class_names': list(self.class_to_idx.keys())
        }
        
        return info
    
    def print_model_info(self):
        """æ‰“å°æ¨¡å‹è³‡è¨Š"""
        info = self.get_model_info()
        
        print(f"\nğŸ“Š æ¨¡å‹è³‡è¨Š:")
        print(f"   æ¨¡å‹: {info['model_name']}")
        print(f"   é¡åˆ¥æ•¸: {info['num_classes']}")
        print(f"   åƒæ•¸é‡: {info['total_parameters']}")
        print(f"   è¨­å‚™: {info['device']}")
        print(f"   é¡åˆ¥åˆ—è¡¨: {info['class_names'][:10]}..." if len(info['class_names']) > 10 else f"   é¡åˆ¥åˆ—è¡¨: {info['class_names']}")

def main():
    """ä¸»å‡½æ•¸ - å‘½ä»¤åˆ—ä»‹é¢"""
    parser = argparse.ArgumentParser(description="EfficientNet æ¨¡å‹æ¨ç†å·¥å…·")
    parser.add_argument('--model', '-m', required=True, help='æ¨¡å‹æª”æ¡ˆè·¯å¾‘ (.pth)')
    parser.add_argument('--image', '-i', help='å–®å¼µåœ–ç‰‡è·¯å¾‘')
    parser.add_argument('--folder', '-f',default='/mnt/e/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset/raw/test',help='åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘(Default:/mnt/e/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset/raw/test)')
    parser.add_argument('--output', '-o', help='è¼¸å‡ºçµæœæª”æ¡ˆ (JSON)')
    parser.add_argument('--top-k', '-k', type=int, default=5, help='å‰ k å€‹é æ¸¬çµæœ')
    parser.add_argument('--show', action='store_true', help='é¡¯ç¤ºé æ¸¬çµæœåœ–')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    try:
        inferencer = EfficientNetInference(args.model)
        inferencer.print_model_info()
    except Exception as e:
        print(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
        return
    
    # åŸ·è¡Œæ¨ç†
    if args.image:
        # å–®å¼µåœ–ç‰‡æ¨ç†
        try:
            if args.show:
                result = inferencer.predict_and_show(args.image)
            else:
                result = inferencer.predict_single(args.image, top_k=args.top_k)
                print(f"\nğŸ¯ é æ¸¬çµæœ:")
                for i, pred in enumerate(result['predictions']):
                    print(f"{i+1}. {pred['class_name']}: {pred['confidence']:.3f}")
        except Exception as e:
            print(f"âŒ é æ¸¬å¤±æ•—: {e}")
    
    elif args.folder:
        # æ‰¹é‡æ¨ç†
        try:
            results = inferencer.predict_batch(
                args.folder, 
                output_file=args.output, 
                top_k=args.top_k
            )
        except Exception as e:
            print(f"âŒ æ‰¹é‡æ¨ç†å¤±æ•—: {e}")
    
    else:
        print("âŒ è«‹æŒ‡å®š --image æˆ– --folder åƒæ•¸")
        print("ğŸ’¡ ä½¿ç”¨ç¯„ä¾‹:")
        print("   python EfficientNet_inference.py -m best_model.pth -i test_image.jpg --show")
        print("   python EfficientNet_inference.py -m best_model.pth -f test_folder/ -o results.json")

if __name__ == "__main__":
    # å¦‚æœæ²’æœ‰å‘½ä»¤åˆ—åƒæ•¸ï¼Œä½¿ç”¨äº’å‹•æ¨¡å¼
    import sys
    
    if len(sys.argv) == 1:
        print("ğŸ”® EfficientNet æ¨ç†å·¥å…· - äº’å‹•æ¨¡å¼")
        print("=" * 50)
        
        # å°‹æ‰¾å¯ç”¨çš„æ¨¡å‹
        model_files = glob.glob("*.pth") + glob.glob("best_*.pth")
        
        if not model_files:
            print("âŒ ç•¶å‰ç›®éŒ„æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ (.pth)")
            model_path = input("è«‹è¼¸å…¥æ¨¡å‹æª”æ¡ˆè·¯å¾‘: ").strip()
        else:
            print("ğŸ” æ‰¾åˆ°ä»¥ä¸‹æ¨¡å‹æª”æ¡ˆ:")
            for i, model_file in enumerate(model_files):
                print(f"{i+1}. {model_file}")
            
            choice = input(f"è«‹é¸æ“‡æ¨¡å‹ (1-{len(model_files)}): ").strip()
            try:
                model_path = model_files[int(choice)-1]
            except (ValueError, IndexError):
                model_path = model_files[0]
        
        # åˆå§‹åŒ–æ¨ç†å™¨
        try:
            inferencer = EfficientNetInference(model_path)
            inferencer.print_model_info()
        except Exception as e:
            print(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
            exit(1)
        
        import platform
        is_wsl = "microsoft" in platform.uname().release.lower() or "WSL" in os.environ.get("WSL_DISTRO_NAME", "")

        if is_wsl:
            test_dir = input("æ¸¬è©¦åœ–ç‰‡ç›®éŒ„ (é è¨­: /mnt/e/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset/raw/test): ").strip()
            if not test_dir:
                test_dir = "/mnt/e/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset/raw/test"
        else:  
            # è¨­å®šæ¸¬è©¦ç›®éŒ„ - Windows è·¯å¾‘
            test_dir = input("æ¸¬è©¦åœ–ç‰‡ç›®éŒ„ (é è¨­: E:/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset/raw/test): ").strip()
            if not test_dir:
                test_dir = "E:/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset/raw/test"

        # è¨­å®šæ‰¹æ¬¡å¤§å° (GPU å„ªåŒ–)
        batch_size_input = input("æ‰¹æ¬¡å¤§å° (é è¨­: 32, GPU å»ºè­° 32-64): ").strip()
        try:
            batch_size = int(batch_size_input) if batch_size_input else 32
        except ValueError:
            batch_size = 32
        
        # è¨­å®šè¼¸å‡ºæª”æ¡ˆ
        model_name = model_files[int(choice)-1].split('_')[0]
        output_file = input(f"è¼¸å‡ºæª”æ¡ˆåç¨± (é è¨­: {model_name}_predictions.csv): ").strip()
        if not output_file:
            output_file = f"{model_name}_predictions.csv"

        # åŸ·è¡Œæ¨ç†
        try:
            print(f"\nğŸ¯ é–‹å§‹é æ¸¬...")
            print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {batch_size}")
            print(f"ğŸ® è¨­å‚™: {inferencer.device}")
            
            df = inferencer.predict_test_dataset(
                test_dir, 
                output_file, 
                batch_size=batch_size,
                use_gpu_batch=inferencer.device.type == 'cuda'
            )
            
            if df is not None:
                print(f"\nğŸ‰ æ¨ç†å®Œæˆï¼")
                print(f"ğŸ“„ çµæœå·²ä¿å­˜è‡³: {output_file}")
                
                # é¡¯ç¤ºå‰å¹¾å€‹çµæœ
                print(f"\nğŸ“‹ å‰ 10 å€‹é æ¸¬çµæœ:")
                print(df.head(10).to_string(index=False))
                
            else:
                print("âŒ æ¨ç†å¤±æ•—ï¼")
                
        except Exception as e:
            print(f"âŒ æ¨ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")