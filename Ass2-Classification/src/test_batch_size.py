#!/usr/bin/env python3
"""
Batch Size æª¢æ¸¬å™¨
å¿«é€Ÿæª¢æ¸¬æ‚¨çš„ GPU å¯ä»¥æ”¯æ´çš„æœ€å¤§ batch size
"""

import torch
import sys
import os

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from MemoryViT_character_classifier import MemoryViTCharacterClassifier, get_best_data_path

def test_batch_size():
    """å¿«é€Ÿæ¸¬è©¦æœ€ä½³ batch size"""
    print("ğŸ” MemoryViT Batch Size æª¢æ¸¬å™¨")
    print("=" * 50)
    
    # æª¢æŸ¥ GPU
    if not torch.cuda.is_available():
        print("âŒ æœªæª¢æ¸¬åˆ° GPUï¼Œç„¡æ³•é€²è¡Œ batch size æª¢æ¸¬")
        print("ğŸ’¡ å»ºè­°åœ¨ CPU æ¨¡å¼ä¸‹ä½¿ç”¨ batch_size=4")
        return
    
    device = torch.device('cuda')
    print(f"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    try:
        # ç²å–è³‡æ–™è·¯å¾‘
        data_paths, data_type = get_best_data_path()
        print(f"ğŸ“‚ ä½¿ç”¨è³‡æ–™: {data_type}")
        
        # åˆå§‹åŒ–åˆ†é¡å™¨
        print("\nğŸ—ï¸ åˆå§‹åŒ– MemoryViT æ¨¡å‹...")
        classifier = MemoryViTCharacterClassifier(
            num_classes=50,
            image_size=224,
            device=device
        )
        
        # æº–å‚™è³‡æ–™ï¼ˆåªéœ€è¦å°‘é‡è³‡æ–™ä¾†å‰µå»ºæ¨¡å‹ï¼‰
        print("ğŸ“Š æº–å‚™è³‡æ–™...")
        train_dataset, val_dataset, test_dataset = classifier.prepare_data(data_paths)
        
        # åŸ·è¡Œ batch size æª¢æ¸¬
        print("\n" + "="*50)
        optimal_batch_size = classifier.find_optimal_batch_size(
            max_batch_size=256,  # æ¸¬è©¦æ›´å¤§çš„ç¯„åœ
            start_batch_size=16
        )
        print("="*50)
        
        # é¡¯ç¤ºå»ºè­°
        print(f"\nğŸ¯ æª¢æ¸¬å®Œæˆï¼")
        print(f"ğŸ“ å»ºè­°åœ¨è¨“ç·´æ™‚ä½¿ç”¨: batch_size={optimal_batch_size}")
        
        # æä¾›ä¸åŒå ´æ™¯çš„å»ºè­°
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè­°:")
        print(f"   ğŸš€ å¿«é€Ÿæ¸¬è©¦: batch_size={min(optimal_batch_size, 32)}")
        print(f"   âš¡ æœ€ä½³æ•ˆèƒ½: batch_size={optimal_batch_size}")
        print(f"   ğŸ›¡ï¸ ä¿å®ˆå®‰å…¨: batch_size={max(16, optimal_batch_size // 2)}")
        
        # ä¿å­˜çµæœ
        with open('optimal_batch_size.txt', 'w') as f:
            f.write(f"Optimal Batch Size: {optimal_batch_size}\n")
            f.write(f"GPU: {torch.cuda.get_device_name(0)}\n")
            f.write(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\n")
        
        print(f"\nğŸ’¾ çµæœå·²ä¿å­˜è‡³: optimal_batch_size.txt")
        
    except FileNotFoundError as e:
        print(f"âŒ è³‡æ–™è·¯å¾‘éŒ¯èª¤: {e}")
        print("è«‹ç¢ºèªè³‡æ–™å¤¾çµæ§‹æ­£ç¢º")
    except Exception as e:
        print(f"âŒ æª¢æ¸¬éç¨‹å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()

def quick_recommendation():
    """åŸºæ–¼ GPU è¨˜æ†¶é«”çµ¦å‡ºå¿«é€Ÿå»ºè­°"""
    print("\nğŸš€ å¿«é€Ÿ GPU è¨˜æ†¶é«”å»ºè­°:")
    
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        gpu_name = torch.cuda.get_device_name(0)
        
        print(f"GPU: {gpu_name} ({gpu_memory:.1f} GB)")
        
        if gpu_memory >= 16:
            recommended = 64
            tier = "ğŸ”¥ é«˜ç«¯"
        elif gpu_memory >= 12:
            recommended = 48
            tier = "ğŸ’ª é«˜æ€§èƒ½"
        elif gpu_memory >= 8:
            recommended = 32
            tier = "âš¡ ä¸­é«˜ç«¯"
        elif gpu_memory >= 6:
            recommended = 24
            tier = "ğŸ‘ ä¸»æµ"
        else:
            recommended = 16
            tier = "ğŸ’¼ å…¥é–€"
        
        print(f"ç­‰ç´š: {tier}")
        print(f"å»ºè­° batch size: {recommended}")
        
        return recommended
    else:
        print("æœªæª¢æ¸¬åˆ° GPU")
        return 4

if __name__ == "__main__":
    print("é¸æ“‡æª¢æ¸¬æ¨¡å¼:")
    print("1. å®Œæ•´æª¢æ¸¬ (ç²¾ç¢ºï¼Œéœ€è¦ç´„ 2-3 åˆ†é˜)")
    print("2. å¿«é€Ÿå»ºè­° (åŸºæ–¼ GPU è¨˜æ†¶é«”ï¼Œç«‹å³å®Œæˆ)")
    
    choice = input("è«‹é¸æ“‡ (1/2): ").strip()
    
    if choice == "2":
        quick_recommendation()
    else:
        test_batch_size()