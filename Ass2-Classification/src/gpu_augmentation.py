#!/usr/bin/env python
"""
GPU åŠ é€Ÿè³‡æ–™å¢å¼·è…³æœ¬ - é‡å°é è™•ç†çš„è¾›æ™®æ£®è§’è‰²è³‡æ–™
ä½¿ç”¨ GPU æ‰¹æ¬¡è™•ç†æå‡é€Ÿåº¦
"""

import os
import random
import shutil
from PIL import Image
import torch
import torch.nn.functional as F
import torchvision.transforms.v2 as T
from torchvision.transforms import functional as TF
from pathlib import Path
import argparse
from tqdm import tqdm
import platform
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import multiprocessing as mp

def get_default_paths():
    """æ ¹æ“šé‹è¡Œç’°å¢ƒè‡ªå‹•é¸æ“‡é è¨­è·¯å¾‘"""
    is_wsl = "microsoft" in platform.uname().release.lower() or "WSL" in os.environ.get("WSL_DISTRO_NAME", "")
    
    if is_wsl:
        base_path = "/mnt/e/NYCU/NYCU_IAII_ML2025/Ass2-Classification"
        input_dir = f"{base_path}/Dataset/preprocessed/train"
        output_dir = f"{base_path}/Dataset/augmented/train"
        backgrounds_dir = f"{base_path}/backgrounds"
    else:
        input_dir = "E:/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset/preprocessed/train"
        output_dir = "E:/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset/augmented/train"
        backgrounds_dir = "E:/NYCU/NYCU_IAII_ML2025/Ass2-Classification/backgrounds"
    
    return input_dir, output_dir, backgrounds_dir

# ===== GPU åŠ é€Ÿçš„å™ªè²é¡åˆ¥ =====

class GPUAddGaussianNoise(torch.nn.Module):
    """GPU åŠ é€Ÿçš„é«˜æ–¯å™ªè²"""
    def __init__(self, mean=0., std=1.):
        super().__init__()
        self.std = std
        self.mean = mean

    def forward(self, tensor):
        if tensor.is_cuda:
            noise = torch.randn_like(tensor, device=tensor.device) * self.std + self.mean
        else:
            noise = torch.randn_like(tensor) * self.std + self.mean
        return tensor + noise

class GPUAddSpeckleNoise(torch.nn.Module):
    """GPU åŠ é€Ÿçš„æ•£æ–‘å™ªè²"""
    def __init__(self, noise_level=0.1):
        super().__init__()
        self.noise_level = noise_level

    def forward(self, tensor):
        if tensor.is_cuda:
            noise = torch.randn_like(tensor, device=tensor.device) * self.noise_level
        else:
            noise = torch.randn_like(tensor) * self.noise_level
        noisy_tensor = tensor * (1 + noise)
        return torch.clamp(noisy_tensor, 0, 1)

class GPUAddPoissonNoise(torch.nn.Module):
    """GPU åŠ é€Ÿçš„æ³Šæ¾å™ªè²"""
    def __init__(self, lam=1.0):
        super().__init__()
        self.lam = lam

    def forward(self, tensor):
        if tensor.is_cuda:
            noise = torch.poisson(self.lam * torch.ones_like(tensor, device=tensor.device))
        else:
            noise = torch.poisson(self.lam * torch.ones_like(tensor))
        noisy_tensor = tensor + noise / 255.0
        return torch.clamp(noisy_tensor, 0, 1)

class GPUAddSaltPepperNoise(torch.nn.Module):
    """GPU åŠ é€Ÿçš„æ¤’é¹½å™ªè²"""
    def __init__(self, salt_prob=0.05, pepper_prob=0.05):
        super().__init__()
        self.salt_prob = salt_prob
        self.pepper_prob = pepper_prob

    def forward(self, tensor):
        if tensor.is_cuda:
            noise = torch.rand_like(tensor, device=tensor.device)
        else:
            noise = torch.rand_like(tensor)
        
        tensor = tensor.clone()
        tensor[noise < self.salt_prob] = 1
        tensor[noise > 1 - self.pepper_prob] = 0
        return tensor

class GPUAugmentationPipeline(torch.nn.Module):
    """GPU åŠ é€Ÿçš„è³‡æ–™å¢å¼·ç®¡é“"""
    
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        super().__init__()
        self.device = device
        
        # èˆ‡ä½ çš„ data_aggV1.py å®Œå…¨ä¸€è‡´çš„è®Šæ›
        self.transforms = torch.nn.Sequential(
            T.RandomApply([T.RandomHorizontalFlip()], p=0.1),
            T.RandomApply([T.RandomVerticalFlip()], p=0.1),
            T.RandomApply([T.RandomRotation(10)], p=0.1),
            
            T.RandomApply([T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)], p=0.1),
            T.RandomGrayscale(p=0.1),
            T.RandomInvert(p=0.1),
            T.RandomPosterize(bits=2, p=0.1),
            T.RandomApply([T.RandomSolarize(threshold=1.0)], p=0.05),
            T.RandomApply([T.RandomAdjustSharpness(sharpness_factor=2)], p=0.1),
            
            T.RandomApply([GPUAddGaussianNoise(0., 0.05)], p=0.1),
            T.RandomApply([GPUAddPoissonNoise(lam=0.1)], p=0.1),
            T.RandomApply([GPUAddSpeckleNoise(noise_level=0.1)], p=0.1),
            T.RandomApply([GPUAddSaltPepperNoise(salt_prob=0.05, pepper_prob=0.05)], p=0.1),
            
            T.RandomApply([T.RandomPerspective(distortion_scale=0.6, p=1.0)], p=0.1),
            T.RandomApply([T.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75))], p=0.1),
            T.RandomApply([T.ElasticTransform(alpha=250.0)], p=0.1),
            
            T.RandomApply([T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.))], p=0.1),
            T.RandomApply([GPUAddGaussianNoise(0., 0.001)], p=1.0)
        )
        
        self.to(device)
    
    def forward(self, batch_tensor):
        """
        è™•ç†ä¸€å€‹æ‰¹æ¬¡çš„åœ–ç‰‡
        Args:
            batch_tensor: (B, C, H, W) çš„ tensor
        Returns:
            å¢å¼·å¾Œçš„ tensor
        """
        return self.transforms(batch_tensor)

def load_images_as_batch(image_paths, batch_size=8, target_size=(256, 256)):
    """
    æ‰¹æ¬¡è¼‰å…¥åœ–ç‰‡ç‚º tensor
    
    Args:
        image_paths: åœ–ç‰‡è·¯å¾‘åˆ—è¡¨
        batch_size: æ‰¹æ¬¡å¤§å°
        target_size: ç›®æ¨™å°ºå¯¸
    
    Returns:
        batches: [(tensor, paths), ...] åˆ—è¡¨
    """
    batches = []
    
    for i in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[i:i+batch_size]
        batch_tensors = []
        valid_paths = []
        
        for img_path in batch_paths:
            try:
                # è¼‰å…¥ä¸¦è½‰æ›ç‚º tensor
                img = Image.open(img_path).convert("RGB")
                img = img.resize(target_size)
                tensor = TF.to_tensor(img)  # (C, H, W)
                batch_tensors.append(tensor)
                valid_paths.append(img_path)
            except Exception as e:
                print(f"âš ï¸  è·³éæå£çš„åœ–ç‰‡: {img_path} - {e}")
        
        if batch_tensors:
            # çµ„åˆæˆæ‰¹æ¬¡ (B, C, H, W)
            batch_tensor = torch.stack(batch_tensors)
            batches.append((batch_tensor, valid_paths))
    
    return batches

def save_batch_images(batch_tensor, output_paths, quality=95):
    """
    æ‰¹æ¬¡å„²å­˜ tensor ç‚ºåœ–ç‰‡
    
    Args:
        batch_tensor: (B, C, H, W) tensor
        output_paths: è¼¸å‡ºè·¯å¾‘åˆ—è¡¨
        quality: JPEG å“è³ª
    """
    # ç¢ºä¿ tensor åœ¨ CPU ä¸Š
    if batch_tensor.is_cuda:
        batch_tensor = batch_tensor.cpu()
    
    for i, (tensor, output_path) in enumerate(zip(batch_tensor, output_paths)):
        # tensor è½‰æ›ç‚º PIL Image
        img = TF.to_pil_image(tensor)
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å„²å­˜åœ–ç‰‡
        img.save(output_path, "JPEG", quality=quality)

def gpu_augment_class(class_dir, output_class_dir, augmentation_pipeline, 
                     augment_per_image=3, batch_size=8, device='cuda'):
    """
    GPU åŠ é€Ÿå–®ä¸€é¡åˆ¥çš„å¢å¼·
    
    Args:
        class_dir: è¼¸å…¥é¡åˆ¥è³‡æ–™å¤¾
        output_class_dir: è¼¸å‡ºé¡åˆ¥è³‡æ–™å¤¾
        augmentation_pipeline: GPU å¢å¼·ç®¡é“
        augment_per_image: æ¯å¼µåœ–ç‰‡çš„å¢å¼·æ•¸é‡
        batch_size: æ‰¹æ¬¡å¤§å°
        device: è¨­å‚™
    """
    # ç²å–æ‰€æœ‰åœ–ç‰‡è·¯å¾‘
    image_files = []
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
        image_files.extend(list(class_dir.glob(ext)))
    
    if not image_files:
        return 0
    
    # å‰µå»ºè¼¸å‡ºè³‡æ–™å¤¾
    output_class_dir.mkdir(parents=True, exist_ok=True)
    
    # å…ˆè¤‡è£½åŸå§‹åœ–ç‰‡
    for img_file in image_files:
        shutil.copy2(img_file, output_class_dir / img_file.name)
    
    total_augmented = 0
    
    # å°æ¯å¼µåœ–ç‰‡ç”Ÿæˆå¤šå€‹å¢å¼·ç‰ˆæœ¬
    for aug_idx in range(augment_per_image):
        # æ‰¹æ¬¡è¼‰å…¥åœ–ç‰‡
        batches = load_images_as_batch(image_files, batch_size)
        
        for batch_tensor, batch_paths in batches:
            # ç§»å‹•åˆ° GPU
            batch_tensor = batch_tensor.to(device)
            
            # åŸ·è¡Œå¢å¼·
            with torch.no_grad():
                augmented_batch = augmentation_pipeline(batch_tensor)
            
            # ç”Ÿæˆè¼¸å‡ºè·¯å¾‘
            output_paths = []
            for img_path in batch_paths:
                base_name = Path(img_path).stem
                ext = Path(img_path).suffix
                aug_filename = f"{base_name}_aug_gpu_{aug_idx:02d}{ext}"
                output_paths.append(output_class_dir / aug_filename)
            
            # æ‰¹æ¬¡å„²å­˜
            save_batch_images(augmented_batch, output_paths)
            total_augmented += len(batch_paths)
    
    return total_augmented

def gpu_augment_dataset(input_dir, output_dir, augment_per_image=3, 
                       batch_size=8, device=None):
    """
    GPU åŠ é€Ÿè³‡æ–™é›†å¢å¼·
    
    Args:
        input_dir: è¼¸å…¥è³‡æ–™å¤¾
        output_dir: è¼¸å‡ºè³‡æ–™å¤¾
        augment_per_image: æ¯å¼µåœ–ç‰‡å¢å¼·æ•¸é‡
        batch_size: GPU æ‰¹æ¬¡å¤§å°
        device: GPU è¨­å‚™
    """
    # è‡ªå‹•é¸æ“‡è¨­å‚™
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"ğŸš€ GPU åŠ é€Ÿè³‡æ–™å¢å¼·")
    print(f"âš¡ ä½¿ç”¨è¨­å‚™: {device}")
    if device == 'cuda':
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # æª¢æŸ¥ GPU è¨˜æ†¶é«”ä¸¦èª¿æ•´æ‰¹æ¬¡å¤§å°
    if device == 'cuda':
        gpu_memory = torch.cuda.get_device_properties(0).total_memory
        if gpu_memory < 4 * 1024**3:  # < 4GB
            batch_size = min(batch_size, 4)
            print(f"âš ï¸  GPU è¨˜æ†¶é«”è¼ƒå°ï¼Œèª¿æ•´æ‰¹æ¬¡å¤§å°ç‚º: {batch_size}")
    
    # å‰µå»ºå¢å¼·ç®¡é“
    augmentation_pipeline = GPUAugmentationPipeline(device)
    
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # è™•ç†æ¯å€‹é¡åˆ¥
    class_dirs = [d for d in input_path.iterdir() if d.is_dir()]
    
    print(f"\nğŸ“ é–‹å§‹è™•ç† {len(class_dirs)} å€‹é¡åˆ¥...")
    
    total_augmented = 0
    
    for class_dir in tqdm(class_dirs, desc="è™•ç†é¡åˆ¥"):
        class_name = class_dir.name
        output_class_dir = output_path / class_name
        
        try:
            augmented_count = gpu_augment_class(
                class_dir, output_class_dir, augmentation_pipeline,
                augment_per_image, batch_size, device
            )
            total_augmented += augmented_count
            
            # æ¸…ç† GPU å¿«å–
            if device == 'cuda':
                torch.cuda.empty_cache()
                
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"âŒ GPU è¨˜æ†¶é«”ä¸è¶³è™•ç† {class_name}ï¼Œé™ä½æ‰¹æ¬¡å¤§å°é‡è©¦...")
                torch.cuda.empty_cache()
                
                # é™ä½æ‰¹æ¬¡å¤§å°é‡è©¦
                smaller_batch = max(1, batch_size // 2)
                augmented_count = gpu_augment_class(
                    class_dir, output_class_dir, augmentation_pipeline,
                    augment_per_image, smaller_batch, device
                )
                total_augmented += augmented_count
            else:
                raise e
    
    print(f"\nâœ… GPU åŠ é€Ÿå¢å¼·å®Œæˆ!")
    print(f"ğŸ“Š ç¸½å…±ç”Ÿæˆ {total_augmented} å¼µå¢å¼·åœ–ç‰‡")

def main():
    """ä¸»å‡½æ•¸"""
    default_input, default_output, _ = get_default_paths()
    
    parser = argparse.ArgumentParser(description="GPU åŠ é€Ÿè¾›æ™®æ£®è§’è‰²è³‡æ–™å¢å¼·")
    parser.add_argument("--input_dir", type=str, default=default_input, help="è¼¸å…¥è³‡æ–™å¤¾è·¯å¾‘")
    parser.add_argument("--output_dir", type=str, default=default_output, help="è¼¸å‡ºè³‡æ–™å¤¾è·¯å¾‘")
    parser.add_argument("--augment_per_image", type=int, default=3, help="æ¯å¼µåœ–ç‰‡å¢å¼·æ•¸é‡")
    parser.add_argument("--batch_size", type=int, default=8, help="GPU æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--device", type=str, default=None, help="æŒ‡å®šè¨­å‚™ (cuda/cpu)")
    parser.add_argument("--cpu_only", action="store_true", help="å¼·åˆ¶ä½¿ç”¨ CPU")
    
    args = parser.parse_args()
    
    # è¨­å‚™é¸æ“‡
    if args.cpu_only:
        device = 'cpu'
    elif args.device:
        device = args.device
    else:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print("âš¡ GPU åŠ é€Ÿè¾›æ™®æ£®è§’è‰²è³‡æ–™å¢å¼·")
    print("=" * 50)
    print(f"ğŸ“‚ è¼¸å…¥: {args.input_dir}")
    print(f"ğŸ“‚ è¼¸å‡º: {args.output_dir}")
    print(f"ğŸ”¢ æ¯å¼µåœ–ç‰‡å¢å¼·: {args.augment_per_image} æ¬¡")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"âš¡ è¨­å‚™: {device}")
    
    # æª¢æŸ¥è¼¸å…¥
    if not os.path.exists(args.input_dir):
        print(f"âŒ è¼¸å…¥è³‡æ–™å¤¾ä¸å­˜åœ¨: {args.input_dir}")
        return
    
    # åŸ·è¡Œå¢å¼·
    gpu_augment_dataset(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        augment_per_image=args.augment_per_image,
        batch_size=args.batch_size,
        device=device
    )

if __name__ == "__main__":
    main()