#!/usr/bin/env python3
"""
ğŸš€ EfficientNet Simpson è§’è‰²åˆ†é¡å™¨ - é«˜é€Ÿç‰ˆæœ¬

EfficientNet å„ªå‹¢ï¼š
- é€Ÿåº¦æ¯” ViT å¿« 5-10 å€
- è¨˜æ†¶é«”éœ€æ±‚ä½
- åœ¨ä¸­å°å‹è³‡æ–™é›†ä¸Šè¡¨ç¾å„ªç§€
- é©åˆå¿«é€Ÿå¯¦é©—å’Œéƒ¨ç½²
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from torchvision import datasets
import torchvision.models as models
import timm
import os
import glob
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm
import time
import json
import platform

class EfficientNetCharacterClassifier:
    """
    ä½¿ç”¨ EfficientNet çš„é«˜é€Ÿè§’è‰²åˆ†é¡å™¨
    """
    
    def __init__(self, num_classes=50, model_name='efficientnet_b3', device=None):
        """
        åˆå§‹åŒ–åˆ†é¡å™¨
        
        Args:
            num_classes: é¡åˆ¥æ•¸é‡
            model_name: EfficientNet æ¨¡å‹åç¨±
                ğŸš€ é€Ÿåº¦å„ªå…ˆ:
                - efficientnet_b0: æœ€å¿« (5.3Måƒæ•¸)
                - efficientnet_b1: å¾ˆå¿« (7.8Måƒæ•¸) 
                - efficientnet_b2: å¿« (9.2Måƒæ•¸)
                
                âš–ï¸ å¹³è¡¡é¸æ“‡:
                - efficientnet_b3: å¹³è¡¡é€Ÿåº¦èˆ‡æº–ç¢ºç‡ (12Måƒæ•¸) [ç•¶å‰]
                - efficientnet_b4: æ›´é«˜æº–ç¢ºç‡ (19Måƒæ•¸)
                
                ğŸ¯ æŠ—éæ“¬åˆ (æ¨è–¦è§£æ±ºKaggleå•é¡Œ):
                - efficientnetv2_s: V2å°ç‰ˆ (21Måƒæ•¸)
                - efficientnetv2_m: V2ä¸­ç‰ˆ (54Måƒæ•¸)
                
                ğŸ† æ¥µè‡´æ€§èƒ½:
                - efficientnet_b5: é«˜æº–ç¢ºç‡ (30Måƒæ•¸)
                - convnext_tiny: ConvNeXt Tiny (28Måƒæ•¸)
                - convnext_base: ConvNeXt Base (89Måƒæ•¸)
            device: è¨ˆç®—è¨­å‚™
        """
        self.num_classes = num_classes
        self.model_name = model_name
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        print(f"ğŸ¯ åˆå§‹åŒ– {model_name} åˆ†é¡å™¨")
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹ (é»˜èªæ¨™æº–æ¨¡å¼ï¼Œå¯åœ¨è¨“ç·´æ™‚å•Ÿç”¨æŠ—éæ“¬åˆ)
        self.model = self._create_model()
        self.class_to_idx = {}
        self.anti_overfitting_mode = False
        
    def _create_model(self, anti_overfitting=False):
        """
        å‰µå»º EfficientNet æ¨¡å‹
        
        Args:
            anti_overfitting: æ˜¯å¦ä½¿ç”¨æŠ—éæ“¬åˆè¨­ç½®
        """
        try:
            # æ ¹æ“šæ˜¯å¦æŠ—éæ“¬åˆèª¿æ•´åƒæ•¸
            if anti_overfitting:
                drop_rate = 0.4      # å¢åŠ dropout: 0.2 â†’ 0.4
                drop_path_rate = 0.4 # å¢åŠ drop_path: 0.2 â†’ 0.4
                print(f"ğŸ›¡ï¸ ä½¿ç”¨æŠ—éæ“¬åˆæ¨¡å¼ (dropout: {drop_rate}, drop_path: {drop_path_rate})")
            else:
                drop_rate = 0.2
                drop_path_rate = 0.2
                print(f"âš¡ ä½¿ç”¨æ¨™æº–æ¨¡å¼ (dropout: {drop_rate}, drop_path: {drop_path_rate})")
            
            # ä½¿ç”¨ timm è¼‰å…¥é è¨“ç·´æ¨¡å‹
            try:
                model = timm.create_model(
                    self.model_name,
                    pretrained=True,
                    num_classes=self.num_classes,
                    drop_rate=drop_rate,
                    drop_path_rate=drop_path_rate
                )
            except RuntimeError as e:
                if "No pretrained weights exist" in str(e):
                    print(f"âš ï¸ {self.model_name} æ²’æœ‰é è¨“ç·´æ¬Šé‡ï¼Œå˜—è©¦æ›¿ä»£æ–¹æ¡ˆ...")
                    
                    # æä¾›æ›¿ä»£æ–¹æ¡ˆ
                    alternative_models = {
                        'efficientnetv2_l': 'efficientnetv2_m.in21k_ft_in1k',
                        'efficientnetv2_xl': 'efficientnetv2_m.in21k_ft_in1k',
                        'convnext_base.fb_in22k_ft_in1k': 'convnext_base',
                        'convnext_large': 'convnext_base',
                    }
                    
                    if self.model_name in alternative_models:
                        alt_model = alternative_models[self.model_name]
                        print(f"ğŸ”„ ä½¿ç”¨æ›¿ä»£æ¨¡å‹: {alt_model}")
                        model = timm.create_model(
                            alt_model,
                            pretrained=True,
                            num_classes=self.num_classes,
                            drop_rate=drop_rate,
                            drop_path_rate=drop_path_rate
                        )
                        self.model_name = alt_model  # æ›´æ–°æ¨¡å‹åç¨±
                    else:
                        raise e
                else:
                    raise e
            
            # è¨ˆç®—æ¨¡å‹åƒæ•¸
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            print(f"ğŸ“Š æ¨¡å‹çµ±è¨ˆ:")
            print(f"   ç¸½åƒæ•¸: {total_params/1e6:.1f}M")
            print(f"   å¯è¨“ç·´åƒæ•¸: {trainable_params/1e6:.1f}M")
            
            return model.to(self.device)
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹å‰µå»ºå¤±æ•—: {e}")
            raise
    
    def get_transforms(self, is_training=True):
        """
        ç²å–è³‡æ–™è®Šæ›
        
        Args:
            is_training: æ˜¯å¦ç‚ºè¨“ç·´æ¨¡å¼
        """
        if is_training:
            # è¨“ç·´æ™‚çš„è³‡æ–™å¢å¼·
            return transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomRotation(degrees=15),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            # é©—è­‰/æ¸¬è©¦æ™‚çš„è®Šæ›
            return transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
    
    def prepare_data(self, data_paths):
        """
        æº–å‚™è³‡æ–™é›† - ä½¿ç”¨å’Œ MemoryViT ç›¸åŒçš„é…ç½®
        
        Args:
            data_paths: åŒ…å« train, val è·¯å¾‘çš„å­—å…¸
        """
        print("\nğŸ“Š æº–å‚™è³‡æ–™...")
        
        # ç¢ºèªè³‡æ–™è·¯å¾‘
        train_path = data_paths['train']
        val_path = data_paths['val']
        
        print(f"ğŸ“ è¼‰å…¥è¨“ç·´è³‡æ–™: {train_path}")
        print(f"ğŸ“ è¼‰å…¥é©—è­‰è³‡æ–™: {val_path}")
        
        # è¨“ç·´è³‡æ–™
        train_transform = self.get_transforms(is_training=True)
        train_dataset = datasets.ImageFolder(
            root=train_path,
            transform=train_transform
        )
        
        # é©—è­‰è³‡æ–™
        val_transform = self.get_transforms(is_training=False)
        val_dataset = datasets.ImageFolder(
            root=val_path,
            transform=val_transform
        )
        
        # å»ºç«‹çµ±ä¸€çš„é¡åˆ¥æ˜ å°„ï¼ˆä½¿ç”¨è¨“ç·´é›†çš„æ˜ å°„ï¼‰
        self.class_to_idx = train_dataset.class_to_idx
        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}
        
        # æª¢æŸ¥é¡åˆ¥ä¸€è‡´æ€§
        if val_dataset.class_to_idx != self.class_to_idx:
            print("âš ï¸ è­¦å‘Šï¼šè¨“ç·´é›†å’Œé©—è­‰é›†çš„é¡åˆ¥æ˜ å°„ä¸å®Œå…¨ä¸€è‡´")
            print(f"   è¨“ç·´é›†é¡åˆ¥æ•¸: {len(self.class_to_idx)}")
            print(f"   é©—è­‰é›†é¡åˆ¥æ•¸: {len(val_dataset.class_to_idx)}")
            
            # ä½¿ç”¨è¨“ç·´é›†çš„é¡åˆ¥æ˜ å°„é‡æ–°æ•´ç†é©—è­‰é›†
            val_dataset.class_to_idx = self.class_to_idx
        
        print(f"âœ… è¨“ç·´é›†: {len(train_dataset)} å¼µåœ–ç‰‡")
        print(f"âœ… é©—è­‰é›†: {len(val_dataset)} å¼µåœ–ç‰‡")
        print(f"ğŸ“ é¡åˆ¥æ•¸: {len(self.class_to_idx)}")
        
        # æ›´æ–°æ¨¡å‹çš„é¡åˆ¥æ•¸ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if len(self.class_to_idx) != self.num_classes:
            print(f"âš ï¸ æ›´æ–°é¡åˆ¥æ•¸ï¼š{self.num_classes} â†’ {len(self.class_to_idx)}")
            self.num_classes = len(self.class_to_idx)
            # é‡æ–°å‰µå»ºæ¨¡å‹ä»¥åŒ¹é…æ–°çš„é¡åˆ¥æ•¸
            self.model = self._create_model()
        
        # é¡¯ç¤ºé¡åˆ¥è³‡è¨Š
        print(f"ğŸ“‹ å‰10å€‹é¡åˆ¥: {list(self.class_to_idx.keys())[:10]}")
        
        # æ¸¬è©¦è³‡æ–™é›†ï¼ˆå¾é©—è­‰é›†åˆ†å‡ºä¸€éƒ¨åˆ†ï¼Œæˆ–è€…æ²’æœ‰æ¸¬è©¦é›†ï¼‰
        test_dataset = None
        
        return train_dataset, val_dataset, test_dataset
    
    def find_optimal_batch_size(self, train_dataset, start_size=16, max_size=128):
        """
        å¿«é€Ÿæ‰¾åˆ°æœ€ä½³ batch size
        
        Args:
            train_dataset: è¨“ç·´è³‡æ–™é›†
            start_size: èµ·å§‹ batch size
            max_size: æœ€å¤§ batch size
        """
        print(f"\nâš™ï¸ å°‹æ‰¾æœ€ä½³ batch size (ç¯„åœ: {start_size}-{max_size})")
        
        # æ¸¬è©¦ä¸åŒ batch size
        batch_sizes = [32, 64, 128]
        if torch.cuda.is_available():
            batch_sizes = [16, 32, 64, 128]
        
        best_batch_size = 16
        best_speed = 0
        
        for batch_size in batch_sizes:
            try:
                # æ¸¬è©¦é€™å€‹ batch size
                test_loader = DataLoader(
                    train_dataset, 
                    batch_size=batch_size, 
                    shuffle=True, 
                    num_workers=2,
                    pin_memory=True if torch.cuda.is_available() else False
                )
                
                # æ¸¬è©¦é€Ÿåº¦
                self.model.train()
                start_time = time.time()
                
                for i, (images, labels) in enumerate(test_loader):
                    if i >= 5:  # åªæ¸¬è©¦ 5 å€‹ batch
                        break
                    
                    images = images.to(self.device)
                    labels = labels.to(self.device)
                    
                    with torch.no_grad():
                        outputs = self.model(images)
                
                elapsed = time.time() - start_time
                speed = 5 * batch_size / elapsed  # æ¯ç§’è™•ç†åœ–ç‰‡æ•¸
                
                print(f"   Batch {batch_size}: {speed:.1f} åœ–ç‰‡/ç§’")
                
                if speed > best_speed:
                    best_speed = speed
                    best_batch_size = batch_size
                    
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"   Batch {batch_size}: âŒ GPU è¨˜æ†¶é«”ä¸è¶³")
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    break
                else:
                    raise e
        
        print(f"âœ… é¸æ“‡ batch size: {best_batch_size} (é€Ÿåº¦: {best_speed:.1f} åœ–ç‰‡/ç§’)")
        return best_batch_size
    
    def train(self, train_dataset, val_dataset=None, 
              batch_size=None, epochs=30, lr=3e-5, 
              auto_batch_size=True, patience=10, resume_from=None):
        """
        è¨“ç·´æ¨¡å‹ (æ”¯æ´æ–·é»çºŒè¨“)
        
        Args:
            train_dataset: è¨“ç·´è³‡æ–™é›†
            val_dataset: é©—è­‰è³‡æ–™é›†
            batch_size: batch size (None è¡¨ç¤ºè‡ªå‹•æª¢æ¸¬)
            epochs: è¨“ç·´è¼ªæ•¸
            lr: å­¸ç¿’ç‡
            auto_batch_size: æ˜¯å¦è‡ªå‹•æª¢æ¸¬ batch size
            patience: æ—©åœè€å¿ƒå€¼
            resume_from: æ–·é»çºŒè¨“æª”æ¡ˆè·¯å¾‘
        """
        # æª¢æŸ¥æ˜¯å¦å¾æª¢æŸ¥é»æ¢å¾©
        start_epoch = 0
        resume_info = None
        
        if resume_from and os.path.exists(resume_from):
            print(f"ğŸ”„ å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´: {resume_from}")
            resume_info = self.load_model(resume_from, load_for_training=True)
            start_epoch = resume_info.get('epoch', 0)
            
            print(f"âœ… å°‡å¾ç¬¬ {start_epoch + 1} è¼ªç¹¼çºŒè¨“ç·´")
        
        print(f"\nğŸš€ {'ç¹¼çºŒ' if resume_from else 'é–‹å§‹'}è¨“ç·´ {self.model_name}...")
        
        # è‡ªå‹•æª¢æ¸¬ batch size
        if auto_batch_size or batch_size is None:
            batch_size = self.find_optimal_batch_size(train_dataset)
        
        # æº–å‚™è³‡æ–™è¼‰å…¥å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True if torch.cuda.is_available() else False,
            drop_last=True
        )
        
        val_loader = None
        if val_dataset:
            val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=2,
                pin_memory=True if torch.cuda.is_available() else False
            )
        
        # æ ¹æ“šæŠ—éæ“¬åˆæ¨¡å¼èª¿æ•´æ¬Šé‡è¡°æ¸›
        weight_decay = 0.02 if self.anti_overfitting_mode else 0.01
        
        # è¨­å®šå„ªåŒ–å™¨å’Œæ’ç¨‹å™¨
        optimizer = optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999)
        )
        print(f"âš™ï¸ æ¬Šé‡è¡°æ¸›: {weight_decay}")
        
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=epochs, eta_min=lr/100
        )
        
        # æ¢å¾©å„ªåŒ–å™¨å’Œèª¿åº¦å™¨ç‹€æ…‹
        if resume_info:
            if resume_info.get('optimizer_state'):
                optimizer.load_state_dict(resume_info['optimizer_state'])
                print("âœ… æ¢å¾©å„ªåŒ–å™¨ç‹€æ…‹")
            
            if resume_info.get('scheduler_state'):
                scheduler.load_state_dict(resume_info['scheduler_state'])
                print("âœ… æ¢å¾©å­¸ç¿’ç‡èª¿åº¦å™¨ç‹€æ…‹")
        
        # æ ¹æ“šæŠ—éæ“¬åˆæ¨¡å¼èª¿æ•´Label Smoothing
        label_smoothing = 0.15 if self.anti_overfitting_mode else 0.1
        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
        print(f"ğŸ¯ ä½¿ç”¨Label Smoothing: {label_smoothing}")
        
        # è¨“ç·´æ­·å² (æ”¯æ´æ–·é»çºŒè¨“)
        if resume_info and 'history' in resume_info:
            history = resume_info['history']
            best_val_acc = resume_info.get('best_val_acc', 0.0)
            print(f"ğŸ“Š æ¢å¾©è¨“ç·´æ­·å²ï¼Œå·²æœ‰ {len(history['train_loss'])} è¼ªè¨˜éŒ„")
        else:
            history = {
                'train_loss': [],
                'train_acc': [],
                'val_loss': [],
                'val_acc': [],
                'lr': []
            }
            best_val_acc = 0.0
        
        patience_counter = 0
        
        print(f"ğŸ“Š è¨“ç·´è¨­å®š:")
        print(f"   Batch size: {batch_size}")
        print(f"   å­¸ç¿’ç‡: {lr}")
        print(f"   è¨“ç·´è¼ªæ•¸: {epochs}")
        print(f"   æ—©åœè€å¿ƒ: {patience}")
        
        # é–‹å§‹è¨“ç·´ (æ”¯æ´æ–·é»çºŒè¨“)
        for epoch in range(start_epoch, epochs):
            start_time = time.time()
            
            # è¨“ç·´éšæ®µ
            train_loss, train_acc = self._train_epoch(train_loader, optimizer, criterion)
            
            # é©—è­‰éšæ®µ
            val_loss, val_acc = 0.0, 0.0
            if val_loader:
                val_loss, val_acc = self._validate_epoch(val_loader, criterion)
            
            # æ›´æ–°å­¸ç¿’ç‡
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            
            # è¨˜éŒ„æ­·å²
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            history['lr'].append(current_lr)
            
            # è¨ˆç®—æ™‚é–“
            epoch_time = time.time() - start_time
            
            # é¡¯ç¤ºé€²åº¦
            print(f"Epoch [{epoch+1}/{epochs}] ({epoch_time:.1f}s)")
            print(f"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%")
            if val_loader:
                print(f"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%")
            print(f"  LR: {current_lr:.2e}")
            
            # å®šæœŸä¿å­˜æª¢æŸ¥é» (æ¯5å€‹epochä¿å­˜ä¸€æ¬¡)
            if (epoch + 1) % 5 == 0:
                checkpoint_filename = f"{self.model_name}_checkpoint_epoch_{epoch+1:03d}_acc_{val_acc:.2f}.pth"
                self.save_model(
                    checkpoint_filename,
                    epoch=epoch,
                    optimizer=optimizer,
                    scheduler=scheduler,
                    val_acc=val_acc,
                    train_acc=train_acc,
                    best_val_acc=best_val_acc,
                    history=history
                )
                print(f"ğŸ“¦ æª¢æŸ¥é»å·²ä¿å­˜: {checkpoint_filename}")
            
            # æ—©åœæª¢æŸ¥
            if val_loader:
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹ (æ–°çš„å‘½åæ ¼å¼: epoch_XX_acc_XX.X)
                    filename = f"{self.model_name}_epoch_{epoch+1:03d}_acc_{val_acc:.2f}.pth"
                    self.save_model(
                        filename,
                        epoch=epoch,
                        optimizer=optimizer,
                        scheduler=scheduler,
                        val_acc=val_acc,
                        train_acc=train_acc,
                        best_val_acc=best_val_acc,
                        history=history
                    )
                else:
                    patience_counter += 1
                    
                if patience_counter >= patience:
                    print(f"\nâ¹ï¸ æ—©åœè§¸ç™¼ï¼æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
                    break
            
            print("-" * 60)
        
        print(f"\nâœ… è¨“ç·´å®Œæˆï¼")
        if val_loader:
            print(f"ğŸ† æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
            
            # é¡¯ç¤ºæœ€ä½³æ¨¡å‹æª”æ¡ˆå
            best_epoch = max(range(len(history['val_acc'])), key=lambda i: history['val_acc'][i]) + start_epoch + 1
            best_filename = f"{self.model_name}_epoch_{best_epoch:03d}_acc_{best_val_acc:.2f}.pth"
            print(f"ğŸ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ç‚º: {best_filename}")
        
        return history
    
    def _train_epoch(self, train_loader, optimizer, criterion):
        """è¨“ç·´ä¸€å€‹ epoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc="Training")
        for images, labels in pbar:
            images = images.to(self.device)
            labels = labels.to(self.device)
            
            optimizer.zero_grad()
            outputs = self.model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # æ›´æ–°é€²åº¦æ¢
            acc = 100.0 * correct / total
            pbar.set_postfix({
                'Loss': f'{total_loss/len(pbar):.4f}',
                'Acc': f'{acc:.2f}%'
            })
        
        return total_loss / len(train_loader), 100.0 * correct / total
    
    def _validate_epoch(self, val_loader, criterion):
        """é©—è­‰ä¸€å€‹ epoch"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc="Validation"):
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        return total_loss / len(val_loader), 100.0 * correct / total
    
    def save_model(self, filename, epoch=None, optimizer=None, scheduler=None, 
                   val_acc=None, train_acc=None, best_val_acc=None, history=None):
        """
        ä¿å­˜æ¨¡å‹ (å¢å¼·ç‰ˆï¼Œæ”¯æ´è¨“ç·´ç‹€æ…‹)
        
        Args:
            filename: ä¿å­˜æª”æ¡ˆå
            epoch: ç•¶å‰è¨“ç·´è¼ªæ•¸
            optimizer: å„ªåŒ–å™¨ (ç”¨æ–¼æ–·é»çºŒè¨“)
            scheduler: å­¸ç¿’ç‡èª¿åº¦å™¨
            val_acc: é©—è­‰æº–ç¢ºç‡
            train_acc: è¨“ç·´æº–ç¢ºç‡
            best_val_acc: æœ€ä½³é©—è­‰æº–ç¢ºç‡
            history: è¨“ç·´æ­·å²
        """
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'model_name': self.model_name,
            'num_classes': self.num_classes,
            'class_to_idx': self.class_to_idx,
            'idx_to_class': self.idx_to_class
        }
        
        # æ·»åŠ è¨“ç·´ç‹€æ…‹ (å¦‚æœæä¾›)
        if epoch is not None:
            checkpoint['epoch'] = epoch
        if optimizer is not None:
            checkpoint['optimizer_state_dict'] = optimizer.state_dict()
        if scheduler is not None:
            checkpoint['scheduler_state_dict'] = scheduler.state_dict()
        if val_acc is not None:
            checkpoint['val_acc'] = val_acc
        if train_acc is not None:
            checkpoint['train_acc'] = train_acc
        if best_val_acc is not None:
            checkpoint['best_val_acc'] = best_val_acc
        if history is not None:
            checkpoint['history'] = history
        
        torch.save(checkpoint, filename)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {filename}")
    
    def load_model(self, filename, load_for_training=False):
        """
        è¼‰å…¥æ¨¡å‹
        
        Args:
            filename: æ¨¡å‹æª”æ¡ˆè·¯å¾‘
            load_for_training: æ˜¯å¦ç‚ºè¨“ç·´è¼‰å…¥ (æœƒè¼‰å…¥å„ªåŒ–å™¨ç‹€æ…‹ç­‰)
        """
        if not os.path.exists(filename):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {filename}")
        
        print(f"ğŸ“‚ è¼‰å…¥æ¨¡å‹: {filename}")
        checkpoint = torch.load(filename, map_location=self.device)
        
        # è¼‰å…¥æ¨¡å‹æ¬Šé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.class_to_idx = checkpoint['class_to_idx']
        self.idx_to_class = checkpoint['idx_to_class']
        
        # æ›´æ–°é¡åˆ¥æ•¸
        if len(self.class_to_idx) != self.num_classes:
            self.num_classes = len(self.class_to_idx)
        
        loaded_info = {
            'model_name': checkpoint.get('model_name', 'unknown'),
            'num_classes': len(self.class_to_idx),
            'accuracy': checkpoint.get('val_acc', 'unknown')
        }
        
        if load_for_training:
            # è¼‰å…¥è¨“ç·´ç›¸é—œè³‡è¨Š
            loaded_info.update({
                'epoch': checkpoint.get('epoch', 0),
                'best_val_acc': checkpoint.get('best_val_acc', 0),
                'train_acc': checkpoint.get('train_acc', 0),
                'optimizer_state': checkpoint.get('optimizer_state_dict'),
                'scheduler_state': checkpoint.get('scheduler_state_dict'),
                'history': checkpoint.get('history', {})
            })
            
            print(f"ï¿½ è¨“ç·´è³‡è¨Š:")
            print(f"   ä¸Šæ¬¡è¨“ç·´åˆ°ç¬¬ {loaded_info['epoch']} è¼ª")
            print(f"   æœ€ä½³é©—è­‰æº–ç¢ºç‡: {loaded_info['best_val_acc']:.2f}%")
            print(f"   ä¸Šæ¬¡è¨“ç·´æº–ç¢ºç‡: {loaded_info['train_acc']:.2f}%")
        
        print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ (æº–ç¢ºç‡: {loaded_info['accuracy']:.1f}%)")
        return loaded_info
    
    def plot_training_history(self, history):
        """ç¹ªè£½è¨“ç·´æ­·å²"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # Loss
        axes[0,0].plot(history['train_loss'], label='Train Loss')
        if history['val_loss']:
            axes[0,0].plot(history['val_loss'], label='Val Loss')
        axes[0,0].set_title('Loss')
        axes[0,0].legend()
        
        # Accuracy
        axes[0,1].plot(history['train_acc'], label='Train Acc')
        if history['val_acc']:
            axes[0,1].plot(history['val_acc'], label='Val Acc')
        axes[0,1].set_title('Accuracy (%)')
        axes[0,1].legend()
        
        # Learning Rate
        axes[1,0].plot(history['lr'])
        axes[1,0].set_title('Learning Rate')
        
        # Speed comparison
        axes[1,1].text(0.1, 0.8, f'Model: {self.model_name}', transform=axes[1,1].transAxes)
        axes[1,1].text(0.1, 0.6, f'Classes: {self.num_classes}', transform=axes[1,1].transAxes)
        axes[1,1].text(0.1, 0.4, f'Device: {self.device}', transform=axes[1,1].transAxes)
        axes[1,1].set_title('Model Info')
        axes[1,1].axis('off')
        
        plt.tight_layout()
        plt.savefig(f'{self.model_name}_training_history.png', dpi=300, bbox_inches='tight')
        plt.show()

def get_best_data_path():
    """
    ä½¿ç”¨å’Œ MemoryViT ç›¸åŒçš„è³‡æ–™è·¯å¾‘é…ç½®ï¼š
    - è¨“ç·´è³‡æ–™ï¼šaugmented/train/
    - é©—è­‰è³‡æ–™ï¼špreprocessed/val/
    """
    # æª¢æ¸¬ç’°å¢ƒ
    import platform
    is_wsl = "microsoft" in platform.uname().release.lower() or "WSL" in os.environ.get("WSL_DISTRO_NAME", "")
    
    if is_wsl:
        base_path = "/mnt/e/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset"
        augmented_train = f"{base_path}/augmented/train"
        preprocessed_val = f"{base_path}/preprocessed/val"
    else:
        base_path = "E:/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset"
        augmented_train = f"{base_path}/augmented/train"
        preprocessed_val = f"{base_path}/preprocessed/val"
    
    # æª¢æŸ¥å¿…è¦çš„è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨
    if os.path.exists(augmented_train) and os.path.exists(preprocessed_val):
        return {
            'train': augmented_train,
            'val': preprocessed_val,
            'use_existing_split': True
        }, "ğŸ¨ ä½¿ç”¨å¢å¼·è¨“ç·´è³‡æ–™ + é è™•ç†é©—è­‰è³‡æ–™"
    else:
        missing = []
        if not os.path.exists(augmented_train):
            missing.append(f"augmented/train: {augmented_train}")
        if not os.path.exists(preprocessed_val):
            missing.append(f"preprocessed/val: {preprocessed_val}")
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å¿…è¦çš„è³‡æ–™å¤¾: {', '.join(missing)}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ EfficientNet Simpson è§’è‰²åˆ†é¡å™¨")
    print("=" * 60)
    
    # æª¢æŸ¥ GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è¨­å‚™: {device}")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print(f"ğŸ® GPU: {gpu_name}")
        print(f"ğŸ’¾ GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # ä½¿ç”¨å’Œ MemoryViT ç›¸åŒçš„è³‡æ–™è·¯å¾‘
    try:
        data_paths, data_type = get_best_data_path()
        print(f"ğŸ“‚ ä½¿ç”¨è³‡æ–™: {data_type}")
        print(f"ğŸ“ è¨“ç·´è·¯å¾‘: {data_paths['train']}")
        print(f"ğŸ“ é©—è­‰è·¯å¾‘: {data_paths['val']}")
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        print("ğŸ’¡ è«‹ç¢ºèªä»¥ä¸‹è·¯å¾‘å­˜åœ¨:")
        print("  - Dataset/augmented/train (å¢å¼·è¨“ç·´è³‡æ–™)")
        print("  - Dataset/preprocessed/val (é è™•ç†é©—è­‰è³‡æ–™)")
        return
    
    # çµ±è¨ˆè³‡æ–™é‡
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif']
    total_images = 0
    
    # çµ±è¨ˆ train è³‡æ–™
    for ext in image_extensions:
        total_images += len(glob.glob(os.path.join(data_paths['train'], '**', ext), recursive=True))
    
    # çµ±è¨ˆ val è³‡æ–™
    if data_paths['val'] and os.path.exists(data_paths['val']):
        for ext in image_extensions:
            total_images += len(glob.glob(os.path.join(data_paths['val'], '**', ext), recursive=True))
    
    print(f"ğŸ“Š ç¸½åœ–ç‰‡æ•¸: {total_images} å¼µ")
    
    if total_images == 0:
        print("âŒ æ‰¾ä¸åˆ°ä»»ä½•åœ–ç‰‡æª”æ¡ˆï¼")
        return
    
    # é¸æ“‡æ¨¡å‹
    print("\nğŸ¯ é¸æ“‡ EfficientNet æ¨¡å‹:")
    print("ğŸš€ é€Ÿåº¦å„ªå…ˆ:")
    print("  1. efficientnet_b0 - æœ€å¿« (5.3Måƒæ•¸)")
    print("  2. efficientnet_b1 - å¾ˆå¿« (7.8Måƒæ•¸)")
    print("  3. efficientnet_b2 - å¿« (9.2Måƒæ•¸)")
    print("\nâš–ï¸ å¹³è¡¡é¸æ“‡:")
    print("  4. efficientnet_b3 - å¹³è¡¡ (12Måƒæ•¸) [ç•¶å‰ä½¿ç”¨]")
    print("  5. efficientnet_b4 - æ›´æº–ç¢º (19Måƒæ•¸)")
    print("\nğŸ›¡ï¸ æŠ—éæ“¬åˆ (æ¨è–¦è§£æ±ºKaggleå•é¡Œ):")
    print("  6. efficientnetv2_s.in1k - V2å°ç‰ˆ (21Måƒæ•¸)")
    print("  7. efficientnetv2_m.in21k_ft_in1k - V2ä¸­ç‰ˆ (54Måƒæ•¸) â­æ¨è–¦")
    print("\nğŸ† æ¥µè‡´æ€§èƒ½:")
    print("  8. efficientnet_b5 - é«˜æº–ç¢º (30Måƒæ•¸)")
    print("  9. convnext_tiny - ConvNeXt Tiny (28Måƒæ•¸) ğŸ”¥æ¨è–¦")
    print("  10. convnext_base - ConvNeXt Base (89Måƒæ•¸) ğŸ”¥æœ€å¼·")
    
    choice = input("è«‹é¸æ“‡ (1-10ï¼Œé è¨­4): ").strip()
    model_mapping = {
        '1': 'efficientnet_b0',
        '2': 'efficientnet_b1', 
        '3': 'efficientnet_b2',
        '4': 'efficientnet_b3',
        '5': 'efficientnet_b4',
        '6': 'efficientnetv2_s.in1k',
        '7': 'efficientnetv2_m.in21k_ft_in1k',
        '8': 'efficientnet_b5',
        '9': 'convnext_tiny',
        '10': 'convnext_base'
    }
    model_name = model_mapping.get(choice, 'efficientnet_b3')
    
    # å¦‚æœé¸æ“‡äº†V2ç‰ˆæœ¬æˆ–ConvNeXtï¼Œè‡ªå‹•å•Ÿç”¨æŠ—éæ“¬åˆæ¨¡å¼
    auto_anti_overfitting_models = ['efficientnetv2', 'convnext']
    anti_overfitting = any(model_type in model_name for model_type in auto_anti_overfitting_models)
    
    if anti_overfitting:
        if 'efficientnetv2' in model_name:
            print(f"âœ… è‡ªå‹•å•Ÿç”¨æŠ—éæ“¬åˆæ¨¡å¼ (EfficientNet V2æ¶æ§‹)")
        elif 'convnext' in model_name:
            print(f"âœ… è‡ªå‹•å•Ÿç”¨æŠ—éæ“¬åˆæ¨¡å¼ (ConvNeXtæ¶æ§‹)")
    else:
        # è©¢å•æ˜¯å¦å•Ÿç”¨æŠ—éæ“¬åˆæ¨¡å¼
        overfitting_choice = input("\nğŸ›¡ï¸ æ˜¯å¦å•Ÿç”¨æŠ—éæ“¬åˆæ¨¡å¼ï¼Ÿ(é©åˆè§£æ±ºKaggleæ¸¬è©¦é›†æº–ç¢ºç‡ä½çš„å•é¡Œ) (y/N): ").strip().lower()
        anti_overfitting = overfitting_choice in ['y', 'yes']
    
    # åˆå§‹åŒ–åˆ†é¡å™¨
    classifier = EfficientNetCharacterClassifier(
        num_classes=50,  # é è¨­50é¡ï¼Œå¯¦éš›æœƒæ ¹æ“šè³‡æ–™èª¿æ•´
        model_name=model_name,
        device=device
    )
    
    # å¦‚æœå•Ÿç”¨æŠ—éæ“¬åˆæ¨¡å¼ï¼Œé‡æ–°å‰µå»ºæ¨¡å‹
    if anti_overfitting:
        classifier.anti_overfitting_mode = True
        classifier.model = classifier._create_model(anti_overfitting=True)
        print("ğŸ›¡ï¸ æŠ—éæ“¬åˆæ¨¡å¼å·²å•Ÿç”¨")
    
    # æº–å‚™è³‡æ–™ - ä½¿ç”¨å’Œ MemoryViT ç›¸åŒçš„è³‡æ–™
    train_dataset, val_dataset, test_dataset = classifier.prepare_data(data_paths)
    
    # æª¢æŸ¥æ˜¯å¦è¦å¾æª¢æŸ¥é»æ¢å¾©
    print("\nğŸ”„ è¨“ç·´æ¨¡å¼é¸æ“‡:")
    print("1. å¾é ­é–‹å§‹è¨“ç·´ (é è¨­)")
    print("2. å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´")
    
    train_mode = input("è«‹é¸æ“‡ (1/2): ").strip()
    resume_from = None
    
    if train_mode == "2":
        # å°‹æ‰¾å¯ç”¨çš„æ¨¡å‹æª”æ¡ˆ (æ–°èˆŠæ ¼å¼éƒ½æ”¯æ´)
        model_files = []
        
        # æœå°‹æ–°æ ¼å¼æª”æ¡ˆ (efficientnet_b3_epoch_XXX_acc_XX.XX.pth)
        for f in os.listdir('.'):
            if (f.startswith('efficientnet_') and '_epoch_' in f and '_acc_' in f and f.endswith('.pth')) or \
               (f.startswith('best_efficientnet_') and f.endswith('.pth')):
                model_files.append(f)
        
        # æŒ‰æª”æ¡ˆä¿®æ”¹æ™‚é–“æ’åº (æœ€æ–°çš„åœ¨å‰é¢)
        model_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        
        if model_files:
            print("\nğŸ“ æ‰¾åˆ°ä»¥ä¸‹æ¨¡å‹æª”æ¡ˆ:")
            for i, file in enumerate(model_files, 1):
                # è§£ææª”æ¡ˆè³‡è¨Š
                file_info = ""
                try:
                    if '_epoch_' in file and '_acc_' in file:
                        # æ–°æ ¼å¼: efficientnet_b3_epoch_015_acc_98.50.pth
                        parts = file.replace('.pth', '').split('_')
                        epoch_idx = parts.index('epoch') + 1
                        acc_idx = parts.index('acc') + 1
                        epoch_num = parts[epoch_idx]
                        accuracy = parts[acc_idx]
                        file_info = f" (ç¬¬{epoch_num}è¼ª, æº–ç¢ºç‡:{accuracy}%)"
                    elif 'best_' in file and 'acc' in file:
                        # èˆŠæ ¼å¼: best_efficientnet_b3_acc98.0.pth
                        acc_part = file.split('acc')[1].replace('.pth', '')
                        file_info = f" (æº–ç¢ºç‡:{acc_part}%)"
                except:
                    pass
                
                # é¡¯ç¤ºæª”æ¡ˆä¿®æ”¹æ™‚é–“
                import time
                mod_time = os.path.getmtime(file)
                time_str = time.strftime('%m/%d %H:%M', time.localtime(mod_time))
                
                print(f"   {i}. {file}{file_info} [{time_str}]")
            
            try:
                choice = int(input(f"è«‹é¸æ“‡æ¨¡å‹æª”æ¡ˆ (1-{len(model_files)}): ")) - 1
                resume_from = model_files[choice]
                print(f"âœ… å°‡å¾ {resume_from} ç¹¼çºŒè¨“ç·´")
            except (ValueError, IndexError):
                print("âŒ é¸æ“‡ç„¡æ•ˆï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
        else:
            print("âŒ æ‰¾ä¸åˆ°å¯ç”¨çš„æ¨¡å‹æª”æ¡ˆï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
    
    # è¨“ç·´åƒæ•¸
    epochs = int(input("ç¸½è¨“ç·´è¼ªæ•¸ (é è¨­ 30): ") or "30")
    lr = float(input("å­¸ç¿’ç‡ (é è¨­ 3e-5): ") or "3e-5")
    
    # é–‹å§‹è¨“ç·´ (æ”¯æ´æ–·é»çºŒè¨“)
    history = classifier.train(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        epochs=epochs,
        lr=lr,
        auto_batch_size=True,
        patience=10,
        resume_from=resume_from
    )
    
    # ç¹ªè£½çµæœ
    classifier.plot_training_history(history)
    
    print("\nğŸ‰ è¨“ç·´å®Œæˆï¼")

if __name__ == "__main__":
    main()