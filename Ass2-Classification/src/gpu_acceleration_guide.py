#!/usr/bin/env python3
"""
GPU åŠ é€Ÿè³‡æ–™å¢å¼·ä½¿ç”¨ç¯„ä¾‹
"""

print("ğŸš€ GPU åŠ é€Ÿè³‡æ–™å¢å¼·ä½¿ç”¨æŒ‡å—")
print("=" * 60)

print("""
ğŸ¯ åŸºæœ¬ä½¿ç”¨ (è‡ªå‹•ä½¿ç”¨ GPU)ï¼š
   python data_augmentation.py

ğŸ¯ è‡ªè¨‚ GPU æ‰¹é‡å¤§å°ï¼š
   python data_augmentation.py --batch_size 64

ğŸ¯ å¼·åˆ¶ä½¿ç”¨ CPU (ä¸ç”¨ GPU)ï¼š
   python data_augmentation.py --no_gpu

ğŸ¯ å®Œæ•´è‡ªè¨‚ç¯„ä¾‹ï¼š
   python data_augmentation.py \\
       --augment_per_image 5 \\
       --batch_size 16 \\
       --max_bg_per_category 20 \\
       --background_prob 0.5

ğŸ’¡ GPU åŠ é€Ÿå„ªå‹¢ï¼š
   âœ… é€Ÿåº¦æå‡ï¼š2-5å€åŠ é€Ÿ (å–æ±ºæ–¼ GPU å‹è™Ÿ)
   âœ… æ‰¹é‡è™•ç†ï¼šåŒæ™‚è™•ç†å¤šå¼µåœ–ç‰‡
   âœ… è¨˜æ†¶é«”å„ªåŒ–ï¼šè‡ªå‹•æ¸…ç† GPU è¨˜æ†¶é«”
   âœ… è‡ªå‹•å›é€€ï¼šGPU ä¸å¯ç”¨æ™‚è‡ªå‹•ç”¨ CPU

âš ï¸ æ³¨æ„äº‹é …ï¼š
   - GPU è¨˜æ†¶é«”ä¸è¶³æ™‚æœƒè‡ªå‹•é™ä½æ‰¹é‡å¤§å°
   - æŸäº›å¢å¼·æ“ä½œä»éœ€åœ¨ CPU ä¸Šå®Œæˆ
   - èƒŒæ™¯åˆæˆå› ç‚ºæ¶‰åŠè¤‡é›œé‚è¼¯ï¼Œä»åœ¨ CPU ä¸Šè™•ç†

ğŸ“Š æ€§èƒ½æ¯”è¼ƒ (åƒè€ƒæ•¸å€¼)ï¼š
   CPU æ¨¡å¼ï¼š     ~50-100 åœ–ç‰‡/åˆ†é˜
   GPU æ¨¡å¼ï¼š     ~200-500 åœ–ç‰‡/åˆ†é˜ (å–æ±ºæ–¼ GPU)
   
ğŸ® æ¨è–¦ GPU è¨­å®šï¼š
   RTX 3060/4060ï¼š   batch_size=16-32
   RTX 3070/4070ï¼š   batch_size=32-64  
   RTX 3080/4080ï¼š   batch_size=64-128
   RTX 3090/4090ï¼š   batch_size=128+
""")

# æª¢æŸ¥ GPU å¯ç”¨æ€§
import torch

if torch.cuda.is_available():
    print(f"âœ… æª¢æ¸¬åˆ° GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # æ¨è–¦æ‰¹é‡å¤§å°
    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory_gb >= 24:
        recommended_batch = 128
    elif gpu_memory_gb >= 12:
        recommended_batch = 64
    elif gpu_memory_gb >= 8:
        recommended_batch = 32
    elif gpu_memory_gb >= 6:
        recommended_batch = 16
    else:
        recommended_batch = 8
    
    print(f"ğŸ’¡ æ¨è–¦æ‰¹é‡å¤§å°: {recommended_batch}")
    print(f"\nğŸš€ ç«‹å³é–‹å§‹ GPU åŠ é€Ÿå¢å¼·ï¼š")
    print(f"   python data_augmentation.py --batch_size {recommended_batch}")
else:
    print("âŒ æœªæª¢æ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼")
    print("ğŸ’¡ ç¢ºèªäº‹é …ï¼š")
    print("   1. å·²å®‰è£ CUDA ç‰ˆæœ¬çš„ PyTorch")
    print("   2. NVIDIA é©…å‹•ç¨‹å¼å·²æ›´æ–°")
    print("   3. CUDA å·¥å…·åŒ…å·²å®‰è£")