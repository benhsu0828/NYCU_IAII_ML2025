#!/usr/bin/env python3
"""
ğŸ”® CoCa (Contrastive Captioners) ç‰¹å¾µæå– + è‡ªå®šç¾©åˆ†é¡é ­

CoCa æ˜¯ä¸€å€‹å¼·å¤§çš„å¤šæ¨¡æ…‹æ¨¡å‹ï¼Œçµåˆäº†ï¼š
- å°æ¯”å­¸ç¿’ (Contrastive Learning)
- åœ–åƒæè¿°ç”Ÿæˆ (Image Captioning)
- å„ªç§€çš„è¦–è¦ºç‰¹å¾µæå–èƒ½åŠ›

é€™å€‹åˆ†é¡å™¨ä½¿ç”¨ CoCa ä½œç‚ºç‰¹å¾µæå–å™¨ï¼Œç„¶å¾Œè¨“ç·´è‡ªå®šç¾©çš„åˆ†é¡é ­ã€‚
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from torchvision import datasets

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os
import json
from datetime import datetime
import time
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# TensorBoard æ”¯æ´
try:
    from torch.utils.tensorboard import SummaryWriter
    print("âœ… TensorBoard å·²å®‰è£")
except ImportError:
    print("âŒ éœ€è¦å®‰è£ tensorboard:")
    print("pip install tensorboard")
    SummaryWriter = None

# å®‰è£å’Œå°å…¥ open_clip (åŒ…å« CoCa)
try:
    import open_clip
    print("âœ… open_clip å·²å®‰è£")
except ImportError:
    print("âŒ éœ€è¦å®‰è£ open_clip:")
    print("pip install open-clip-torch")
    raise ImportError("è«‹å…ˆå®‰è£ open_clip: pip install open-clip-torch")

class CoCaCharacterClassifier:
    """
    åŸºæ–¼ CoCa çš„è¾›æ™®æ£®è§’è‰²åˆ†é¡å™¨
    
    ç‰¹é»ï¼š
    - ä½¿ç”¨ CoCa ä½œç‚ºå¼·å¤§çš„è¦–è¦ºç‰¹å¾µæå–å™¨
    - å‡çµ CoCa æ¬Šé‡ï¼Œåªè¨“ç·´åˆ†é¡é ­
    - æ”¯æ´å¤šç¨® CoCa æ¨¡å‹ç‰ˆæœ¬
    - é«˜æ•ˆçš„é·ç§»å­¸ç¿’
    """
    
    def __init__(self, num_classes=50, coca_model='coca_ViT-B-32', device=None):
        """
        åˆå§‹åŒ– CoCa åˆ†é¡å™¨
        
        Args:
            num_classes: åˆ†é¡é¡åˆ¥æ•¸
            coca_model: CoCa æ¨¡å‹ç‰ˆæœ¬ ('coca_ViT-B-32', 'coca_ViT-L-14')
            device: è¨ˆç®—è¨­å‚™
        """
        self.num_classes = num_classes
        self.coca_model_name = coca_model
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        print(f"ğŸ”® CoCa è¾›æ™®æ£®è§’è‰²åˆ†é¡å™¨")
        print(f"ğŸ¯ æ¨¡å‹: {coca_model}")
        print(f"ğŸ“Š é¡åˆ¥æ•¸: {num_classes}")
        print(f"ğŸ–¥ï¸ è¨­å‚™: {self.device}")
        
        # è¼‰å…¥ CoCa æ¨¡å‹
        self.coca_model, self.preprocess = self._load_coca_model(partial_unfreeze=True)
        
        # å‰µå»ºåˆ†é¡é ­
        self.classifier_head = self._create_classification_head()
        
        # å®Œæ•´æ¨¡å‹
        self.model = CoCaClassifier(self.coca_model, self.classifier_head)
        self.model.to(self.device)
        
        # é¡åˆ¥æ˜ å°„
        self.class_to_idx = {}
        self.idx_to_class = {}
        
        print("âœ… CoCa åˆ†é¡å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_checkpoint(self, checkpoint_path, load_for_training=True):
        """
        è¼‰å…¥æª¢æŸ¥é» (æ”¯æ´æ–·é»çºŒè¨“)
        
        Args:
            checkpoint_path: æª¢æŸ¥é»æ–‡ä»¶è·¯å¾‘
            load_for_training: æ˜¯å¦è¼‰å…¥è¨“ç·´ç‹€æ…‹ (å„ªåŒ–å™¨ã€èª¿åº¦å™¨ç­‰)
            
        Returns:
            dict: åŒ…å«è¼‰å…¥ä¿¡æ¯çš„å­—å…¸
        """
        print(f"ğŸ“‚ è¼‰å…¥æª¢æŸ¥é»: {checkpoint_path}")
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æª¢æŸ¥é»æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # è¼‰å…¥æ¨¡å‹ç‹€æ…‹
            self.model.load_state_dict(checkpoint['model_state_dict'])
            
            # è¼‰å…¥é¡åˆ¥æ˜ å°„
            self.class_to_idx = checkpoint['class_to_idx']
            self.idx_to_class = checkpoint['idx_to_class']
            
            print("âœ… æ¨¡å‹ç‹€æ…‹è¼‰å…¥æˆåŠŸ")
            print(f"ğŸ“Š æ¨¡å‹è¨“ç·´åˆ°ç¬¬ {checkpoint['epoch'] + 1} è¼ª")
            print(f"ğŸ¯ æœ€ä½³æº–ç¢ºç‡: {checkpoint['accuracy']:.2f}%")
            print(f"ğŸ·ï¸ é¡åˆ¥æ•¸: {len(self.class_to_idx)}")
            
            load_info = {
                'epoch': checkpoint['epoch'],
                'accuracy': checkpoint['accuracy'],
                'history': checkpoint.get('history', {}),
                'optimizer_state': checkpoint.get('optimizer_state_dict'),
                'scheduler_state': checkpoint.get('scheduler_state_dict')
            }
            
            return load_info
            
        except Exception as e:
            raise RuntimeError(f"è¼‰å…¥æª¢æŸ¥é»å¤±æ•—: {e}")
    
    def _load_coca_model(self, partial_unfreeze=True):
        """è¼‰å…¥é è¨“ç·´çš„ CoCa æ¨¡å‹ (æ”¯æ´éƒ¨åˆ†è§£å‡)"""
        print(f"ğŸ”„ è¼‰å…¥ CoCa æ¨¡å‹: {self.coca_model_name}")
        
        try:
            # è¼‰å…¥ CoCa æ¨¡å‹å’Œé è™•ç†
            model, _, preprocess = open_clip.create_model_and_transforms(
                self.coca_model_name, 
                pretrained='laion2b_s13b_b90k'  # ä½¿ç”¨ LAION-2B é è¨“ç·´æ¬Šé‡
            )
            
            print("âœ… CoCa æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            
            # æ™ºèƒ½å‡çµç­–ç•¥
            if partial_unfreeze:
                # å…ˆå…¨éƒ¨å‡çµ
                for param in model.parameters():
                    param.requires_grad = False
                
                # éƒ¨åˆ†è§£å‡æœ€å¾Œå¹¾å±¤
                unfrozen_layers = 0
                for name, param in model.named_parameters():
                    # è§£å‡è¦–è¦ºè®Šæ›å™¨çš„æœ€å¾Œ3å±¤
                    if any(layer in name for layer in [
                        'visual.transformer.resblocks.11',
                        'visual.transformer.resblocks.10', 
                        'visual.transformer.resblocks.9',
                        'visual.ln_post',  # æœ€å¾Œçš„ layer norm
                        'visual.proj'      # è¦–è¦ºæŠ•å½±å±¤
                    ]):
                        param.requires_grad = True
                        unfrozen_layers += 1
                        print(f"ğŸ”“ è§£å‡å±¤: {name}")
                
                print(f"â„ï¸ CoCa éƒ¨åˆ†å‡çµ ({unfrozen_layers} å±¤å¯è¨“ç·´)")
                
            else:
                # å®Œå…¨å‡çµ
                for param in model.parameters():
                    param.requires_grad = False
                print("â„ï¸ CoCa ç‰¹å¾µæå–å™¨å®Œå…¨å‡çµ")
            
            # ç²å–ç‰¹å¾µç¶­åº¦
            dummy_input = torch.randn(1, 3, 224, 224)
            with torch.no_grad():
                features = model.encode_image(dummy_input)
                self.feature_dim = features.shape[-1]
            
            print(f"ğŸ“ ç‰¹å¾µç¶­åº¦: {self.feature_dim}")
            
            return model, preprocess
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥ CoCa æ¨¡å‹å¤±æ•—: {e}")
            print("ğŸ’¡ å˜—è©¦ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹...")
            
            # å‚™é¸ï¼šä½¿ç”¨è¼ƒå°çš„ CLIP æ¨¡å‹
            try:
                model, _, preprocess = open_clip.create_model_and_transforms(
                    'ViT-B-32', 
                    pretrained='openai'
                )
                print("âœ… æ”¹ç”¨ CLIP ViT-B-32 æ¨¡å‹")
                
                # å‡çµåƒæ•¸
                for param in model.parameters():
                    param.requires_grad = False
                
                # ç²å–ç‰¹å¾µç¶­åº¦
                dummy_input = torch.randn(1, 3, 224, 224)
                with torch.no_grad():
                    features = model.encode_image(dummy_input)
                    self.feature_dim = features.shape[-1]
                
                print(f"ğŸ“ ç‰¹å¾µç¶­åº¦: {self.feature_dim}")
                
                return model, preprocess
                
            except Exception as e2:
                raise RuntimeError(f"ç„¡æ³•è¼‰å…¥ä»»ä½•è¦–è¦ºæ¨¡å‹: {e2}")
    
    def _create_classification_head(self):
        """å‰µå»ºåˆ†é¡é ­"""
        print(f"ğŸ—ï¸ å‰µå»ºåˆ†é¡é ­: {self.feature_dim} â†’ {self.num_classes}")
        
        # å¤šå±¤åˆ†é¡é ­ï¼Œæé«˜è¡¨é”èƒ½åŠ›
        classifier = nn.Sequential(
            nn.Linear(self.feature_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(1024, 512),
            nn.ReLU(), 
            nn.Dropout(0.2),
            
            nn.Linear(512, self.num_classes)
        )
        
        # çµ±è¨ˆåˆ†é¡é ­åƒæ•¸
        classifier_params = sum(p.numel() for p in classifier.parameters())
        print(f"ğŸ¯ åˆ†é¡é ­åƒæ•¸: {classifier_params:,} ({classifier_params/1e6:.2f}M)")
        
        return classifier
    
    def get_transforms(self, is_training=True):
        """
        ç²å–è³‡æ–™é è™•ç†è®Šæ›
        ä½¿ç”¨ CoCa çš„æ¨™æº–é è™•ç†ï¼Œä¸¦æ·»åŠ è¨“ç·´æ™‚çš„å¢å¼·
        """
        if is_training:
            # è¨“ç·´æ™‚æ·»åŠ ä¸€äº›å¢å¼·ï¼Œä½†ä¿æŒèˆ‡ CoCa é è™•ç†çš„ç›¸å®¹æ€§
            transform = transforms.Compose([
                transforms.Resize((224, 224)),  # CoCa æ¨™æº–è¼¸å…¥å°ºå¯¸
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.48145466, 0.4578275, 0.40821073],  # CLIP/CoCa æ¨™æº–
                    std=[0.26862954, 0.26130258, 0.27577711]
                )
            ])
        else:
            # é©—è­‰æ™‚ä½¿ç”¨æ¨™æº–é è™•ç†
            transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.48145466, 0.4578275, 0.40821073],
                    std=[0.26862954, 0.26130258, 0.27577711]
                )
            ])
        
        return transform
    
    def prepare_data(self, data_paths):
        """
        æº–å‚™è³‡æ–™é›† - ç›´æ¥è®€å–ç¾æœ‰è³‡æ–™ï¼Œä¸é€²è¡Œ data aggregation
        
        Args:
            data_paths: åŒ…å« train, val è·¯å¾‘çš„å­—å…¸
        """
        print("\nğŸ“Š æº–å‚™è³‡æ–™...")
        
        # ç¢ºèªè³‡æ–™è·¯å¾‘
        train_path = data_paths['train']
        val_path = data_paths['val']
        
        print(f"ğŸ“ è¼‰å…¥è¨“ç·´è³‡æ–™: {train_path}")
        print(f"ğŸ“ è¼‰å…¥é©—è­‰è³‡æ–™: {val_path}")
        
        # æª¢æŸ¥è·¯å¾‘æ˜¯å¦å­˜åœ¨
        if not os.path.exists(train_path):
            raise FileNotFoundError(f"è¨“ç·´è³‡æ–™è·¯å¾‘ä¸å­˜åœ¨: {train_path}")
        if not os.path.exists(val_path):
            raise FileNotFoundError(f"é©—è­‰è³‡æ–™è·¯å¾‘ä¸å­˜åœ¨: {val_path}")
        
        # è¨“ç·´è³‡æ–™
        train_transform = self.get_transforms(is_training=True)
        train_dataset = datasets.ImageFolder(
            root=train_path,
            transform=train_transform
        )
        
        # é©—è­‰è³‡æ–™
        val_transform = self.get_transforms(is_training=False)
        val_dataset = datasets.ImageFolder(
            root=val_path,
            transform=val_transform
        )
        
        # å»ºç«‹çµ±ä¸€çš„é¡åˆ¥æ˜ å°„
        self.class_to_idx = train_dataset.class_to_idx
        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}
        
        # é¡¯ç¤ºè³‡æ–™é›†çµ±è¨ˆ
        print(f"\nğŸ“ˆ è³‡æ–™é›†çµ±è¨ˆ:")
        print(f"   è¨“ç·´æ¨£æœ¬: {len(train_dataset):,}")
        print(f"   é©—è­‰æ¨£æœ¬: {len(val_dataset):,}")
        print(f"   é¡åˆ¥æ•¸é‡: {len(self.class_to_idx)}")
        
        # é¡¯ç¤ºéƒ¨åˆ†é¡åˆ¥
        class_names = list(self.class_to_idx.keys())
        if len(class_names) <= 10:
            print(f"   é¡åˆ¥åˆ—è¡¨: {', '.join(class_names)}")
        else:
            print(f"   é¡åˆ¥ç¯„ä¾‹: {', '.join(class_names[:5])} ... (+{len(class_names)-5} æ›´å¤š)")
        
        return train_dataset, val_dataset
    
    def train(self, train_dataset, val_dataset=None, 
              batch_size=32, epochs=50, lr=3e-5, 
              patience=10, save_dir='models', resume_from=None, use_tensorboard=True):
        """
        è¨“ç·´åˆ†é¡å™¨ (æ”¯æ´ TensorBoard + æ–·é»çºŒè¨“)
        
        Args:
            train_dataset: è¨“ç·´è³‡æ–™é›†
            val_dataset: é©—è­‰è³‡æ–™é›†
            batch_size: æ‰¹æ¬¡å¤§å°
            epochs: è¨“ç·´è¼ªæ•¸
            lr: å­¸ç¿’ç‡
            patience: æ—©åœè€å¿ƒå€¼
            save_dir: æ¨¡å‹ä¿å­˜ç›®éŒ„
            resume_from: çºŒè¨“æª¢æŸ¥é»è·¯å¾‘ (å¯é¸)
            use_tensorboard: æ˜¯å¦ä½¿ç”¨ TensorBoard
        """
        print(f"\nğŸš€ é–‹å§‹è¨“ç·´ CoCa åˆ†é¡å™¨")
        print(f"ğŸ“Š è¨“ç·´åƒæ•¸:")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   å­¸ç¿’ç‡: {lr}")
        print(f"   æœ€å¤§è¼ªæ•¸: {epochs}")
        print(f"   æ—©åœè€å¿ƒ: {patience}")
        
        # è¨­ç½® TensorBoard
        writer = None
        if use_tensorboard and SummaryWriter is not None:
            log_dir = os.path.join('runs', f'coca_classifier_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
            writer = SummaryWriter(log_dir)
            print(f"ğŸ“ˆ TensorBoard æ—¥èªŒ: {log_dir}")
            print(f"ğŸ’¡ å•Ÿå‹• TensorBoard: tensorboard --logdir=runs --port=6006")
        elif use_tensorboard:
            print("âš ï¸ TensorBoard ä¸å¯ç”¨ï¼Œè«‹å®‰è£: pip install tensorboard")
        
        # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=6,
            pin_memory=True
        )
        
        val_loader = None
        if val_dataset:
            val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=6,
                pin_memory=True
            )
        
        # è¨­å®šå„ªåŒ–å™¨ - æ”¯æ´ä¸åŒå­¸ç¿’ç‡
        # åˆ†é›¢ CoCa å’Œåˆ†é¡é ­åƒæ•¸
        coca_params = [p for p in self.model.coca_model.parameters() if p.requires_grad]
        classifier_params = list(self.model.classifier_head.parameters())
        
        if coca_params:
            # å¦‚æœæœ‰ CoCa åƒæ•¸å¯è¨“ç·´ï¼Œä½¿ç”¨ä¸åŒå­¸ç¿’ç‡
            optimizer = optim.AdamW([
                {'params': coca_params, 'lr': lr * 0.1, 'name': 'coca'},        # CoCa ç”¨è¼ƒå°å­¸ç¿’ç‡
                {'params': classifier_params, 'lr': lr, 'name': 'classifier'}   # åˆ†é¡é ­ç”¨æ­£å¸¸å­¸ç¿’ç‡
            ], weight_decay=0.01)
            print(f"ğŸ¯ å¤šå±¤å­¸ç¿’ç‡: CoCa {lr * 0.1:.2e}, åˆ†é¡é ­ {lr:.2e}")
        else:
            # åªè¨“ç·´åˆ†é¡é ­
            optimizer = optim.AdamW(classifier_params, lr=lr, weight_decay=0.01)
            print(f"ğŸ¯ åˆ†é¡é ­å­¸ç¿’ç‡: {lr:.2e}")
        
        criterion = nn.CrossEntropyLoss()
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        
        # åˆå§‹åŒ–è¨“ç·´ç‹€æ…‹
        start_epoch = 0
        best_val_acc = 0.0
        patience_counter = 0
        train_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        # ğŸ”„ æ–·é»çºŒè¨“
        if resume_from:
            print(f"ğŸ”„ å¾æª¢æŸ¥é»çºŒè¨“: {resume_from}")
            load_info = self.load_checkpoint(resume_from, load_for_training=True)
            
            start_epoch = load_info['epoch'] + 1
            best_val_acc = load_info['accuracy']
            
            # è¼‰å…¥è¨“ç·´æ­·å²
            if 'history' in load_info and load_info['history']:
                train_history = load_info['history']
                print(f"ğŸ“Š è¼‰å…¥è¨“ç·´æ­·å²: {len(train_history['train_loss'])} è¼ª")
            
            # è¼‰å…¥å„ªåŒ–å™¨ç‹€æ…‹
            if load_info['optimizer_state']:
                optimizer.load_state_dict(load_info['optimizer_state'])
                print("âœ… å„ªåŒ–å™¨ç‹€æ…‹è¼‰å…¥æˆåŠŸ")
            
            # è¼‰å…¥èª¿åº¦å™¨ç‹€æ…‹
            if load_info['scheduler_state']:
                scheduler.load_state_dict(load_info['scheduler_state'])
                print("âœ… å­¸ç¿’ç‡èª¿åº¦å™¨è¼‰å…¥æˆåŠŸ")
            
            print(f"ğŸš€ å¾ç¬¬ {start_epoch + 1} è¼ªé–‹å§‹ç¹¼çºŒè¨“ç·´")
            print(f"ğŸ¯ ç•¶å‰æœ€ä½³æº–ç¢ºç‡: {best_val_acc:.2f}%")
        
        start_time = time.time()
        
        print(f"\nğŸ“ˆ é–‹å§‹è¨“ç·´...")
        
        for epoch in range(start_epoch, epochs):
            epoch_start = time.time()
            
            # è¨“ç·´éšæ®µ
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [è¨“ç·´]")
            
            for batch_idx, (images, labels) in enumerate(train_pbar):
                images, labels = images.to(self.device), labels.to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘å‚³æ’­
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                
                # åå‘å‚³æ’­
                loss.backward()
                optimizer.step()
                
                # çµ±è¨ˆ
                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                # æ›´æ–°é€²åº¦æ¢
                current_acc = 100. * train_correct / train_total
                train_pbar.set_postfix({
                    'Loss': f'{train_loss/(batch_idx+1):.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })
            
            # è¨ˆç®—è¨“ç·´æŒ‡æ¨™
            avg_train_loss = train_loss / len(train_loader)
            train_acc = 100. * train_correct / train_total
            
            train_history['train_loss'].append(avg_train_loss)
            train_history['train_acc'].append(train_acc)
            
            # é©—è­‰éšæ®µ
            val_loss, val_acc = 0.0, 0.0
            if val_loader:
                val_loss, val_acc = self._validate(val_loader, criterion)
                train_history['val_loss'].append(val_loss)
                train_history['val_acc'].append(val_acc)
            
            # ğŸ“ˆ TensorBoard è¨˜éŒ„
            if writer is not None:
                # æå¤±å’Œæº–ç¢ºç‡
                writer.add_scalar('Loss/Train', avg_train_loss, epoch)
                writer.add_scalar('Accuracy/Train', train_acc, epoch)
                
                if val_loader:
                    writer.add_scalar('Loss/Validation', val_loss, epoch)
                    writer.add_scalar('Accuracy/Validation', val_acc, epoch)
                
                # å­¸ç¿’ç‡
                current_lr = optimizer.param_groups[0]['lr']
                writer.add_scalar('Learning_Rate', current_lr, epoch)
                
                # å¦‚æœæœ‰å¤šå€‹åƒæ•¸çµ„ï¼Œè¨˜éŒ„æ‰€æœ‰å­¸ç¿’ç‡
                if len(optimizer.param_groups) > 1:
                    for i, group in enumerate(optimizer.param_groups):
                        writer.add_scalar(f'Learning_Rate/{group.get("name", f"group_{i}")}', group['lr'], epoch)
            
            # æ›´æ–°å­¸ç¿’ç‡
            scheduler.step()
            
            # è¨ˆç®—æ™‚é–“
            epoch_time = time.time() - epoch_start
            
            # æ‰“å°çµæœ
            print(f"\nEpoch {epoch+1}/{epochs} ({epoch_time:.1f}s):")
            print(f"  è¨“ç·´ - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%")
            if val_loader:
                print(f"  é©—è­‰ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
            
            # æ—©åœå’Œæ¨¡å‹ä¿å­˜
            current_val_acc = val_acc if val_loader else train_acc
            
            if current_val_acc > best_val_acc:
                best_val_acc = current_val_acc
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self._save_model(save_dir, epoch, current_val_acc, train_history, optimizer, scheduler)
                print(f"  ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹! é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
                
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                print(f"\nâ¹ï¸ æ—©åœ! {patience} è¼ªç„¡æ”¹å–„")
                break
        
        # è¨“ç·´å®Œæˆ
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è¨“ç·´å®Œæˆ!")
        print(f"â±ï¸ ç¸½æ™‚é–“: {total_time:.1f}s")
        print(f"ğŸ¯ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
        
        # é—œé–‰ TensorBoard
        if writer is not None:
            # è¨˜éŒ„æœ€çµ‚çµæœ
            writer.add_hparams(
                {'lr': lr, 'batch_size': batch_size, 'epochs': epochs},
                {'final_train_acc': train_history['train_acc'][-1] if train_history['train_acc'] else 0,
                 'final_val_acc': train_history['val_acc'][-1] if train_history['val_acc'] else 0,
                 'best_val_acc': best_val_acc}
            )
            writer.close()
            print(f"ğŸ“ˆ TensorBoard æ—¥èªŒå·²é—œé–‰")
        
        # ç¹ªè£½è¨“ç·´æ›²ç·š
        self._plot_training_history(train_history, save_dir)
        
        return train_history
    
    def _validate(self, val_loader, criterion):
        """é©—è­‰æ¨¡å‹"""
        self.model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(self.device), labels.to(self.device)
                
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_acc = 100. * val_correct / val_total
        
        return avg_val_loss, val_acc
    
    def _save_model(self, save_dir, epoch, accuracy, history, optimizer=None, scheduler=None):
        """ä¿å­˜æ¨¡å‹ (æ”¯æ´æ–·é»çºŒè¨“)"""
        os.makedirs(save_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        model_name = f"coca_classifier_epoch_{epoch+1:03d}_acc_{accuracy:.2f}_{timestamp}.pth"
        model_path = os.path.join(save_dir, model_name)
        
        # ä¿å­˜å®Œæ•´çš„æ¨¡å‹ç‹€æ…‹ (åŒ…å«è¨“ç·´ç‹€æ…‹)
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'accuracy': accuracy,
            'best_accuracy': accuracy,
            'num_classes': self.num_classes,
            'class_to_idx': self.class_to_idx,
            'idx_to_class': self.idx_to_class,
            'coca_model_name': self.coca_model_name,
            'feature_dim': self.feature_dim,
            'history': history
        }
        
        # ä¿å­˜å„ªåŒ–å™¨å’Œèª¿åº¦å™¨ç‹€æ…‹ (ç”¨æ–¼æ–·é»çºŒè¨“)
        if optimizer is not None:
            checkpoint['optimizer_state_dict'] = optimizer.state_dict()
        if scheduler is not None:
            checkpoint['scheduler_state_dict'] = scheduler.state_dict()
        
        torch.save(checkpoint, model_path)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¹Ÿä¿å­˜ä¸€å€‹ "latest" ç‰ˆæœ¬
        latest_path = os.path.join(save_dir, "coca_classifier_latest.pth")
        torch.save(checkpoint, latest_path)
    
    def _plot_training_history(self, history, save_dir):
        """ç¹ªè£½è¨“ç·´æ­·å²"""
        plt.figure(figsize=(12, 4))
        
        # Loss æ›²ç·š
        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='Training Loss', marker='o')
        if history['val_loss']:
            plt.plot(history['val_loss'], label='Validation Loss', marker='s')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training and Validation Loss')
        plt.legend()
        plt.grid(True)
        
        # Accuracy æ›²ç·š
        plt.subplot(1, 2, 2)
        plt.plot(history['train_acc'], label='Training Accuracy', marker='o')
        if history['val_acc']:
            plt.plot(history['val_acc'], label='Validation Accuracy', marker='s')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.title('Training and Validation Accuracy')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–ç‰‡
        plot_path = os.path.join(save_dir, 'coca_training_history.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š è¨“ç·´æ›²ç·šå·²ä¿å­˜: {plot_path}")
    
    def resume_training(self, checkpoint_path, train_dataset, val_dataset=None,
                       additional_epochs=20, new_lr=None, **kwargs):
        """
        ä¾¿æ·çš„æ–·é»çºŒè¨“å‡½æ•¸
        
        Args:
            checkpoint_path: æª¢æŸ¥é»è·¯å¾‘
            train_dataset: è¨“ç·´è³‡æ–™é›†
            val_dataset: é©—è­‰è³‡æ–™é›†
            additional_epochs: é¡å¤–è¨“ç·´è¼ªæ•¸
            new_lr: æ–°å­¸ç¿’ç‡ (å¯é¸ï¼Œç”¨æ–¼å¾®èª¿)
            **kwargs: å…¶ä»–è¨“ç·´åƒæ•¸
        """
        print(f"ğŸ”„ CoCa åˆ†é¡å™¨æ–·é»çºŒè¨“")
        print(f"ğŸ“‚ æª¢æŸ¥é»: {checkpoint_path}")
        print(f"â• é¡å¤–è¨“ç·´: {additional_epochs} è¼ª")
        
        # è¼‰å…¥æª¢æŸ¥é»ä¿¡æ¯
        load_info = self.load_checkpoint(checkpoint_path, load_for_training=False)
        current_epoch = load_info['epoch'] + 1
        target_epochs = current_epoch + additional_epochs
        
        print(f"ğŸ¯ ç›®æ¨™è¼ªæ•¸: {current_epoch} â†’ {target_epochs}")
        
        # è¨­å®šå­¸ç¿’ç‡
        lr = new_lr if new_lr else 1e-4  # é»˜èªä½¿ç”¨è¼ƒå°çš„å­¸ç¿’ç‡
        if new_lr:
            print(f"ğŸ“‰ ä½¿ç”¨æ–°å­¸ç¿’ç‡: {lr}")
        
        # é–‹å§‹çºŒè¨“
        return self.train(
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            epochs=target_epochs,
            lr=lr,
            resume_from=checkpoint_path,
            **kwargs
        )
    
    @staticmethod
    def find_checkpoints(model_dir='models'):
        """å°‹æ‰¾å¯ç”¨çš„æª¢æŸ¥é»"""
        print(f"ğŸ” æœå°‹æª¢æŸ¥é»: {model_dir}")
        
        if not os.path.exists(model_dir):
            print("âŒ æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨")
            return []
        
        checkpoints = []
        for file in os.listdir(model_dir):
            if file.endswith('.pth') and 'coca' in file:
                file_path = os.path.join(model_dir, file)
                try:
                    # å˜—è©¦è¼‰å…¥æª¢æŸ¥é»ä¿¡æ¯
                    checkpoint = torch.load(file_path, map_location='cpu')
                    info = {
                        'path': file_path,
                        'filename': file,
                        'epoch': checkpoint.get('epoch', 0),
                        'accuracy': checkpoint.get('accuracy', 0),
                        'timestamp': os.path.getmtime(file_path)
                    }
                    checkpoints.append(info)
                except:
                    continue
        
        # æŒ‰æº–ç¢ºç‡æ’åº
        checkpoints.sort(key=lambda x: x['accuracy'], reverse=True)
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(checkpoints)} å€‹æª¢æŸ¥é»:")
        for i, cp in enumerate(checkpoints[:5]):  # åªé¡¯ç¤ºå‰5å€‹
            timestamp = datetime.fromtimestamp(cp['timestamp']).strftime('%m/%d %H:%M')
            print(f"  {i+1}. {cp['filename']} (ç¬¬{cp['epoch']+1}è¼ª, æº–ç¢ºç‡:{cp['accuracy']:.2f}%) [{timestamp}]")
        
        return checkpoints

class CoCaClassifier(nn.Module):
    """
    CoCa åˆ†é¡å™¨æ¨¡å‹
    çµ„åˆ CoCa ç‰¹å¾µæå–å™¨å’Œè‡ªå®šç¾©åˆ†é¡é ­
    """
    
    def __init__(self, coca_model, classifier_head):
        super(CoCaClassifier, self).__init__()
        self.coca_model = coca_model
        self.classifier_head = classifier_head
        
    def forward(self, x):
        # ä½¿ç”¨ CoCa æå–ç‰¹å¾µ
        with torch.no_grad():  # å‡çµç‰¹å¾µæå–å™¨
            features = self.coca_model.encode_image(x)
            
        # é€šéåˆ†é¡é ­
        output = self.classifier_head(features)
        return output

def main():
    """ä¸»å‡½æ•¸ (æ”¯æ´çºŒè¨“)"""
    print("ğŸ”® CoCa è¾›æ™®æ£®è§’è‰²åˆ†é¡å™¨")
    print("=" * 50)
    
    # è¨­å®šåƒæ•¸
    NUM_CLASSES = 50  # è¾›æ™®æ£®è§’è‰²æ•¸é‡
    BATCH_SIZE = 32
    EPOCHS = 50
    LEARNING_RATE = 3e-3
    
    # è³‡æ–™è·¯å¾‘ (æ ¹æ“šæ‚¨çš„å¯¦éš›è·¯å¾‘èª¿æ•´)
    # æª¢æ¸¬ç’°å¢ƒ
    import platform
    is_wsl = "microsoft" in platform.uname().release.lower() or "WSL" in os.environ.get("WSL_DISTRO_NAME", "")
    Data_path = {}

    if is_wsl:
        base_path = "/mnt/e/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset"
        Data_path["train"] = f"{base_path}/augmented/train"
        Data_path["val"] = f"{base_path}/preprocessed/val"
    else:
        base_path = "E:/NYCU/NYCU_IAII_ML2025/Ass2-Classification/Dataset"
        Data_path["train"] = f"{base_path}/augmented/train"
        Data_path["val"] = f"{base_path}/preprocessed/val"

    try:
        # è¨“ç·´æ¨¡å¼é¸æ“‡
        print("\nğŸ”„ è¨“ç·´æ¨¡å¼é¸æ“‡:")
        print("1. å¾é ­é–‹å§‹è¨“ç·´ (é è¨­)")
        print("2. å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´")
        
        mode = input("è«‹é¸æ“‡ (1/2): ").strip()
        
        if mode == "2":
            # å°‹æ‰¾å¯ç”¨æª¢æŸ¥é»
            checkpoints = CoCaCharacterClassifier.find_checkpoints('models')
            
            if not checkpoints:
                print("âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„æª¢æŸ¥é»ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
                resume_from = None
            else:
                print(f"\nè«‹é¸æ“‡æª¢æŸ¥é» (1-{len(checkpoints)}): ", end="")
                try:
                    choice = int(input()) - 1
                    if 0 <= choice < len(checkpoints):
                        resume_from = checkpoints[choice]['path']
                        print(f"âœ… é¸æ“‡æª¢æŸ¥é»: {checkpoints[choice]['filename']}")
                    else:
                        print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œå¾é ­é–‹å§‹è¨“ç·´")
                        resume_from = None
                except:
                    print("âŒ è¼¸å…¥éŒ¯èª¤ï¼Œå¾é ­é–‹å§‹è¨“ç·´")
                    resume_from = None
        else:
            resume_from = None
        
        # åˆå§‹åŒ–åˆ†é¡å™¨
        print("ğŸš€ åˆå§‹åŒ– CoCa åˆ†é¡å™¨...")
        classifier = CoCaCharacterClassifier(
            num_classes=NUM_CLASSES,
            coca_model='coca_ViT-B-32'  # æˆ–å˜—è©¦ 'coca_ViT-L-14'
        )
        
        # æº–å‚™è³‡æ–™
        print("ğŸ“Š æº–å‚™è³‡æ–™...")
        train_dataset, val_dataset = classifier.prepare_data(Data_path)
        
        # é–‹å§‹è¨“ç·´
        if resume_from:
            print("ğŸ”„ çºŒè¨“æ¨¡å¼...")
            # å¦‚æœæ˜¯çºŒè¨“ï¼Œä½¿ç”¨è¼ƒå°çš„å­¸ç¿’ç‡å’Œè¼ƒå°‘è¼ªæ•¸
            history = classifier.train(
                train_dataset=train_dataset,
                val_dataset=val_dataset,
                batch_size=BATCH_SIZE,
                epochs=EPOCHS,
                lr=LEARNING_RATE / 2,  # è¼ƒå°çš„å­¸ç¿’ç‡
                patience=8,
                save_dir='models',
                resume_from=resume_from
            )
        else:
            print("ğŸ¯ å…¨æ–°è¨“ç·´...")
            history = classifier.train(
                train_dataset=train_dataset,
                val_dataset=val_dataset,
                batch_size=BATCH_SIZE,
                epochs=EPOCHS,
                lr=LEARNING_RATE,
                patience=10,
                save_dir='models'
            )
        
        print("\nğŸ‰ CoCa åˆ†é¡å™¨è¨“ç·´å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
