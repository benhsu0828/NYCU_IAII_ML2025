#!/usr/bin/env python3
"""
ğŸ”® CoCa (Contrastive Captioners) ç‰¹å¾µæå– + è‡ªå®šç¾©åˆ†é¡é ­

CoCa æ˜¯ä¸€å€‹å¼·å¤§çš„å¤šæ¨¡æ…‹æ¨¡å‹ï¼Œçµåˆäº†ï¼š
- å°æ¯”å­¸ç¿’ (Contrastive Learning)
- åœ–åƒæè¿°ç”Ÿæˆ (Image Captioning)
- å„ªç§€çš„è¦–è¦ºç‰¹å¾µæå–èƒ½åŠ›

é€™å€‹åˆ†é¡å™¨ä½¿ç”¨ CoCa ä½œç‚ºç‰¹å¾µæå–å™¨ï¼Œç„¶å¾Œè¨“ç·´è‡ªå®šç¾©çš„åˆ†é¡é ­ã€‚
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from torchvision import datasets

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os
import json
from datetime import datetime
import time
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# å®‰è£å’Œå°å…¥ open_clip (åŒ…å« CoCa)
try:
    import open_clip
    print("âœ… open_clip å·²å®‰è£")
except ImportError:
    print("âŒ éœ€è¦å®‰è£ open_clip:")
    print("pip install open-clip-torch")
    raise ImportError("è«‹å…ˆå®‰è£ open_clip: pip install open-clip-torch")

class CoCaCharacterClassifier:
    """
    åŸºæ–¼ CoCa çš„è¾›æ™®æ£®è§’è‰²åˆ†é¡å™¨
    
    ç‰¹é»ï¼š
    - ä½¿ç”¨ CoCa ä½œç‚ºå¼·å¤§çš„è¦–è¦ºç‰¹å¾µæå–å™¨
    - å‡çµ CoCa æ¬Šé‡ï¼Œåªè¨“ç·´åˆ†é¡é ­
    - æ”¯æ´å¤šç¨® CoCa æ¨¡å‹ç‰ˆæœ¬
    - é«˜æ•ˆçš„é·ç§»å­¸ç¿’
    """
    
    def __init__(self, num_classes=50, coca_model='coca_ViT-B-32', device=None):
        """
        åˆå§‹åŒ– CoCa åˆ†é¡å™¨
        
        Args:
            num_classes: åˆ†é¡é¡åˆ¥æ•¸
            coca_model: CoCa æ¨¡å‹ç‰ˆæœ¬ ('coca_ViT-B-32', 'coca_ViT-L-14')
            device: è¨ˆç®—è¨­å‚™
        """
        self.num_classes = num_classes
        self.coca_model_name = coca_model
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        print(f"ğŸ”® CoCa è¾›æ™®æ£®è§’è‰²åˆ†é¡å™¨")
        print(f"ğŸ¯ æ¨¡å‹: {coca_model}")
        print(f"ğŸ“Š é¡åˆ¥æ•¸: {num_classes}")
        print(f"ğŸ–¥ï¸ è¨­å‚™: {self.device}")
        
        # è¼‰å…¥ CoCa æ¨¡å‹
        self.coca_model, self.preprocess = self._load_coca_model()
        
        # å‰µå»ºåˆ†é¡é ­
        self.classifier_head = self._create_classification_head()
        
        # å®Œæ•´æ¨¡å‹
        self.model = CoCaClassifier(self.coca_model, self.classifier_head)
        self.model.to(self.device)
        
        # é¡åˆ¥æ˜ å°„
        self.class_to_idx = {}
        self.idx_to_class = {}
        
        print("âœ… CoCa åˆ†é¡å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_coca_model(self):
        """è¼‰å…¥é è¨“ç·´çš„ CoCa æ¨¡å‹"""
        print(f"ğŸ”„ è¼‰å…¥ CoCa æ¨¡å‹: {self.coca_model_name}")
        
        try:
            # è¼‰å…¥ CoCa æ¨¡å‹å’Œé è™•ç†
            model, _, preprocess = open_clip.create_model_and_transforms(
                self.coca_model_name, 
                pretrained='laion2b_s13b_b90k'  # ä½¿ç”¨ LAION-2B é è¨“ç·´æ¬Šé‡
            )
            
            print("âœ… CoCa æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            
            # å‡çµ CoCa åƒæ•¸
            for param in model.parameters():
                param.requires_grad = False
            
            print("â„ï¸ CoCa ç‰¹å¾µæå–å™¨å·²å‡çµ")
            
            # ç²å–ç‰¹å¾µç¶­åº¦
            dummy_input = torch.randn(1, 3, 224, 224)
            with torch.no_grad():
                features = model.encode_image(dummy_input)
                self.feature_dim = features.shape[-1]
            
            print(f"ğŸ“ ç‰¹å¾µç¶­åº¦: {self.feature_dim}")
            
            return model, preprocess
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥ CoCa æ¨¡å‹å¤±æ•—: {e}")
            print("ğŸ’¡ å˜—è©¦ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹...")
            
            # å‚™é¸ï¼šä½¿ç”¨è¼ƒå°çš„ CLIP æ¨¡å‹
            try:
                model, _, preprocess = open_clip.create_model_and_transforms(
                    'ViT-B-32', 
                    pretrained='openai'
                )
                print("âœ… æ”¹ç”¨ CLIP ViT-B-32 æ¨¡å‹")
                
                # å‡çµåƒæ•¸
                for param in model.parameters():
                    param.requires_grad = False
                
                # ç²å–ç‰¹å¾µç¶­åº¦
                dummy_input = torch.randn(1, 3, 224, 224)
                with torch.no_grad():
                    features = model.encode_image(dummy_input)
                    self.feature_dim = features.shape[-1]
                
                print(f"ğŸ“ ç‰¹å¾µç¶­åº¦: {self.feature_dim}")
                
                return model, preprocess
                
            except Exception as e2:
                raise RuntimeError(f"ç„¡æ³•è¼‰å…¥ä»»ä½•è¦–è¦ºæ¨¡å‹: {e2}")
    
    def _create_classification_head(self):
        """å‰µå»ºåˆ†é¡é ­"""
        print(f"ğŸ—ï¸ å‰µå»ºåˆ†é¡é ­: {self.feature_dim} â†’ {self.num_classes}")
        
        # å¤šå±¤åˆ†é¡é ­ï¼Œæé«˜è¡¨é”èƒ½åŠ›
        classifier = nn.Sequential(
            nn.Linear(self.feature_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(1024, 512),
            nn.ReLU(), 
            nn.Dropout(0.2),
            
            nn.Linear(512, self.num_classes)
        )
        
        # çµ±è¨ˆåˆ†é¡é ­åƒæ•¸
        classifier_params = sum(p.numel() for p in classifier.parameters())
        print(f"ğŸ¯ åˆ†é¡é ­åƒæ•¸: {classifier_params:,} ({classifier_params/1e6:.2f}M)")
        
        return classifier
    
    def get_transforms(self, is_training=True):
        """
        ç²å–è³‡æ–™é è™•ç†è®Šæ›
        ä½¿ç”¨ CoCa çš„æ¨™æº–é è™•ç†ï¼Œä¸¦æ·»åŠ è¨“ç·´æ™‚çš„å¢å¼·
        """
        if is_training:
            # è¨“ç·´æ™‚æ·»åŠ ä¸€äº›å¢å¼·ï¼Œä½†ä¿æŒèˆ‡ CoCa é è™•ç†çš„ç›¸å®¹æ€§
            transform = transforms.Compose([
                transforms.Resize((224, 224)),  # CoCa æ¨™æº–è¼¸å…¥å°ºå¯¸
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.48145466, 0.4578275, 0.40821073],  # CLIP/CoCa æ¨™æº–
                    std=[0.26862954, 0.26130258, 0.27577711]
                )
            ])
        else:
            # é©—è­‰æ™‚ä½¿ç”¨æ¨™æº–é è™•ç†
            transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.48145466, 0.4578275, 0.40821073],
                    std=[0.26862954, 0.26130258, 0.27577711]
                )
            ])
        
        return transform
    
    def prepare_data(self, data_paths):
        """
        æº–å‚™è³‡æ–™é›† - ç›´æ¥è®€å–ç¾æœ‰è³‡æ–™ï¼Œä¸é€²è¡Œ data aggregation
        
        Args:
            data_paths: åŒ…å« train, val è·¯å¾‘çš„å­—å…¸
        """
        print("\nğŸ“Š æº–å‚™è³‡æ–™...")
        
        # ç¢ºèªè³‡æ–™è·¯å¾‘
        train_path = data_paths['train']
        val_path = data_paths['val']
        
        print(f"ğŸ“ è¼‰å…¥è¨“ç·´è³‡æ–™: {train_path}")
        print(f"ğŸ“ è¼‰å…¥é©—è­‰è³‡æ–™: {val_path}")
        
        # æª¢æŸ¥è·¯å¾‘æ˜¯å¦å­˜åœ¨
        if not os.path.exists(train_path):
            raise FileNotFoundError(f"è¨“ç·´è³‡æ–™è·¯å¾‘ä¸å­˜åœ¨: {train_path}")
        if not os.path.exists(val_path):
            raise FileNotFoundError(f"é©—è­‰è³‡æ–™è·¯å¾‘ä¸å­˜åœ¨: {val_path}")
        
        # è¨“ç·´è³‡æ–™
        train_transform = self.get_transforms(is_training=True)
        train_dataset = datasets.ImageFolder(
            root=train_path,
            transform=train_transform
        )
        
        # é©—è­‰è³‡æ–™
        val_transform = self.get_transforms(is_training=False)
        val_dataset = datasets.ImageFolder(
            root=val_path,
            transform=val_transform
        )
        
        # å»ºç«‹çµ±ä¸€çš„é¡åˆ¥æ˜ å°„
        self.class_to_idx = train_dataset.class_to_idx
        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}
        
        # é¡¯ç¤ºè³‡æ–™é›†çµ±è¨ˆ
        print(f"\nğŸ“ˆ è³‡æ–™é›†çµ±è¨ˆ:")
        print(f"   è¨“ç·´æ¨£æœ¬: {len(train_dataset):,}")
        print(f"   é©—è­‰æ¨£æœ¬: {len(val_dataset):,}")
        print(f"   é¡åˆ¥æ•¸é‡: {len(self.class_to_idx)}")
        
        # é¡¯ç¤ºéƒ¨åˆ†é¡åˆ¥
        class_names = list(self.class_to_idx.keys())
        if len(class_names) <= 10:
            print(f"   é¡åˆ¥åˆ—è¡¨: {', '.join(class_names)}")
        else:
            print(f"   é¡åˆ¥ç¯„ä¾‹: {', '.join(class_names[:5])} ... (+{len(class_names)-5} æ›´å¤š)")
        
        return train_dataset, val_dataset
    
    def train(self, train_dataset, val_dataset=None, 
              batch_size=32, epochs=50, lr=1e-3, 
              patience=10, save_dir='models'):
        """
        è¨“ç·´åˆ†é¡å™¨
        
        Args:
            train_dataset: è¨“ç·´è³‡æ–™é›†
            val_dataset: é©—è­‰è³‡æ–™é›†
            batch_size: æ‰¹æ¬¡å¤§å°
            epochs: è¨“ç·´è¼ªæ•¸
            lr: å­¸ç¿’ç‡
            patience: æ—©åœè€å¿ƒå€¼
            save_dir: æ¨¡å‹ä¿å­˜ç›®éŒ„
        """
        print(f"\nğŸš€ é–‹å§‹è¨“ç·´ CoCa åˆ†é¡å™¨")
        print(f"ğŸ“Š è¨“ç·´åƒæ•¸:")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   å­¸ç¿’ç‡: {lr}")
        print(f"   æœ€å¤§è¼ªæ•¸: {epochs}")
        print(f"   æ—©åœè€å¿ƒ: {patience}")
        
        # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        
        val_loader = None
        if val_dataset:
            val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=4,
                pin_memory=True
            )
        
        # è¨­å®šå„ªåŒ–å™¨å’Œæå¤±å‡½æ•¸
        optimizer = optim.AdamW(
            self.model.classifier_head.parameters(),  # åªè¨“ç·´åˆ†é¡é ­
            lr=lr,
            weight_decay=0.01
        )
        
        criterion = nn.CrossEntropyLoss()
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        
        # è¨“ç·´è¨˜éŒ„
        train_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        best_val_acc = 0.0
        patience_counter = 0
        start_time = time.time()
        
        print(f"\nğŸ“ˆ é–‹å§‹è¨“ç·´...")
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è¨“ç·´éšæ®µ
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [è¨“ç·´]")
            
            for batch_idx, (images, labels) in enumerate(train_pbar):
                images, labels = images.to(self.device), labels.to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘å‚³æ’­
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                
                # åå‘å‚³æ’­
                loss.backward()
                optimizer.step()
                
                # çµ±è¨ˆ
                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                # æ›´æ–°é€²åº¦æ¢
                current_acc = 100. * train_correct / train_total
                train_pbar.set_postfix({
                    'Loss': f'{train_loss/(batch_idx+1):.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })
            
            # è¨ˆç®—è¨“ç·´æŒ‡æ¨™
            avg_train_loss = train_loss / len(train_loader)
            train_acc = 100. * train_correct / train_total
            
            train_history['train_loss'].append(avg_train_loss)
            train_history['train_acc'].append(train_acc)
            
            # é©—è­‰éšæ®µ
            val_loss, val_acc = 0.0, 0.0
            if val_loader:
                val_loss, val_acc = self._validate(val_loader, criterion)
                train_history['val_loss'].append(val_loss)
                train_history['val_acc'].append(val_acc)
            
            # æ›´æ–°å­¸ç¿’ç‡
            scheduler.step()
            
            # è¨ˆç®—æ™‚é–“
            epoch_time = time.time() - epoch_start
            
            # æ‰“å°çµæœ
            print(f"\nEpoch {epoch+1}/{epochs} ({epoch_time:.1f}s):")
            print(f"  è¨“ç·´ - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%")
            if val_loader:
                print(f"  é©—è­‰ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
            
            # æ—©åœå’Œæ¨¡å‹ä¿å­˜
            current_val_acc = val_acc if val_loader else train_acc
            
            if current_val_acc > best_val_acc:
                best_val_acc = current_val_acc
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self._save_model(save_dir, epoch, current_val_acc, train_history)
                print(f"  ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹! é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
                
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                print(f"\nâ¹ï¸ æ—©åœ! {patience} è¼ªç„¡æ”¹å–„")
                break
        
        # è¨“ç·´å®Œæˆ
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è¨“ç·´å®Œæˆ!")
        print(f"â±ï¸ ç¸½æ™‚é–“: {total_time:.1f}s")
        print(f"ğŸ¯ æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.2f}%")
        
        # ç¹ªè£½è¨“ç·´æ›²ç·š
        self._plot_training_history(train_history, save_dir)
        
        return train_history
    
    def _validate(self, val_loader, criterion):
        """é©—è­‰æ¨¡å‹"""
        self.model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(self.device), labels.to(self.device)
                
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_acc = 100. * val_correct / val_total
        
        return avg_val_loss, val_acc
    
    def _save_model(self, save_dir, epoch, accuracy, history):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(save_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        model_name = f"coca_classifier_epoch_{epoch+1:03d}_acc_{accuracy:.2f}_{timestamp}.pth"
        model_path = os.path.join(save_dir, model_name)
        
        # ä¿å­˜å®Œæ•´çš„æ¨¡å‹ç‹€æ…‹
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': None,  # å¯é¸
            'accuracy': accuracy,
            'num_classes': self.num_classes,
            'class_to_idx': self.class_to_idx,
            'idx_to_class': self.idx_to_class,
            'coca_model_name': self.coca_model_name,
            'feature_dim': self.feature_dim,
            'history': history
        }, model_path)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¹Ÿä¿å­˜ä¸€å€‹ "latest" ç‰ˆæœ¬
        latest_path = os.path.join(save_dir, "coca_classifier_latest.pth")
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': None,
            'accuracy': accuracy,
            'num_classes': self.num_classes,
            'class_to_idx': self.class_to_idx,
            'idx_to_class': self.idx_to_class,
            'coca_model_name': self.coca_model_name,
            'feature_dim': self.feature_dim,
            'history': history
        }, latest_path)
    
    def _plot_training_history(self, history, save_dir):
        """ç¹ªè£½è¨“ç·´æ­·å²"""
        plt.figure(figsize=(12, 4))
        
        # Loss æ›²ç·š
        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='Training Loss', marker='o')
        if history['val_loss']:
            plt.plot(history['val_loss'], label='Validation Loss', marker='s')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training and Validation Loss')
        plt.legend()
        plt.grid(True)
        
        # Accuracy æ›²ç·š
        plt.subplot(1, 2, 2)
        plt.plot(history['train_acc'], label='Training Accuracy', marker='o')
        if history['val_acc']:
            plt.plot(history['val_acc'], label='Validation Accuracy', marker='s')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.title('Training and Validation Accuracy')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–ç‰‡
        plot_path = os.path.join(save_dir, 'coca_training_history.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š è¨“ç·´æ›²ç·šå·²ä¿å­˜: {plot_path}")

class CoCaClassifier(nn.Module):
    """
    CoCa åˆ†é¡å™¨æ¨¡å‹
    çµ„åˆ CoCa ç‰¹å¾µæå–å™¨å’Œè‡ªå®šç¾©åˆ†é¡é ­
    """
    
    def __init__(self, coca_model, classifier_head):
        super(CoCaClassifier, self).__init__()
        self.coca_model = coca_model
        self.classifier_head = classifier_head
        
    def forward(self, x):
        # ä½¿ç”¨ CoCa æå–ç‰¹å¾µ
        with torch.no_grad():  # å‡çµç‰¹å¾µæå–å™¨
            features = self.coca_model.encode_image(x)
            
        # é€šéåˆ†é¡é ­
        output = self.classifier_head(features)
        return output

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ”® CoCa è¾›æ™®æ£®è§’è‰²åˆ†é¡å™¨")
    print("=" * 50)
    
    # è¨­å®šåƒæ•¸
    NUM_CLASSES = 50  # è¾›æ™®æ£®è§’è‰²æ•¸é‡
    BATCH_SIZE = 16   # CoCa æ¨¡å‹è¼ƒå¤§ï¼Œä½¿ç”¨è¼ƒå°çš„æ‰¹æ¬¡
    EPOCHS = 30
    LEARNING_RATE = 1e-3
    
    # è³‡æ–™è·¯å¾‘ (æ ¹æ“šæ‚¨çš„å¯¦éš›è·¯å¾‘èª¿æ•´)
    data_paths = {
        'train': '/Users/nimab/Desktop/é™½äº¤å¤§/NYCU_IAII_ML2025/Ass2-Classification/Dataset/train',
        'val': '/Users/nimab/Desktop/é™½äº¤å¤§/NYCU_IAII_ML2025/Ass2-Classification/Dataset/val'
    }
    
    try:
        # åˆå§‹åŒ–åˆ†é¡å™¨
        print("ğŸš€ åˆå§‹åŒ– CoCa åˆ†é¡å™¨...")
        classifier = CoCaCharacterClassifier(
            num_classes=NUM_CLASSES,
            coca_model='coca_ViT-B-32'  # æˆ–å˜—è©¦ 'coca_ViT-L-14'
        )
        
        # æº–å‚™è³‡æ–™
        print("ğŸ“Š æº–å‚™è³‡æ–™...")
        train_dataset, val_dataset = classifier.prepare_data(data_paths)
        
        # é–‹å§‹è¨“ç·´
        print("ğŸ¯ é–‹å§‹è¨“ç·´...")
        history = classifier.train(
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            batch_size=BATCH_SIZE,
            epochs=EPOCHS,
            lr=LEARNING_RATE,
            patience=10,
            save_dir='models'
        )
        
        print("\nğŸ‰ CoCa åˆ†é¡å™¨è¨“ç·´å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
